Machine LearningTom M. MitchellProduct Details 
• Hardcover: 432 pages ; Dimensions (in inches): 0.75 x 10.00 x 6.50 
• Publisher: McGraw-Hill Science/Engineering/Math; (March 1, 1997) 
• ISBN: 0070428077 
• Average Customer Review: 
 Based on 16 reviews. 
• Amazon.com Sales Rank: 42,816 
• Popular in: Redmond, WA (#17) , Ithaca, NY (#9) 
 
 
Editorial ReviewsFrom Book News, Inc. An introductory text on primary approaches to machine learning andthe study of computer algorithms that improve au to ma ti ca ll yt hr ou gh ex pe ri en ce . In tr od uc ebasics concepts from statistics, artificial intellig ence, information theory, and other disciplines asneed arises, with balanced coverage of theory and practice, and presents major algorithms withillustrations of their use. Includes chapter exerci ses. Online data sets and implementations ofseveral algorithms are available ona Web site. No prior background in artificial intelligence orstatistics is assumed. For advanced undergraduat es and graduate students in computer science, 
engineering, statistics, and social scien ces, as well as software professionals. Book News, Inc.®, 
Portland, ORBook Info: Presents the key algorithms and theory that form the core of machine learning. 
Discusses such theoretical issues as How does learning performance vary with the number oftraining examples presented? and Which learning algorithms are most a ppropriate for varioustypes of learning tasks? DLC: Computer algorithms. 
Book Description: This book covers the field of machin e learning, which is the study ofalgorithms that allow computer programs to automatically improve through experience. Thebook is intended to support upper le vel undergraduate and introductory level graduate courses inmachine learningPREFACEThe field of machine learning is concerned with the question of how to constructcomputer programs that automatically improve with experience. In recent yearsmany successful machine learning applications have been developed, ranging fromdata-mining programs that learn to detect fraudulent credit card transactions, toinformation-filtering systems that learn users' reading preferences, to autonomousvehicles that learn to drive on public highways. At the same time, there have beenimportant advances in the theory and algorithms that form the foundations of thisfield. 
The goal of this textbook isto present the key algorithms and theory thatform the core of machine learning. Machine learning draws on concepts andresults from many fields, including statistics, artificial intelligence, philosophy, 
information theory, biology, cognitive science, computational complexity, andcontrol theory. My belief is that the best way to learn about machine learning isto view it from all of these perspectives and to understand the problem settings, 
algorithms, and assumptions that underlie each. In the past, this has been difficultdue to the absence ofa broad-based single source introduction to the field. Theprimary goal of this book isto provide such an introduction. 
Because of the interdisciplinary nature of the material, this book makesfew assumptions about the background of the reader. Instead, it introduces basicconcepts from statistics, artificial intelligence, information theory, and other disci- 
plines as the need arises, focusing on just those concepts most relevant to machinelearning. The book is intended for both undergraduate and graduate students infields such as computer science, engineering, statistics, and the social sciences, 
and asa reference for software professionals and practitioners. Two principlesthat guided the writing of the book were that it should be accessible to undergrad- 
uate students and that it should contain the material I would want my own Ph.D. 
students to learn before beginning their doctoral research in machine learning. 
xvi PREFACEA third principle that guided the writing of this book was that it shouldpresent a balance of theory and practice. Machine learning theory attempts toan- 
swer questions such as "How does learning performance vary with the number oftraining examples presented?" and "Which learning algorithms are most appropri- 
ate for various types of learning tasks?" This book includes discussions of theseand other theoretical issues, drawing on theoretical constructs from statistics, com- 
putational complexity, and Bayesian analysis. The practice of machine learningis covered by presenting the major algorithms in the field, along with illustrativetraces of their operation. Online data sets and implementations of several algo- 
rithms are available via the World Wide Web at http://www.cs.cmu.edu/-tom1mlbook.html. These include neural network code and data for face recognition, 
decision tree learning, code and data for financial loan analysis, and Bayes clas- 
sifier code and data for analyzing text documents. Iam grateful toa number ofcolleagues who have helped to create these online resources, including Jason Ren- 
nie, Paul Hsiung, Jeff Shufelt, Matt Glickman, Scott Davies, Joseph O'Sullivan, 
Ken Lang, Andrew McCallum, and Thorsten Joachims. 
ACKNOWLEDGMENTSIn writing this book, I have been fortunate tobe assisted by technical expertsin many of the subdisciplines that make up the field of machine learning. Thisbook could not have been written without their help. Iam deeply indebted tothe following scientists who took the time to review chapter drafts and, in manycases, to tutor me and help organize chapters in their individual areas of expertise. 
Avrim Blum, Jaime Carbonell, William Cohen, Greg Cooper, Mark Craven, 
Ken DeJong, Jerry DeJong, Tom Dietterich, Susan Epstein, Oren Etzioni, 
Scott Fahlman, Stephanie Forrest, David Haussler, Haym Hirsh, Rob Holte, 
Leslie Pack Kaelbling, Dennis Kibler, Moshe Koppel, John Koza, MiroslavKubat, John Lafferty, Ramon Lopez de Mantaras, Sridhar Mahadevan, StanMatwin, Andrew McCallum, Raymond Mooney, Andrew Moore, KatharinaMorik, Steve Muggleton, Michael Pazzani, David Poole, Armand Prieditis, 
Jim Reggia, Stuart Russell, Lorenza Saitta, Claude Sammut, Jeff Schneider, 
Jude Shavlik, Devika Subramanian, Michael Swain, Gheorgh Tecuci, Se- 
bastian Thrun, Peter Turney, Paul Utgoff, Manuela Veloso, Alex Waibel, 
Stefan Wrobel, and Yiming Yang. 
Iam also grateful to the many instructors and students at various universi- 
ties who have field tested various drafts of this book and who have contributedtheir suggestions. Although there isno space to thank the hundreds of students, 
instructors, and others who tested earlier drafts of this book, I would like to thankthe following for particularly helpful comments and discussions: 
Shumeet Baluja, Andrew Banas, Andy Barto, Jim Blackson, Justin Boyan, 
Rich Caruana, Philip Chan, Jonathan Cheyer, Lonnie Chrisman, Dayne Frei- 
tag, Geoff Gordon, Warren Greiff, Alexander Harm, Tom Ioerger, ThorstenPREFACE xviiJoachim, Atsushi Kawamura, Martina Klose, Sven Koenig, Jay Modi, An- 
drew Ng, Joseph O'Sullivan, Patrawadee Prasangsit, Doina Precup, BobPrice, Choon Quek, Sean Slattery, Belinda Thom, Astro Teller, Will TraczI would like to thank Joan Mitchell for creating the index for the book. Ialso would like to thank Jean Harpley for help in editing many of the figures. 
Jane Loftus from ETP Harrison improved the presentation significantly throughher copyediting of the manuscript and generally helped usher the manuscriptthrough the intricacies of final production. Eric Munson, my editor at McGrawHill, provided encouragement and expertise in all phases of this project. 
As always, the greatest debt one owes isto one's colleagues, friends, andfamily. Inmy case, this debt is especially large. I can hardly imagine a moreintellectually stimulating environment and supportive set of friends than those Ihave at Carnegie Mellon. Among the many here who helped, I would especiallylike to thank Sebastian Thrun, who throughout this project was a constant sourceof encouragement, technical expertise, and support of all kinds. My parents, asalways, encouraged and asked "Isit done yet?" at just the right times. Finally, Imust thank my family: Meghan, Shannon, and Joan. They are responsible for thisbook in more ways than even they know. This book is dedicated to them. 
Tom M. MitchellCHAPTERINTRODUCTIONEver since computers were invented, we have wondered whether they might bemade to learn. Ifwe could understand how to program them to learn-to improveautomatically with experience-the impact would be dramatic. Imagine comput- 
ers learning from medical records which treatments are most effective for newdiseases, houses learning from experience to optimize energy costs based on theparticular usage patterns of their occupants, or personal software assistants learn- 
ing the evolving interests of their users in order to highlight especially relevantstories from the online morning newspaper. A successful understanding of how tomake computers learn would open up many new uses of computers and new levelsof competence and customization. And a detailed understanding of information- 
processing algorithms for machine learning might lead toa better understandingof human learning abilities (and disabilities) as well. 
Wedo not yet know how to make computers learn nearly as well as peoplelearn. However, algorithms have been invented that are effective for certain typesof learning tasks, and a theoretical understanding of learning is beginning toemerge. Many practical computer programs have been developed to exhibit use- 
ful types of learning, and significant commercial applications have begun toap- 
pear. For problems such as speech recognition, algorithms based on machinelearning outperform all other approaches that have been attempted to date. Inthe field known as data mining, machine learning algorithms are being used rou- 
tinely to discover valuable knowledge from large commercial databases containingequipment maintenance records, loan applications, financial transactions, medicalrecords, and the like. As our understanding of computers continues to mature, it2 MACHINE LEARNINGseems inevitable that machine learning will play an increasingly central role incomputer science and computer technology. 
A few specific achievements provide a glimpse of the state of the art: pro- 
grams have been developed that successfully learn to recognize spoken words 
(Waibel 1989; Lee 1989), predict recovery rates of pneumonia patients (Cooperet al. 1997), detect fraudulent use of credit cards, drive autonomous vehicleson public highways (Pomerleau 1989), and play games such as backgammon atlevels approaching the performance of human world champions (Tesauro 1992, 
1995). Theoretical results have been developed that characterize the fundamentalrelationship among the number of training examples observed, the number ofhy- 
potheses under consideration, and the expected error in learned hypotheses. Weare beginning to obtain initial models of human and animal learning and toun- 
derstand their relationship to learning algorithms developed for computers (e.g., 
Laird etal. 1986; Anderson 1991; Qin etal. 1992; Chi and Bassock 1989; Ahnand Brewer 1993). In applications, algorithms, theory, and studies of biologicalsystems, the rate of progress has increased significantly over the past decade. Sev- 
eral recent applications of machine learning are summarized in Table 1.1. Langleyand Simon (1995) and Rumelhart etal. (1994) survey additional applications ofmachine learning. 
This book presents the field of machine learning, describing a variety oflearning paradigms, algorithms, theoretical results, and applications. Machinelearning is inherently a multidisciplinary field. It draws on results from artifi- 
cial intelligence, probability and statistics, computational complexity theory, con- 
trol theory, information theory, philosophy, psychology, neurobiology, and otherfields. Table 1.2 summarizes key ideas from each of these fields that impact thefield of machine learning. While the material in this book is based on results frommany diverse fields, the reader need not bean expert in any of them. Key ideasare presented from these fields using a nonspecialist's vocabulary, with unfamiliarterms and concepts introduced as the need arises. 
1.1 WELL-POSED LEARNING PROBLEMSLet us begin our study of machine learning by considering a few learning tasks. Forthe purposes of this book we will define learning broadly, to include any .computerprogram that improves its performance at some task through experience. Put moreprecisely, 
Definition: A computer program is said to learn from experience E with respectto some class of tasks T and performance measure P, if its performance at tasks inT, as measured byP, improves with experience E. 
For example, a computer program that learns to play checkers might improveits performance as measured by its abiliry to win at the class of tasks involvingplaying checkers games, through experience obtained by playing games againstitself. In general, to have a well-defined learning problem, we must identity theseCHAPTER 1 INTRODUCITON 30 Learning to recognize spoken words. 
All of the most successful speech recognition systems employ machine learning in some form. 
For example, the SPHINX system (e.g., Lee 1989) learns speaker-specific strategies for recognizingthe primitive sounds (phonemes) and words from the observed speech signal. Neural networklearning methods (e.g., Waibel etal. 1989) and methods for learning hidden Markov models 
(e.g., Lee 1989) are effective for automatically customizing to,individual speakers, vocabularies, 
microphone characteristics, background noise, etc. Similar techniques have potential applicationsin many signal-interpretation problems. 
0 Learning to drive an autonomous vehicle. 
Machine learning methods have been used to train computer-controlled vehicles to steer correctlywhen driving ona variety of road types. For example, the ALVINN system (Pomerleau 1989) 
has used its learned strategies to drive unassisted at 70 miles per hour for 90 miles on publichighways among other cars. Similar techniques have possible applications in many sensor-basedcontrol problems. 
0 Learning to classify new astronomical structures. 
Machine learning methods have been applied toa variety of large databases to learn generalregularities implicit in the data. For example, decision tree learning algorithms have been usedby NASA to learn how to classify celestial objects from the second Palomar Observatory SkySurvey (Fayyad etal. 1995). This system is now used to automatically classify all objects in theSky Survey, which consists of three terrabytes of image data. 
0 Learning to play world-class backgammon. 
The most successful computer programs for playing games such as backgammon are based onmachiie learning algorithms. For example, the world's top computer program for backgammon, 
TD-GAMMON (Tesauro 1992, 1995). learned its strategy by playing over one million practicegames against itself. It now plays ata level competitive with the human world champion. Similartechniques have applications in many practical problems where very large search spaces must beexamined efficiently. 
TABLE 1.1Some successful applications of machiie learning. 
three features: the class of tasks, the measure of performance tobe improved, andthe source of experience. 
A checkers learning problem: 
Task T: playing checkers0 Performance measure P: percent of games won against opponentsTraining experience E: playing practice games against itselfWe can specify many learning problems in this fashion, such as learningto recognize handwritten words, or learning to drive a robotic automobile au- 
tonomously. 
A handwriting recognition learning problem: 
0 Task T: recognizing and classifying handwritten words within images0 Performance measure P: percent of words correctly classified4 MACHINE LEARNINGArtificial intelligenceLearning symbolic representations of concepts. Machine learning asa search problem. Learningas an approach to improving problem solving. Using prior knowledge together with training datato guide learning. 
0 Bayesian methodsBayes' theorem as the basis for calculating probabilities of hypotheses. The naive Bayes classifier. 
Algorithms for estimating values of unobserved variables. 
0 Computational complexity theoryTheoretical bounds on the inherent complexity of different learning tasks, measured in terms ofthe computational effort, number of training examples, number of mistakes, etc. required in orderto learn. 
Control theoryProcedures that learn to control processes in order to optimize predefined objectives and that learnto predict the next state of the process they are controlling. 
0 Information theoryMeasures of entropy and information content. Minimum description length approaches to learning. 
Optimal codes and their relationship to optimal training sequences for encoding a hypothesis. 
PhilosophyOccam's razor, suggesting that the simplest hypothesis is the best. Analysis of the justification forgeneralizing beyond observed data. 
0 Psychology and neurobiologyThe power law of practice, which states that over a very broad range of learning problems, 
people's response time improves with practice according toa power law. Neurobiological studiesmotivating artificial neural network models of learning. 
0 StatisticsCharacterization of errors (e.g., bias and variance) that occur when estimating the accuracy of ahypothesis based ona limited sample of data. Confidence intervals, statistical tests. 
TABLE 1.2Some disciplines and examples of their influence on machine learning. 
0 Training experience E: a database of handwritten words with given classi- 
ficationsA robot driving learning problem: 
0 Task T: driving on public four-lane highways using vision sensors0 Performance measure P: average distance traveled before an error (as judgedby human overseer) 
0 Training experience E: a sequence of images and steering commands record- 
ed while observing a human driverOur definition of learning is broad enough to include most tasks that wewould conventionally call "learning" tasks, aswe use the word in everyday lan- 
guage. Itis also broad enough to encompass computer programs that improvefrom experience in quite straightforward ways. For example, a database systemCHAFTlB 1 INTRODUCTION 5that allows users to update data entries would fit our definition ofa learningsystem: it improves its performance at answering database queries, based on theexperience gained from database updates. Rather than worry about whether thistype of activity falls under the usual informal conversational meaning of the word 
"learning," we will simply adopt our technical definition of the class of programsthat improve through experience. Within this class we will find many types ofproblems that require more or less sophisticated solutions. Our concern here isnot to analyze the meaning of the English word "learning" asit is used inev- 
eryday language. Instead, our goal isto define precisely a class of problems thatencompasses interesting forms of learning, to explore algorithms that solve suchproblems, and to understand the fundamental structure of learning problems andprocesses. 
1.2 DESIGNING A LEARNING SYSTEMIn order to illustrate some of the basic design issues and approaches to machinelearning, let us consider designing a program to learn to play checkers, withthe goal of entering itin the world checkers tournament. We adopt the obviousperformance measure: the percent of games it wins in this world tournament. 
1.2.1 Choosing the Training ExperienceThe first design choice we face isto choose the type of training experience fromwhich our system will learn. The type of training experience available can have asignificant impact on success or failure of the learner. One key attribute is whetherthe training experience provides direct or indirect feedback regarding the choicesmade by the performance system. For example, in learning to play checkers, thesystem might learn from direct training examples consisting of individual checkersboard states and the correct move for each. Alternatively, it might have availableonly indirect information consisting of the move sequences and final outcomesof various games played. In this later case, information about the correctnessof specific moves early in the game must be inferred indirectly from the factthat the game was eventually won or lost. Here the learner faces an additionalproblem of credit assignment, or determining the degree to which each move inthe sequence deserves credit or blame for the final outcome. Credit assignment canbe a particularly difficult problem because the game can be lost even when earlymoves are optimal, if these are followed later by poor moves. Hence, learning fromdirect training feedback is typically easier than learning from indirect feedback. 
A second important attribute of the training experience is the degree to whichthe learner controls the sequence of training examples. For example, the learnermight rely on the teacher to select informative board states and to provide thecorrect move for each. Alternatively, the learner might itself propose board statesthat it finds particularly confusing and ask the teacher for the correct move. Or thelearner may have complete control over both the board states and (indirect) trainingclassifications, asit does when it learns by playing against itself with no teacherpresent. Notice in this last case the learner may choose between experimentingwith novel board states that it has not yet considered, or honing its skill by playingminor variations of lines of play it currently finds most promising. Subsequentchapters consider a number of settings for learning, including settings in whichtraining experience is provided bya random process outside the learner's control, 
settings in which the learner may pose various types of queries toan expert teacher, 
and settings in which the learner collects training examples by autonomouslyexploring its environment. 
A third important attribute of the training experience is how well it repre- 
sents the distribution of examples over which the final system performance P mustbe measured. In general, learning is most reliable when the training examples fol- 
low a distribution similar to that of future test examples. In our checkers learningscenario, the performance metric Pis the percent of games the system wins inthe world tournament. If its training experience E consists only of games playedagainst itself, there isan obvious danger that this training experience might notbe fully representative of the distribution of situations over which it will later betested. For example, the learner might never encounter certain crucial board statesthat are very likely tobe played by the human checkers champion. In practice, 
itis often necessary to learn from a distribution of examples that is somewhatdifferent from those on which the final system will be evaluated (e.g., the worldcheckers champion might not be interested in teaching the program!). Such situ- 
ations are problematic because mastery of one distribution of examples will notnecessary lead to strong performance over some other distribution. We shall seethat most current theory of machine learning rests on the crucial assumption thatthe distribution of training examples is identical to the distribution of test ex- 
amples. Despite our need to make this assumption in order to obtain theoreticalresults, itis important to keep in mind that this assumption must often be violatedin practice. 
To proceed with our design, let us decide that our system will train byplaying games against itself. This has the advantage that no external trainer needbe present, and it therefore allows the system to generate as much training dataas time permits. We now have a fully specified learning task. 
A checkers learning problem: 
0 Task T: playing checkers0 Performance measure P: percent of games won in the world tournament0 Training experience E: games played against itselfIn order to complete the design of the learning system, we must now choose1. the exact type of knowledge tobe,learned2. a representation for this target knowledge3. a learning mechanismCHAFTER I INTRODUCTION 71.2.2 Choosing the Target FunctionThe next design choice isto determine exactly what type of knowledge will belearned and how this will be used by the performance program. Let us begin witha checkers-playing program that can generate the legal moves from any boardstate. The program needs only to learn how to choose the best move from amongthese legal moves. This learning task is representative ofa large class of tasks forwhich the legal moves that define some large search space are known a priori, butfor which the best search strategy is not known. Many optimization problems fallinto this class, such as the problems of scheduling and controlling manufacturingprocesses where the available manufacturing steps are well understood, but thebest strategy for sequencing them is not. 
Given this setting where we must learn to choose among the legal moves, 
the most obvious choice for the type of information tobe learned isa program, 
or function, that chooses the best move for any given board state. Let us call thisfunction ChooseMove and use the notation ChooseMove : B -+ Mto indicatethat this function accepts as input any board from the set of legal board states Band produces as output some move from the set of legal moves M. Throughoutour discussion of machine learning we will find it useful to reduce the problemof improving performance Pat task Tto the problem of learning some particu- 
lar targetfunction such as ChooseMove. The choice of the target function willtherefore bea key design choice. 
Although ChooseMove isan obvious choice for the target function in ourexample, this function will turn out tobe very difficult to learn given the kind ofin- 
direct training experience available to our system. An alternative target function- 
and one that will turn out tobe easier to learn in this setting-isan evaluationfunction that assigns a numerical score to any given board state. Let us call thistarget function V and again use the notation V : B + 8 to denote that V mapsany legal board state from the set Bto some real value (we use 8 to denote the setof real numbers). We intend for this target function Vto assign higher scores tobetter board states. If the system can successfully learn such a target function V, 
then it can easily use itto select the best move from any current board position. 
This can be accomplished by generating the successor board state produced byevery legal move, then using Vto choose the best successor state and thereforethe best legal move. 
What exactly should be the value of the target function V for any givenboard state? Of course any evaluation function that assigns higher scores to betterboard states will do. Nevertheless, we will find it useful to define one particulartarget function V among the many that produce optimal play. Aswe shall see, 
this will make it easier to design a training algorithm. Let us therefore define thetarget value V(b) for an arbitrary board state bin B, as follows: 
1. ifb isa final board state that is won, then V(b) = 1002. ifb isa final board state that is lost, then V(b) = -1003. ifb isa final board state that is drawn, then V(b) = 04. ifb isa not a final state in the game, then V(b) = V(bl), where b' is the bestfinal board state that can be achieved starting from b and playing optimallyuntil the end of the game (assuming the opponent plays optimally, as well). 
While this recursive definition specifies a value ofV(b) for every boardstate b, this definition is not usable by our checkers player because itis notefficiently computable. Except for the trivial cases (cases 1-3) in which the gamehas already ended, determining the value ofV(b) for a particular board staterequires (case 4) searching ahead for the optimal line of play, all the way tothe end of the game! Because this definition is not efficiently computable by ourcheckers playing program, we say that itis a nonoperational definition. The goalof learning in this case isto discover an operational description ofV; that is, adescription that can be used by the checkers-playing program to evaluate statesand select moves within realistic time bounds. 
Thus, we have reduced the learning task in this case to the problem ofdiscovering an operational description of the ideal targetfunction V. It may bevery difficult in general to learn such an operational form ofV perfectly. In fact, 
we often expect learning algorithms to acquire only some approximation to thetarget function, and for this reason the process of learning the target functionis often called function approximation. In the current discussion we will use thesymbol ? to refer to the function that is actually learned by our program, todistinguish it from the ideal target function V. 
1.23 Choosing a Representation for the Target FunctionNow that we have specified the ideal target function V, we must choose a repre- 
sentation that the learning program will use to describe the function c that it willlearn. As with earlier design choices, we again have many options. We could, 
for example, allow the program to represent using a large table with a distinctentry specifying the value for each distinct board state. Orwe could allow it torepresent using a collection of rules that match against features of the boardstate, ora quadratic polynomial function of predefined board features, oran arti- 
ficial neural network. In general, this choice of representation involves a crucialtradeoff. On one hand, we wish to pick a very expressive representation to allowrepresenting as close an approximation as possible to the ideal target function V. 
On the other hand, the more expressive the representation, the more training datathe program will require in order to choose among the alternative hypotheses itcan represent. To keep the discussion brief, let us choose a simple representation: 
for any given board state, the function c will be calculated asa linear combinationof the following board features: 
0 xl: the number of black pieces on the boardx2: the number of red pieces on the board0 xs: the number of black kings on the board0 x4: the number of red kings on the boardCHAPTER I INTRODUCTION 9x5: the number of black pieces threatened by red (i.e., which can be capturedon red's next turn) 
X6: the number of red pieces threatened by blackThus, our learning program will represent c(b) asa linear function of theformwhere wo through W6 are numerical coefficients, or weights, tobe chosen by thelearning algorithm. Learned values for the weights wl through W6 will determinethe relative importance of the various board features in determining the value ofthe board, whereas the weight wo will provide an additive constant to the boardvalue. 
To summarize our design choices thus far, we have elaborated the originalformulation of the learning problem by choosing a type of training experience, 
a target function tobe learned, and a representation for this target function. Ourelaborated learning task is nowPartial design ofa checkers learning program: 
Task T: playing checkersPerformance measure P: percent of games won in the world tournamentTraining experience E: games played against itselfTargetfunction: V:Board + 8Targetfunction representationThe first three items above correspond to the specification of the learning task, 
whereas the final two items constitute design choices for the implementation of thelearning program. Notice the net effect of this set of design choices isto reducethe problem of learning a checkers strategy to the problem of learning values forthe coefficients wo through w6 in the target function representation. 
1.2.4 Choosing a Function Approximation AlgorithmIn order to learn the target function fwe require a set of training examples, eachdescribing a specific board state b and the training value Vtrain(b) for b. In otherwords, each training example isan ordered pair of the form (b, V',,,i,(b)). Forinstance, the following training example describes a board state bin which blackhas won the game (note x2 = 0 indicates that red has no remaining pieces) andfor which the target function value VZrain(b) is therefore +100. 
10 MACHINE LEARNINGBelow we describe a procedure that first derives such training examples fromthe indirect training experience available to the learner, then adjusts the weightswi to best fit these training examples. 
1.2.4.1 ESTIMATING TRAINING VALUESRecall that according to our formulation of the learning problem, the only traininginformation available to our learner is whether the game was eventually won orlost. On the other hand, we require training examples that assign specific scoresto specific board states. While itis easy to assign a value to board states thatcorrespond to the end of the game, itis less obvious how to assign training valuesto the more numerous intermediate board states that occur before the game's end. 
Of course the fact that the game was eventually won or lost does not necessarilyindicate that every board state along the game path was necessarily good or bad. 
For example, even if the program loses the game, it may still be the case thatboard states occurring early in the game should be rated very highly and that thecause of the loss was a subsequent poor move. 
Despite the ambiguity inherent in estimating training values for intermediateboard states, one simple approach has been found tobe surprisingly successful. 
This approach isto assign the training value of Krain(b) for any intermediate boardstate bto be ?(~uccessor(b)), where ? is the learner's current approximation toV and where Successor(b) denotes the next board state following b for which itis again the program's turn to move (i.e., the board state following the program'smove and the opponent's response). This rule for estimating training values canbe summarized as 
~ulk for estimating training values. 
V,,,i. (b) cc(~uccessor(b)) 
While it may seem strange to use the current version off to estimate trainingvalues that will be used to refine this very same function, notice that we are usingestimates of the value of the Successor(b) to estimate the value of board state b. In- 
tuitively, we can see this will make sense if ? tends tobe more accurate for boardstates closer to game's end. In fact, under certain conditions (discussed in Chap- 
ter 13) the approach of iteratively estimating training values based on estimates ofsuccessor state values can be proven to converge toward perfect estimates of Vtrain. 
1.2.4.2 ADJUSTING THE WEIGHTSAll that remains isto specify the learning algorithm for choosing the weights wito^ 
best fit the set of training examples {(b, Vtrain(b))}. Asa first step we must definewhat we mean by the bestfit to the training data. One common approach is todefine the best hypothesis, or set of weights, as that which minimizes the squargerror E between the training values and the values predicted by the hypothesis V. 
Thus, we seek the weights, or equivalently the c, that minimize E for the observedtraining examples. Chapter 6 discusses settings in which minimizing the sum ofsquared errors is equivalent to finding the most probable hypothesis given theobserved training data. 
Several algorithms are known for finding weights ofa linear function thatminimize E defined in this way. In our case, we require an algorithm that willincrementally refine the weights as new training examples become available andthat will be robust to errors in these estimated training values. One such algorithmis called the least mean squares, or LMS training rule. For each observed trainingexample it adjusts the weights a small amount in the direction that reduces theerror on this training example. As discussed in Chapter 4, this algorithm can beviewed as performing a stochastic gradient-descent search through the space ofpossible hypotheses (weight values) to minimize the squared enor E. The LMSalgorithm is defined as follows: 
LMS weight update rule. 
For each training example (b, Kmin(b)) 
Use the current weights to calculate ?(b) 
For each weight mi, update it asHere qis a small constant (e.g., 0.1) that moderates the size of the weight update. 
To get an intuitive understanding for why this weight update rule works, noticethat when the error (Vtrain(b) - c(b)) is zero, no weights are changed. When 
(V,,ain(b) - e(b)) is positive (i.e., when f(b) is too low), then each weight isincreased in proportion to the value of its corresponding feature. This will raisethe value of ?(b), reducing the error. Notice that if the value of some featurexi is zero, then its weight is not altered regardless of the error, so that the onlyweights updated are those whose features actually occur on the training exampleboard. Surprisingly, in certain settings this simple weight-tuning method can beproven to converge to the least squared error approximation to the &,in values 
(as discussed in Chapter 4). 
1.2.5 The Final DesignThe final design of our checkers learning system can be naturally described by fourdistinct program modules that represent the central components in many learningsystems. These four modules, summarized in Figure 1.1, are as follows: 
0 The Performance System is the module that must solve the given per- 
formance task, in this case playing checkers, by using the learned targetfunction(s). It takes an instance ofa new problem (new game) as input andproduces a trace of its solution (game history) as output. In our case, the12 MACHINE LEARNINGExperimentGeneratorNew problem Hypothesis 
(initial game board) f VJPerformance GeneralizerSystemSolution tract Training examples 
(game history) /<bl .Ymtn (blJ >. <bZ. Em(b2) >. ... ICriticFIGURE 1.1Final design of the checkers learning program. 
strategy used by the Performance System to select its next move at each stepis determined by the learned p evaluation function. Therefore, we expectits performance to improve as this evaluation function becomes increasinglyaccurate. 
e The Critic takes as input the history or trace of the game and produces asoutput a set of training examples of the target function. As shown in thediagram, each training example in this case corresponds to some game statein the trace, along with an estimate Vtrai, of the target function value for thisexample. In our example, the Critic corresponds to the training rule givenby Equation (1.1). 
The Generalizer takes as input the training examples and produces an outputhypothesis that is its estimate of the target function. It generalizes from thespecific training examples, hypothesizing a general function that covers theseexamples and other cases beyond the training examples. In our example, theGeneralizer corresponds to the LMS algorithm, and the output hypothesis isthe function f described by the learned weights wo, . . . , W6. 
The Experiment Generator takes as input the current hypothesis (currentlylearned function) and outputs a new problem (i.e., initial board state) for thePerformance System to explore. Its role isto pick new practice problems thatwill maximize the learning rate of the overall system. In our example, theExperiment Generator follows a very simple strategy: It always proposes thesame initial game board to begin a new game. More sophisticated strategiescould involve creating board positions designed to explore particular regionsof the state space. 
Together, the design choices we made for our checkers program producespecific instantiations for the performance system, critic; generalizer, and experi- 
ment generator. Many machine learning systems can-be usefully characterized interms of these four generic modules. 
The sequence of design choices made for the checkers program is summa- 
rized in Figure 1.2. These design choices have constrained the learning task in anumber of ways. We have restricted the type of knowledge that can be acquiredto a single linear evaluation function. Furthermore, we have constrained this eval- 
uation function to depend on only the six specific board features provided. If thetrue target function V can indeed be represented bya linear combination of theseDetermine Typeof Training Experience 1DetermineTarget Function II Determine Representationof Learned Function 
... 
Linear function Artificial neuralof six features network 
/ \ 
I DetermineLearning Algorithm IFIGURE 1.2Sununary of choices in designing the checkers learning program. 
particular features, then our program has a good chance to learn it. If not, then thebest we can hope for is that it will learn a good approximation, since a programcan certainly never learn anything that it cannot at least represent. 
Let us suppose that a good approximation to the true V function can, in fact, 
be represented in this form. The question then arises asto whether this learningtechnique is guaranteed to find one. Chapter 13 provides a theoretical analysisshowing that under rather restrictive assumptions, variations on this approachdo indeed converge to the desired evaluation function for certain types of searchproblems. Fortunately, practical experience indicates that this approach to learningevaluation functions is often successful, even outside the range of situations forwhich such guarantees can be proven. 
Would the program we have designed be able to learn well enough to beatthe human checkers world champion? Probably not. In part, this is because thelinear function representation for ? is too simple a representation to capture wellthe nuances of the game. However, given a more sophisticated representation forthe target function, this general approach can, in fact, be quite successful. Forexample, Tesauro (1992, 1995) reports a similar design for a program that learnsto play the game of backgammon, by learning a very similar evaluation functionover states of the game. His program represents the learned evaluation functionusing an artificial neural network that considers the complete description of theboard state rather than a subset of board features. After training on over one millionself-generated training games, his program was able to play very competitivelywith top-ranked human backgammon players. 
Of course we could have designed many alternative algorithms for thischeckers learning task. One might, for example, simply store the given trainingexamples, then try to find the "closest" stored situation to match any new situation 
(nearest neighbor algorithm, Chapter 8). Orwe might generate a large number ofcandidate checkers programs and allow them to play against each other, keep- 
ing only the most successful programs and further elaborating or mutating thesein a kind of simulated evolution (genetic algorithms, Chapter 9). Humans seemto follow yet a different approach to learning strategies, in which they analyze, 
or explain to themselves, the reasons underlying specific successes and failuresencountered during play (explanation-based learning, Chapter 11). Our design issimply one of many, presented here to ground our discussion of the decisions thatmust go into designing a learning method for a specific class of tasks. 
1.3 PERSPECTIVES AND ISSUES IN MACHINE LEARNINGOne useful perspective on machine learning is that it involves searching a verylarge space of possible hypotheses to determine one that best fits the observed dataand any prior knowledge held by the learner. For example, consider the space ofhypotheses that could in principle be output by the above checkers learner. Thishypothesis space consists of all evaluation functions that can be represented bysome choice of values for the weights wo through w6. The learner's task is thus tosearch through this vast space to locate the hypothesis that is most consistent withthe available training examples. The LMS algorithm for fitting weights achievesthis goal by iteratively tuning the weights, adding a correction to each weighteach time the hypothesized evaluation function predicts a value that differs fromthe training value. This algorithm works well when the hypothesis representationconsidered by the learner defines a continuously parameterized space of potentialhypotheses. 
Many of the chapters in this book present algorithms that search a hypothesisspace defined by some underlying representation (e.g., linear functions, logicaldescriptions, decision trees, artificial neural networks). These different hypothesisrepresentations are appropriate for learning different kinds of target functions. Foreach of these hypothesis representations, the corresponding learning algorithmtakes advantage ofa different underlying structure to organize the search throughthe hypothesis space. 
Throughout this book we will return to this perspective of learning as asearch problem in order to characterize learning methods by their search strategiesand by the underlying structure of the search spaces they explore. We will alsofind this viewpoint useful in formally analyzing the relationship between the sizeof the hypothesis space tobe searched, the number of training examples available, 
and the confidence we can have that a hypothesis consistent with the training datawill correctly generalize to unseen examples. 
1.3.1 Issues in Machine LearningOur checkers example raises a number of generic questions about machine learn- 
ing. The field of machine learning, and much of this book, is concerned withanswering questions such as the following: 
What algorithms exist for learning general target functions from specifictraining examples? In what settings will particular algorithms converge to thedesired function, given sufficient training data? Which algorithms performbest for which types of problems and representations? 
How much training data is sufficient? What general bounds can be foundto relate the confidence in learned hypotheses to the amount of trainingexperience and the character of the learner's hypothesis space? 
When and how can prior knowledge held by the learner guide the processof generalizing from examples? Can prior knowledge be helpful even whenit is only approximately correct? 
What is the best strategy for choosing a useful next training experience, andhow does the choice of this strategy alter the complexity of the learningproblem? 
What is the best way to reduce the learning task to one or more functionapproximation problems? Put another way, what specific functions shouldthe system attempt to learn? Can this process itself be automated? 
How can the learner automatically alter its representation to improve itsability to represent and learn the target function? 
16 MACHINE LEARNING1.4 HOW TO READ THIS BOOKThis book contains an introduction to the primary algorithms and approaches tomachine learning, theoretical results on the feasibility of various learning tasksand the capabilities of specific algorithms, and examples of practical applicationsof machine learning to real-world problems. Where possible, the chapters havebeen written tobe readable in any sequence. However, some interdependenceis unavoidable. If this is being used asa class text, I recommend first coveringChapter 1 and Chapter 2. Following these two chapters, the remaining chapterscan be read in nearly any sequence. A one-semester course in machine learningmight cover the first seven chapters, followed by whichever additional chaptersare of greatest interest to the class. Below isa brief survey of the chapters. 
Chapter 2 covers concept learning based on symbolic or logical representa- 
tions. It also discusses the general-to-specific ordering over hypotheses, andthe need for inductive bias in learning. 
0 Chapter 3 covers decision tree learning and the problem of overfitting thetraining data. It also examines Occam's razor-a principle recommendingthe shortest hypothesis among those consistent with the data. 
0 Chapter 4 covers learning of artificial neural networks, especially the well- 
studied BACKPROPAGATION algorithm, and the general approach of gradientdescent. This includes a detailed example of neural network learning forface recognition, including data and algorithms available over the WorldWide Web. 
0 Chapter 5 presents basic concepts from statistics and estimation theory, fo- 
cusing on evaluating the accuracy of hypotheses using limited samples ofdata. This includes the calculation of confidence intervals for estimatinghypothesis accuracy and methods for comparing the accuracy of learningmethods. 
0 Chapter 6 covers the Bayesian perspective on machine learning, includingboth the use of Bayesian analysis to characterize non-Bayesian learning al- 
gorithms and specific Bayesian algorithms that explicitly manipulate proba- 
bilities. This includes a detailed example applying a naive Bayes classifier tothe task of classifying text documents, including data and software availableover the World Wide Web. 
0 Chapter 7 covers computational learning theory, including the Probably Ap- 
proximately Correct (PAC) learning model and the Mistake-Bound learningmodel. This includes a discussion of the WEIGHTED MAJORITY algorithm forcombining multiple learning methods. 
0 Chapter 8 describes instance-based learning methods, including nearest neigh- 
bor learning, locally weighted regression, and case-based reasoning. 
0 Chapter 9 discusses learning algorithms modeled after biological evolution, 
including genetic algorithms and genetic programming. 
0 Chapter 10 covers algorithms for learning sets of rules, including InductiveLogic Programming approaches to learning first-order Horn clauses. 
0 Chapter 11 covers explanation-based learning, a learning method that usesprior knowledge to explain observed training examples, then generalizesbased on these explanations. 
0 Chapter 12 discusses approaches to combining approximate prior knowledgewith available training data in order to improve the accuracy of learnedhypotheses. Both symbolic and neural network algorithms are considered. 
0 Chapter 13 discusses reinforcement learning-an approach to control learn- 
ing that accommodates indirect or delayed feedback as training information. 
The checkers learning algorithm described earlier in Chapter 1 isa simpleexample of reinforcement learning. 
The end of each chapter contains a summary of the main concepts covered, 
suggestions for further reading, and exercises. Additional updates to chapters, aswell as data sets and implementations of algorithms, are available on the WorldWide Web at http://www.cs.cmu.edu/-tom/mlbook.html. 
1.5 SUMMARY AND FURTHER READINGMachine learning addresses the question of how to build computer programs thatimprove their performance at some task through experience. Major points of thischapter include: 
Machine learning algorithms have proven tobe of great practical value in avariety of application domains. They are especially useful in (a) data miningproblems where large databases may contain valuable implicit regularitiesthat can be discovered automatically (e.g., to analyze outcomes of medicaltreatments from patient databases orto learn general rules for credit worthi- 
ness from financial databases); (b) poorly understood domains where humansmight not have the knowledge needed to develop effective algorithms (e.g., 
human face recognition from images); and (c) domains where the programmust dynamically adapt to changing conditions (e.g., controlling manufac- 
turing processes under changing supply stocks or adapting to the changingreading interests of individuals). 
Machine learning draws on ideas from a diverse set of disciplines, includingartificial intelligence, probability and statistics, computational complexity, 
information theory, psychology and neurobiology, control theory, and phi- 
losophy. 
0 A well-defined learning problem requires a well-specified task, performancemetric, and source of training experience. 
0 Designing a machine learning approach involves a number of design choices, 
including choosing the type of training experience, the target function tobe learned, a representation for this target function, and an algorithm forlearning the target function from training examples. 
18 MACHINE LEARNING0 Learning involves search: searching through a space of possible hypothesesto find the hypothesis that best fits the available training examples and otherprior constraints or knowledge. Much of this book is organized around dif- 
ferent learning methods that search different hypothesis spaces (e.g., spacescontaining numerical functions, neural networks, decision trees, symbolicrules) and around theoretical results that characterize conditions under whichthese search methods converge toward an optimal hypothesis. 
There are a number of good sources for reading about the latest researchresults in machine learning. Relevant journals include Machine Learning, NeuralComputation, Neural Networks, Journal of the American Statistical Association, 
and the IEEE Transactions on Pattern Analysis and Machine Intelligence. Thereare also numerous annual conferences that cover different aspects of machinelearning, including the International Conference on Machine Learning, NeuralInformation Processing Systems, Conference on Computational Learning The- 
ory, International Conference on Genetic Algorithms, International Conferenceon Knowledge Discovery and Data Mining, European Conference on MachineLearning, and others. 
EXERCISES1.1. Give three computer applications for which machine learning approaches seem ap- 
propriate and three for which they seem inappropriate. Pick applications that are notalready mentioned in this chapter, and include a one-sentence justification for each. 
1.2. Pick some learning task not mentioned in this chapter. Describe it informally in aparagraph in English. Now describe itby stating as precisely as possible the task, 
performance measure, and training experience. Finally, propose a target function tobe learned and a target representation. Discuss the main tradeoffs you considered informulating this learning task. 
1.3. Prove that the LMS weight update rule described in this chapter performs a gradientdescent to minimize the squared error. In particular, define the squared error Eas inthe text. Now calculate the derivative ofE with respect to the weight wi, assumingthat ?(b) isa linear function as defined in the text. Gradient descent is achieved byupdating each weight in proportion to -e. Therefore, you must show that the LMStraining rule alters weights in this proportion for each training example it encounters. 
1.4. Consider alternative strategies for the Experiment Generator module of Figure 1.2. 
In particular, consider strategies in which the Experiment Generator suggests newboard positions byGenerating random legal board positions0 Generating a position by picking a board state from the previous game, thenapplying one of the moves that was not executedA strategy of your own designDiscuss tradeoffs among these strategies. Which do you feel would work best if thenumber of training examples was held constant, given the performance measure ofwinning the most games at the world championships? 
1.5. Implement an algorithm similar to that discussed for the checkers problem, but usethe simpler game of tic-tac-toe. Represent the learned function Vas a linear com- 
bination of board features of your choice. To train your program, play it repeatedlyagainst a second copy of the program that uses a fixed evaluation function you cre- 
ate by hand. Plot the percent of games won by your system, versus the number oftraining games played. 
REFERENCESAhn, W., & Brewer, W. F. (1993). Psychological studies of explanation-based learning. InG. DeJong 
(Ed.), Investigating explanation-based learning. Boston: Kluwer Academic Publishers. 
Anderson, J. R. (1991). The place of cognitive architecture in rational analysis. InK. VanLehn (Ed.), 
Architectures for intelligence @p. 1-24). Hillsdale, NJ: Erlbaum. 
Chi, M. T. H., & Bassock, M. (1989). Learning from examples via self-explanations. InL. Resnick 
(Ed.), Knowing, learning, and instruction: Essays in honor of Robert Glaser. Hillsdale, NJ: 
L. Erlbaum Associates. 
Cooper, G., etal. (1997). An evaluation of machine-learning methods for predicting pneumoniamortality. Artificial Intelligence in Medicine, (to appear). 
Fayyad, U. M., Uthurusamy, R. (Eds.) (1995). Proceedings of the First International Conference onKnowledge Discovery and Data Mining. Menlo Park, CA: AAAI Press. 
Fayyad, U. M., Smyth, P., Weir, N., Djorgovski, S. (1995). Automated analysis and exploration ofimage databases: Results, progress, and challenges. Journal of Intelligent Information Systems, 
4, 1-19. 
Laird, J., Rosenbloom, P., & Newell, A. (1986). SOAR: The anatomy ofa general learning mecha- 
nism. Machine Learning, 1(1), 1146. 
Langley, P., & Simon, H. (1995). Applications of machine learning and rule induction. Communica- 
tions of the ACM, 38(1 I), 55-64. 
Lee, K. (1989). Automatic speech recognition: The development of the Sphinx system. Boston: KluwerAcademic Publishers. 
Pomerleau, D. A. (1989). ALVINN: An autonomous land vehicle ina neural network. (TechnicalReport CMU-CS-89-107). Pittsburgh, PA: Carnegie Mellon University. 
Qin, Y., Mitchell, T., & Simon, H. (1992). Using EBG to simulate human learning from examplesand learning by doing. Proceedings of the Florida AI Research Symposium (pp. 235-239). 
Rudnicky, A. I., Hauptmann, A. G., & Lee, K. -F. (1994). Survey of current speech technology inartificial intelligence. Communications of the ACM, 37(3), 52-57. 
Rumelhart, D., Widrow, B., & Lehr, M. (1994). The basic ideas in neural networks. Communicationsof the ACM, 37(3), 87-92. 
Tesauro, G. (1992). Practical issues in temporal difference learning. Machine Learning, 8, 257. 
Tesauro, G. (1995). Temporal difference learning and TD-gammon. Communications of the ACM, 
38(3), 5848. 
Waibel, A,, Hanazawa, T., Hinton, G., Shikano, K., & Lang, K. (1989). Phoneme recognition usingtime-delay neural networks. IEEE Transactions on Acoustics, Speech and Signal Processing, 
37(3), 328-339. 
CHAPTERCONCEPTLEARNINGAND THEGENERAL-TO-SPECIFIC0,RDERINGThe problem of inducing general functions from specific training examples is centralto learning. This chapter considers concept learning: acquiring the definition of ageneral category given a sample of positive and negative training examples of thecategory. Concept learning can be formulated asa problem of searching through apredefined space of potential hypotheses for the hypothesis that best fits the train- 
ing examples. In many cases this search can be efficiently organized by takingadvantage ofa naturally occurring structure over the hypothesis space-a general- 
to-specific ordering of hypotheses. This chapter presents several learning algorithmsand considers situations under which they converge to the correct hypothesis. Wealso examine the nature of inductive learning and the justification by which anyprogram may successfully generalize beyond the observed training data. 
2.1 INTRODUCTIONMuch of learning involves acquiring general concepts from specific training exam- 
ples. People, for example, continually learn general concepts or categories suchas "bird," "car," "situations in which I should study more in order to pass theexam," etc. Each such concept can be viewed as describing some subset ofob- 
jects or events defined over a larger set (e.g., the subset of animals that constituteCHAFER 2 CONCEm LEARNING AND THE GENERAL-TO-SPECIFIC ORDERWG 21birds). Alternatively, each concept can be thought ofas a boolean-valued functiondefined over this larger set (e.g., a function defined over all animals, whose valueis true for birds and false for other animals). 
In this chapter we consider the problem of automatically inferring the generaldefinition of some concept, given examples labeled as+.members or nonmembersof the concept. This task is commonly referred toas concept learning, or approx- 
imating a boolean-valued function from examples. 
Concept learning. Inferring a boolean-valued function from training examples ofits input and output. 
2.2 A CONCEPT LEARNING TASKTo ground our discussion of concept learning, consider the example task of learn- 
ing the target concept "days on which my friend Aldo enjoys his favorite watersport." Table 2.1 describes a set of example days, each represented bya set ofattributes. The attribute EnjoySport indicates whether or not Aldo enjoys hisfavorite water sport on this day. The task isto learn to predict the value ofEnjoySport for an arbitrary day, based on the values of its other attributes. 
What hypothesis representation shall we provide to the learner in this case? 
Let us begin by considering a simple representation in which each hypothesisconsists ofa conjunction of constraints on the instance attributes. In particular, 
let each hypothesis bea vector of six constraints, specifying the values of the sixattributes Sky, AirTemp, Humidity, Wind, Water, and Forecast. For each attribute, 
the hypothesis will either0 indicate bya "?' that any value is acceptable for this attribute, 
0 specify a single required value (e.g., Warm) for the attribute, or0 indicate bya "0" that no value is acceptable. 
If some instance x satisfies all the constraints of hypothesis h, then h clas- 
sifies xas a positive example (h(x) = 1). To illustrate, the hypothesis that Aldoenjoys his favorite sport only on cold days with high humidity (independent ofthe values of the other attributes) is represented by the expression 
(?, Cold, High, ?, ?, ?) 
Example Sky AirTemp Humidity Wind Water Forecast EnjoySport1 Sunny Warm Normal Strong Warm Same Yes2 Sunny Warm High Strong Warm Same Yes3 Rainy Cold High Strong Warm Change No4 Sunny Warm High Strong Cool Change YesTABLE 2.1Positive and negative training examples for the target concept EnjoySport. 
22 MACHINE LEARNINGThe most general hypothesis-that every day isa positive example-is repre- 
sented by 
(?, ?, ?, ?, ?, ?) 
and the most specific possible hypothesis-that no day isa positive example-isrepresented by 
(0,0,0,0,0,0) 
To summarize, the EnjoySport concept learning task requires learning theset of days for which EnjoySport = yes, describing this set bya conjunctionof constraints over the instance attributes. In general, any concept learning taskcan be described by the set of instances over which the target function is defined, 
the target function, the set of candidate hypotheses considered by the learner, andthe set of available training examples. The definition of the EnjoySport conceptlearning task in this general form is given in Table 2.2. 
2.2.1 NotationThroughout this book, we employ the following terminology when discussingconcept learning problems. The set of items over which the concept is definedis called the set of instances, which we denote byX. In the current example, Xis the set of all possible days, each represented by the attributes Sky, AirTemp, 
Humidity, Wind, Water, and Forecast. The concept or function tobe learned iscalled the target concept, which we denote byc. In general, c can be any boolean- 
valued function defined over the instances X; that is, c : X + {O, 1). In the currentexample, the target concept corresponds to the value of the attribute EnjoySport 
(i.e., c(x) = 1 if EnjoySport = Yes, and c(x) = 0 if EnjoySport = No). 
- 
0 Given: 
0 Instances X: Possible days, each described by the attributes0 Sky (with possible values Sunny, Cloudy, and Rainy), 
0 AirTemp (with values Warm and Cold), 
0 Humidity (with values Normal and High), 
0 Wind (with values Strong and Weak), 
0 Water (with values Warm and Cool), and0 Forecast (with values Same and Change). 
0 Hypotheses H: Each hypothesis is described bya conjunction of constraints on the at- 
tributes Sky, AirTemp, Humidity, Wind, Water, and Forecast. The constraints may be "?" 
(any value is acceptable), "0 (no value is acceptable), ora specific value. 
0 Target concept c: EnjoySport : X + (0,l) 
0 Training examples D: Positive and negative examples of the target function (see Table 2.1). 
0 Determine: 
0 A hypothesis hin H such that h(x) = c(x) for all xin X. 
TABLE 2.2The EnjoySport concept learning task. 
When learning the target concept, the learner is presented a set of trainingexamples, each consisting ofan instance x from X, along with its target conceptvalue c(x) (e.g., the training examples in Table 2.1). Instances for which c(x) = 1are called positive examples, or members of the target concept. Instances for whichC(X) = 0 are called negative examples, or nonmembers of the target concept. 
We will often write the ordered pair (x, c(x)) to describe the training exampleconsisting of the instance x and its target concept value c(x). We use the symbolD to denote the set of available training examples. 
Given a set of training examples of the target concept c, the problem facedby the learner isto hypothesize, or estimate, c. We use the symbol Hto denotethe set of all possible hypotheses that the learner may consider regarding theidentity of the target concept. Usually His determined by the human designer'schoice of hypothesis representation. In general, each hypothesis hin H representsa boolean-valued function defined over X; that is, h : X --+ {O, 1). The goal of thelearner isto find a hypothesis h such that h(x) = c(x) for a" xin X. 
2.2.2 The Inductive Learning HypothesisNotice that although the learning task isto determine a hypothesis h identicalto the target concept c over the entire set of instances X, the only informationavailable about cis its value over the training examples. Therefore, inductivelearning algorithms can at best guarantee that the output hypothesis fits the targetconcept over the training data. Lacking any further information, our assumptionis that the best hypothesis regarding unseen instances is the hypothesis that bestfits the observed training data. This is the fundamental assumption of inductivelearning, and we will have much more to say about it throughout this book. Westate it here informally and will revisit and analyze this assumption more formallyand more quantitatively in Chapters 5, 6, and 7. 
The inductive learning hypothesis. Any hypothesis found to approximate the targetfunction well over a sufficiently large set of training examples will also approximatethe target function well over other unobserved examples. 
2.3 CONCEPT LEARNING AS SEARCHConcept learning can be viewed as the task of searching through a large space ofhypotheses implicitly defined by the hypothesis representation. The goal of thissearch isto find the hypothesis that best fits the training examples. Itis importantto note that by selecting a hypothesis representation, the designer of the learningalgorithm implicitly defines the space of all hypotheses that the program canever represent and therefore can ever learn. Consider, for example, the instancesX and hypotheses Hin the EnjoySport learning task. Given that the attributeSky has three possible values, and that AirTemp, Humidity, Wind, Water, andForecast each have two possible values, the instance space X contains exactly3 .2 2 .2 2 .2 = 96 distinct instances. A similar calculation shows that there are5.4-4 -4 -4.4 = 5 120 syntactically distinct hypotheses within H. Notice, however, 
that every hypothesis containing one or more "IZI" symbols represents the emptyset of instances; that is, it classifies every instance as negative. Therefore, thenumber of semantically distinct hypotheses is only 1 + (4.3.3.3.3.3) = 973. OurEnjoySport example isa very simple learning task, with a relatively small, finitehypothesis space. Most practical learning tasks involve much larger, sometimesinfinite, hypothesis spaces. 
Ifwe view learning asa search problem, then itis natural that our studyof learning algorithms will exa~the different strategies for searching the hypoth- 
esis space. We will be particula ly interested in algorithms capable of efficientlysearching very large or infinite hypothesis spaces, to find the hypotheses that bestfit the training data. 
2.3.1 General-to-Specific Ordering of HypothesesMany algorithms for concept learning organize the search through the hypothesisspace by relying ona very useful structure that exists for any concept learningproblem: a general-to-specific ordering of hypotheses. By taking advantage of thisnaturally occurring structure over the hypothesis space, we can design learningalgorithms that exhaustively search even infinite hypothesis spaces without explic- 
itly enumerating every hypothesis. To illustrate the general-to-specific ordering, 
consider the two hypotheseshi = (Sunny, ?, ?, Strong, ?, ?) 
h2 = (Sunny, ?, ?, ?, ?, ?) 
Now consider the sets of instances that are classified positive byhl and by h2. 
Because h2 imposes fewer constraints on the instance, it classifies more instancesas positive. In fact, any instance classified positive byhl will also be classifiedpositive by h2. Therefore, we say that h2 is more general than hl. 
This intuitive "more general than" relationship between hypotheses can bedefined more precisely as follows. First, for any instance xin X and hypothesish inH, we say that x satisjies hif and only ifh(x) = 1. We now define themore-general~han_or.-equal~o relation in terms of the sets of instances that sat- 
isfy the two hypotheses: Given hypotheses hj and hk, hjis more-general-thanm-- 
equaldo hkif and only if any instance that satisfies hk also satisfies hi. 
Definition: Let hj and hkbe boolean-valued functions defined over X. Then hj ismoregeneral-than-or-equal-tohk (written hj 2, hk) if and only ifWe will also find it useful to consider cases where one hypothesis is strictly moregeneral than the other. Therefore, we will say that hjis (strictly) more-generaldhanCHAPTER 2 CONCEPT LEARNING AND THE GENERAL-TO-SPECIFIC ORDERING 25Imtances X Hypotheses HIIA SpecificGeneralt 
iXI= <Sunny, Wan, High, Strong, Cool, Same> hl= <Sunny, ?, ?, Strong, ?, ?> 
x = <Sunny, Warm, High, Light, Warm, Same> 
2h = <Sunny, ?, ?, ?, ?, ?> 
2h 3 = <Sunny, ?, ?, 7, Cool, ?> 
FIGURE 2.1Instances, hypotheses, and the more-general-than relation. The box on the left represents the set Xof all instances, the box on the right the set Hof all hypotheses. Each hypothesis corresponds tosome subset ofX-the subset of instances that it classifies positive. The arrows connecting hypothesesrepresent the more-general-than relation, with the arrow pointing toward the less general hypothesis. 
Note the subset of instances characterized by h2 subsumes the subset characterized byhl, hence h2is more-general-than hl. 
hk (written hj >, hk) if and only if (hjp, hk) A (hk 2, hi). Finally, we willsometimes find the inverse useful and will say that hjis morespecijkthan hkwhen hkis more_general-than hj. 
To illustrate these definitions, consider the three hypotheses hl, h2, andh3 from our Enjoysport example, shown in Figure 2.1. How are these threehypotheses related by the p, relation? As noted earlier, hypothesis h2 is moregeneral than hl because every instance that satisfies hl also satisfies h2. Simi- 
larly, h2 is more general than h3. Note that neither hl nor h3 is more generalthan the other; although the instances satisfied by these two hypotheses intersect, 
neither set subsumes the other. Notice also that the p, and >, relations are de- 
fined independent of the target concept. They depend only on which instancessatisfy the two hypotheses and not on the classification of those instances accord- 
ing to the target concept. Formally, the p, relation defines a partial order overthe hypothesis space H (the relation is reflexive, antisymmetric, and transitive). 
Informally, when we say the structure isa partial (as opposed to total) order, wemean there may be pairs of hypotheses such ashl and h3, such that hl 2, h3 andh3 2, hl. 
The pg relation is important because it provides a useful structure over thehypothesis space H for any concept learning problem. The following sectionspresent concept learning algorithms that take advantage of this partial order toefficiently organize the search for hypotheses that fit the training data. 
1. Initialize hto the most specific hypothesis in H2. For each positive training instance x0 For each attribute constraint a, in hIf the constraint a, is satisfied by xThen do nothingElse replace a, inh by the next more general constraint that is satisfied by x3. Output hypothesis hTABLE 2.3FIND-S Algorithm. 
2.4 FIND-S: FINDING A MAXIMALLY SPECIFIC HYPOTHESISHow can we use the more-general-than partial ordering to organize the search fora hypothesis consistent with the observed training examples? One way isto beginwith the most specific possible hypothesis inH, then generalize this hypothesiseach time it fails to cover an observed positive training example. (We say thata hypothesis "covers" a positive example ifit correctly classifies the example aspositive.) Tobe more precise about how the partial ordering is used, consider theFIND-S algorithm defined in Table 2.3. 
To illustrate this algorithm, assume the learner is given the sequence oftraining examples from Table 2.1 for the EnjoySport task. The first step of FIND- 
Sis to initialize hto the most specific hypothesis in HUpon observing the first training example from Table 2.1, which happens tobe apositive example, it becomes clear that our hypothesis is too specific. In particular, 
none of the "0" constraints inh are satisfied by this example, so each is replacedby the next more general constraint {hat fits the example; namely, the attributevalues for this training example. 
h -+ (Sunny, Warm, Normal, Strong, Warm, Same) 
This his still very specific; it asserts that all instances are negative except forthe single positive training example we have observed. Next, the second trainingexample (also positive in this case) forces the algorithm to further generalize h, 
this time substituting a "?' in place of any attribute value inh that is not satisfiedby the new example. The refined hypothesis in this case ish -+ (Sunny, Warm, ?, Strong, Warm, Same) 
Upon encountering the third training example-in this case a negative exam- 
ple-the algorithm makes no change toh. In fact, the FIND-S algorithm simplyignores every negative example! While this may at first seem strange, notice thatin the current case our hypothesis his already consistent with the new negative ex- 
ample (i-e., h correctly classifies this example as negative), and hence no revisionis needed. In the general case, as long aswe assume that the hypothesis space Hcontains a hypothesis that describes the true target concept c and that the trainingdata contains no errors, then the current hypothesis h can never require a revisionin response toa negative example. To see why, recall that the current hypothesish is the most specific hypothesis inH consistent with the observed positive exam- 
ples. Because the target concept cis also assumed tobe inH and tobe consistentwith the positive training examples, c must be more.general_than-or-equaldo h. 
But the target concept c will never cover a negative example, thus neither willh (by the definition of more-general~han). Therefore, no revision toh will berequired in response to any negative example. 
To complete our trace of FIND-S, the fourth (positive) example leads to afurther generalization ofhh t (Sunny, Warm, ?, Strong, ?, ?) 
The FIND-S algorithm illustrates one way in which the more-generaldhanpartial ordering can be used to organize the search for an acceptable hypothe- 
sis. The search moves from hypothesis to hypothesis, searching from the mostspecific to progressively more general hypotheses along one chain of the partialordering. Figure 2.2 illustrates this search in terms of the instance and hypoth- 
esis spaces. At each step, the hypothesis is generalized only as far as neces- 
sary to cover the new positive example. Therefore, at each stage the hypothesisis the most specific hypothesis consistent with the training examples observedup to this point (hence the name FIND-S). The literature on concept learning isInstances X Hypotheses HspecificGeneral 
* 1 = <Sunny Warm Normal Strong Warm Same>, + h, = <Sunny Warm Normal Strong Warm Same> 
x2 = <Sunny Warm High Strong Warm Same>, + h2 = <Sunny Warm ? Strong Warm Same> 
X3 = <Rainy Cold High Strong Warm Change>, - h = <Sunny Warm ? Strong Warm Same> 3x - <Sunny Warm High Strong Cool Change>, + h - <Sunny Warm ? Strong ? ? > 4- 4 - 
FIGURE 2.2 
'The hypothesis space search performed by FINDS. The search begins (ho) with the most specifichypothesis inH, then considers increasingly general hypotheses (hl through h4) as mandated by thetraining examples. In the instance space diagram, positive training examples are denoted by "+," 
negative by "-," and instances that have not been presented as training examples are denoted by asolid circle. 
populated by many different algorithms that utilize this same more-general-thanpartial ordering to organize the search in one fashion or another. A number ofsuch algorithms are discussed in this chapter, and several others are presented inChapter 10. 
The key property of the FIND-S algorithm is that for hypothesis spaces de- 
scribed by conjunctions of attribute constraints (such asH for the EnjoySporttask), FIND-Sis guaranteed to output the most specific hypothesis within Hthat is consistent with the positive training examples. Its final hypothesis willalso be consistent with the negative examples provided the correct target con- 
cept is contained inH, and provided the training examples are correct. How- 
ever, there are several questions still left unanswered by this learning algorithm, 
such as: 
Has the learner converged to the correct target concept? Although FIND-Swill find a hypothesis consistent with the training data, it has no way todetermine whether it has found the only hypothesis inH consistent withthe data (i.e., the correct target concept), or whether there are many otherconsistent hypotheses as well. We would prefer a learning algorithm thatcould determine whether it had converged and, if not, at least characterizeits uncertainty regarding the true identity of the target concept. 
0 Why prefer the most specific hypothesis? In case there are multiple hypothe- 
ses consistent with the training examples, FIND-S will find the most specific. 
Itis unclear whether we should prefer this hypothesis over, say, the mostgeneral, or some other hypothesis of intermediate generality. 
0 Are the training examples consistent? In most practical learning problemsthere is some chance that the training examples will contain at least someerrors or noise. Such inconsistent sets of training examples can severelymislead FIND-S, given the fact that it ignores negative examples. We wouldprefer an algorithm that could at least detect when the training data isin- 
consistent and, preferably, accommodate such errors. 
0 What if there are several maximally specific consistent hypotheses? In thehypothesis language H for the EnjoySport task, there is always a unique, 
most specific hypothesis consistent with any set of positive examples. How- 
ever, for other hypothesis spaces (discussed later) there can be several maxi- 
mally specific hypotheses consistent with the data. In this case, FIND-S mustbe extended to allow itto backtrack on its choices of how to generalize thehypothesis, to accommodate the possibility that the target concept lies alonga different branch of the partial ordering than the branch it has selected. Fur- 
thermore, we can define hypothesis spaces for which there isno maximallyspecific consistent hypothesis, although this is more ofa theoretical issuethan a practical one (see Exercise 2.7). 
2.5 VERSION SPACES AND THE CANDIDATE-ELIMINATIONALGORITHMThis section describes a second approach to concept learning, the CANDIDATE- 
ELIMINATION algorithm, that addresses several of the limitations of FIND-S. Noticethat although FIND-S outputs a hypothesis from H,that is consistent with thetraining examples, this is just one of many hypotheses from H that might fit thetraining data equally well. The key idea in the CANDIDATE-ELIMINATION algorithmis to output a description of the set of all hypotheses consistent with the train- 
ing examples. Surprisingly, the CANDIDATE-ELIMINATION algorithm computes thedescription of this set without explicitly enumerating all of its members. This isaccomplished by again using the more-general-than partial ordering, this timeto maintain a compact representation of the set of consistent hypotheses and toincrementally refine this representation as each new training example is encoun- 
tered. 
The CANDIDATE-ELIMINATION algorithm has been applied to problems suchas learning regularities in chemical mass spectroscopy (Mitchell 1979) and learn- 
ing control rules for heuristic search (Mitchell etal. 1983). Nevertheless, prac- 
tical applications of the CANDIDATE-ELIMINATION and FIND-S algorithms are lim- 
ited by the fact that they both perform poorly when given noisy training data. 
More importantly for our purposes here, the CANDIDATE-ELIMINATION algorithmprovides a useful conceptual framework for introducing several fundamental is- 
sues in machine learning. In the remainder of this chapter we present the algo- 
rithm and discuss these issues. Beginning with the next chapter, we will ex- 
amine learning algorithms that are used more frequently with noisy trainingdata. 
2.5.1 RepresentationThe CANDIDATE-ELIMINATION algorithm finds all describable hypotheses that areconsistent with the observed training examples. In order to define this algorithmprecisely, we begin with a few basic definitions. First, let us say that a hypothesisis consistent with the training examples ifit correctly classifies these examples. 
Definition: A hypothesis his consistent with a set of training examples Dif andonly ifh(x) = c(x) for each example (x, c(x)) inD. 
Notice the key difference between this definition of consistent and our earlierdefinition of satisfies. An example xis said to satisfy hypothesis h when h(x) = 1, 
regardless of whether xis a positive or negative example of the target concept. 
However, whether such an example is consistent with h depends on the targetconcept, and in particular, whether h(x) = c(x). 
The CANDIDATE-ELIMINATION algorithm represents the set of all hypothesesconsistent with the observed training examples. This subset of all hypotheses iscalled the version space with respect to the hypothesis space H and the trainingexamples D, because it contains all plausible versions of the target concept. 
Dejnition: The version space, denoted VSHVD, with respect to hypothesis space Hand training examples D, is the subset of hypotheses from H consistent with thetraining examples inD. 
VSH,~ = {hE HIConsistent(h, D)] 
2.5.2 The LIST-THEN-ELIMINATE AlgorithmOne obvious way to represent the version space is simply to list all of its members. 
This leads toa simple learning algorithm, which we might call the LIST-THEN- 
ELIMINATE algorithm, defined in Table 2.4. 
The LIST-THEN-ELIMINATE algorithm first initializes the version space to con- 
tain all hypotheses inH, then eliminates any hypothesis found inconsistent withany training example. The version space of candidate hypotheses thus shrinksas more examples are observed, until ideally just one hypothesis remains that isconsistent with all the observed examples. This, presumably, is the desired targetconcept. If insufficient data is available to narrow the version space toa singlehypothesis, then the algorithm can output the entire set of hypotheses consistentwith the observed data. 
In principle, the LIST-THEN-ELIMINATE algorithm can be applied wheneverthe hypothesis space His finite. It has many advantages, including the fact that itis guaranteed to output all hypotheses consistent with the training data. Unfortu- 
nately, it requires exhaustively enumerating all hypotheses inH-an unrealisticrequirement for all but the most trivial hypothesis spaces. 
2.5.3 A More Compact Representation for Version SpacesThe CANDIDATE-ELIMINATION algorithm works on the same principle as the aboveLIST-THEN-ELIMINATE algorithm. However, it employs a much more compact rep- 
resentation of the version space. In particular, the version space is representedby its most general and least general members. These members form general andspecific boundary sets that delimit the version space within the partially orderedhypothesis space. 
The LIST-THEN-ELIMINATE Algorithm1. VersionSpace ca list containing every hypothesis in H2. For each training example, (x, c(x)) 
remove from VersionSpace any hypothesis h for which h(x) # c(x) 
3. Output the list of hypotheses in VersionSpaceTABLE 2.4The LIST-THEN-ELIMINATE algorithm. 
{<Sunny, Warm, ?, Strong, 7, ?> 1 
<Sunny, ?, 7, Strong, 7, ?> <Sunny, Warm, ?. ?, ?, ?> <?, Warm, ?, strbng, ?, ?> 
FIGURE 2.3A version space with its general and specific boundary sets. The version space includes all sixhypotheses shown here, but can be represented more simply byS and G. Arrows indicate instancesof the more-general-than relation. This is the version space for the Enjoysport concept learningproblem and training examples described in Table 2.1. 
To illustrate this representation for version spaces, consider again the En- 
joysport concept learning problem described in Table 2.2. Recall that given thefour training examples from Table 2.1, FIND-S outputs the hypothesish = (Sunny, Warm, ?, Strong, ?, ?) 
In fact, this is just one of six different hypotheses from H that are consistentwith these training examples. All six hypotheses are shown in Figure 2.3. Theyconstitute the version space relative to this set of data and this hypothesis repre- 
sentation. The arrows among these six hypotheses in Figure 2.3 indicate instancesof the more-general~han relation. The CANDIDATE-ELIMINATION algorithm rep- 
resents the version space by storing only its most general members (labeled Gin Figure 2.3) and its most specific (labeled Sin the figure). Given only thesetwo sets S and G, itis possible to enumerate all members of the version spaceas needed by generating the hypotheses that lie between these two sets in thegeneral-to-specific partial ordering over hypotheses. 
Itis intuitively plausible that we can represent the version space in terms ofits most specific and most general members. Below we define the boundary setsG and S precisely and prove that these sets doin fact represent the version space. 
Definition: The general boundary G, with respect to hypothesis space H and trainingdata D, is the set of maximally general members ofH consistent with D. 
G = {gE HIConsistent(g, D) A (-3gf EH)[(gf >, g) A Consistent(gt, D)]] 
Definition: The specific boundary S, with respect to hypothesis space H and trainingdata D, is the set of minimally general (i.e., maximally specific) members of Hconsistent with D. 
Srn {sE H(Consistent(s, D) A (-3s' EH)[(s >, sf) A Consistent(st, D)]) 
As long as the sets G and S are well defined (see Exercise 2.7), they com- 
pletely specify the version space. In particular, we can show that the version spaceis precisely the set of hypotheses contained inG, plus those contained inS, plusthose that lie between G and Sin the partially ordered hypothesis space. This isstated precisely in Theorem 2.1. 
Theorem 2.1. Version space representation theorem. Let Xbe an arbitrary setof instances and let Hbe a set of boolean-valued hypotheses defined over X. Letc : X + {O, 1) bean arbitrary target concept defined over X, and let Dbe anarbitrary set of training examples {(x, c(x))). For all X, H, c, and D such that S andG are well defined, 
Proof. To prove the theorem it suffices to show that (1) every h satisfying the right- 
hand side of the above expression isin VSH,~ and (2) every member of VSH,~ 
satisfies the right-hand side of the expression. To show (1) let gbe an arbitrarymember ofG, sbe an arbitrary member ofS, and hbe an arbitrary member ofH, 
such that g 2, h 2, s. Then by the definition ofS, s must be satisfied by all positiveexamples inD. Because h 2, s, h must also be satisfied by all positive examples inD. Similarly, by the definition ofG, g cannot be satisfied by any negative examplein D, and because g 2, h, h cannot be satisfied by any negative example inD. 
Because his satisfied by all positive examples inD and byno negative examplesin D, his consistent with D, and therefore his a member of VSH,~. This provesstep (1). The argument for (2) isa bit more complex. It can be proven by assumingsome hin VSH,~ that does not satisfy the right-hand side of the expression, thenshowing that this leads toan inconsistency. (See Exercise 2.6.) 02.5.4 CANDIDATE-ELIMINATION Learning AlgorithmThe CANDIDATE-ELIMINATION algorithm computes the version space containingall hypotheses from H that are consistent with an observed sequence of trainingexamples. It begins by initializing the version space to the set of all hypothesesin H; that is, by initializing the G boundary set to contain the most generalhypothesis in HGo + {(?, ?, ?, ?, ?, ?)} 
and initializing the S boundary set to contain the most specific (least general) 
hypothesisso c- ((@,PI, @,PI, 0,0)1These two boundary sets delimit the entire hypothesis space, because every otherhypothesis inH is both more general than So and more specific than Go. Aseach training example is considered, the S and G boundary sets are generalizedand specialized, respectively, to eliminate from the version space any hypothe- 
ses found inconsistent with the new training example. After all examples havebeen processed, the computed version space contains all the hypotheses consis- 
tent with these examples and only these hypotheses. This algorithm is summarizedin Table 2.5. 
CHAPTER 2 CONCEET LEARNJNG AND THE GENERAL-TO-SPECIFIC ORDERING 33Initialize Gto the set of maximally general hypotheses in HInitialize Sto the set of maximally specific hypotheses in HFor each training example d, do0 Ifd isa positive exampleRemove from G any hypothesis inconsistent with d , 
0 For each hypothesis sin S that is not consistent with d ,- 
0 Remove s from S0 Add toS all minimal generalizations hof s such that0 his consistent with d, and some member ofG is more general than h0 Remove from S any hypothesis that is more general than another hypothesis in S0 Ifd isa negative example0 Remove from S any hypothesis inconsistent with dFor each hypothesis gin G that is not consistent with dRemove g from G0 Add toG all minimal specializations hof g such that0 his consistent with d, and some member ofS is more specific than h0 Remove from G any hypothesis that is less general than another hypothesis in GTABLE 2.5CANDIDATE-ELIMINATION algorithm using version spaces. Notice the duality in how positive andnegative examples influence S and G. 
Notice that the algorithm is specified in terms of operations such as comput- 
ing minimal generalizations and specializations of given hypotheses, and identify- 
ing nonrninimal and nonmaximal hypotheses. The detailed implementation of theseoperations will depend, of course, on the specific representations for instances andhypotheses. However, the algorithm itself can be applied to any concept learn- 
ing task and hypothesis space for which these operations are well-defined. In thefollowing example trace of this algorithm, we see how such operations can beimplemented for the representations used in the EnjoySport example problem. 
2.5.5 An Illustrative ExampleFigure 2.4 traces the CANDIDATE-ELIMINATION algorithm applied to the first twotraining examples from Table 2.1. As described above, the boundary sets are firstinitialized toGo and So, the most general and most specific hypotheses inH, 
respectively. 
When the first training example is presented (a positive example in thiscase), the CANDIDATE-ELIMINATION algorithm checks the S boundary and findsthat itis overly specific-it fails to cover the positive example. The boundary istherefore revised by moving itto the least more general hypothesis that coversthis new example. This revised boundary is shown as S1 in Figure 2.4. Noup- 
date of the G boundary is needed in response to this training example becauseGo correctly covers this example. When the second training example (also pos- 
itive) is observed, it has a similar effect of generalizing S further to S2, leavingG again unchanged (i.e., G2 = GI = GO). Notice the processing of these first34 MACHINE LEARNINGS 1 : {<Sunny, Warm, Normal, Strong, Warm, Same> } 
1Training examples: 
t1. <Sunny, Warm, Normal, Strong, Warm, Same>, Enjoy Sport = Yes2. <Sunny, Warm, High, Strong, Warm, Same>, Enjoy Sport = YesS2 : 
FIGURE 2.4CANDIDATE-ELIMINATION Trace 1. So and Go are the initial boundary sets corresponding to the mostspecific and most general hypotheses. Training examples 1 and 2 force the S boundary to becomemore general, asin the FIND-S algorithm. They have no effect on the G boundary. 
{<Sunny, Warm, ?, Strong, Warm, Same>} 
two positive examples is very similar to the processing performed by the FIND-Salgorithm. 
As illustrated by these first two steps, positive training examples may forcethe S boundary of the version space to become increasingly general. Negativetraining examples play the complimentary role of forcing the G boundary tobecome increasingly specific. Consider the third training example, shown in Fig- 
ure 2.5. This negative example reveals that the G boundary of the version spaceis overly general; that is, the hypothesis inG incorrectly predicts that this newexample isa positive example. The hypothesis in the G boundary must thereforebe specialized until it correctly classifies this new negative example. As shown inFigure 2.5, there are several alternative minimally more specific hypotheses. Allof these become members of the new G3 boundary set. - 
Given that there are six attributes that could be specified to specialize G2, 
why are there only three new hypotheses in G3? For example, the hypothesish = (?, ?, Normal, ?, ?, ?) isa minimal specialization of G2 that correctly la- 
bels the new example asa negative example, but itis not included inGg. Thereason this hypothesis is excluded is that itis inconsistent with the previouslyencountered positive examples. The algorithm determines this simply by notingthat his not more general than the current specific boundary, Sz. In fact, the Sboundary of the version space forms a summary of the previously encounteredpositive examples that can be used to determine whether any given hypothesisCHmR 2 CONCEPT LEARNING AND THE GENERAL-TO-SPECIFIC ORDERING 35s2 9 s 3 : 
Training Example: 
( <Sunny, Wann, ?. Strong, Warn Same> )] 
G 3: 
3. <Rainy, Cold, High, Strong, Warm, Change>, EnjoySporkNo 
(<Sunny, ?, ?, ?, ?, ?> <?, Wann, ?, ?, ?, ?> <?, ?, ?, ?, ?, Same>} 
FIGURE 2.5CANDIDATE-ELMNATION Trace 2. Training example 3 isa negative example that forces the G2boundary tobe specialized to G3. Note several alternative maximally general hypotheses are includedin Gj. 
is consistent with these examples. Any hypothesis more general than S will, bydefinition, cover any example that S covers and thus will cover any past positiveexample. Ina dual fashion, the G boundary summarizes the information frompreviously encountered negative examples. Any hypothesis more specific than Gis assured tobe consistent with past negative examples. This is true because anysuch hypothesis, by definition, cannot cover examples that G does not cover. 
The fourth training example, as shown in Figure 2.6, further generalizes theS boundary of the version space. It also results in removing one member of the Gboundary, because this member fails to cover the new positive example. This lastaction results from the first step under the condition "Ifd isa positive example" 
in the algorithm shown in Table 2.5. To understand the rationale for this step, it isuseful to consider why the offending hypothesis must be removed from G. Noticeit cannot be specialized, because specializing it would not make it cover the newexample. It also cannot be generalized, because by the definition ofG, any moregeneral hypothesis will cover at least one negative training example. Therefore, 
the hypothesis must be dropped from the G boundary, thereby removing an entirebranch of the partial ordering from the version space of hypotheses remainingunder consideration. 
After processing these four examples, the boundary sets S4 and G4 delimitthe version space of all hypotheses consistent with the set of incrementally ob- 
served training examples. The entire version space, including those hypothesesA 
'32: I<?, ?, ?, ?, ?,?>IS 3: {<Sunny, Warm, ?, Strong, Warm, Same>) 
IS 4: I ( <Sunny, Warm ?, Strong, ?, ?> ) ITraining Example: 
4. <Sunny, Warm, High, Strong, Cool, Change>, EnjoySport = YesFIGURE 2.6CANDIDATE-ELIMINATION Trace 3. The positive training example generalizes the S boundary, fromS3 to S4. One member ofGg must also be deleted, because itis no longer more general than the S4boundary. 
bounded by S4 and G4, is shown in Figure 2.7. This learned version space isindependent of the sequence in which the training examples are presented (be- 
cause in the end it contains all hypotheses consistent with the set of examples). 
As further training data is encountered, the S and G boundaries will move mono- 
tonically closer to each other, delimiting a smaller and smaller version space ofcandidate hypotheses. 
<Sunny, ?, ?, Strong, ?, ?> <Sunny, Warm, ?, ?, ?, ?> <?, Warm, ?, Strong, ?, ?> 
s4: 
{<Sunny, ?, ?, ?, ?, ?>, <?, Warm, ?, ?, ?, ?>) 
{<Sunny, Warm, ?, Strong, ?, ?>) 
FIGURE 2.7The final version space for the EnjoySport concept learning problem and training examples describedearlier. 
CH.4PTF.R 2 CONCEFT LEARNING AND THE GENERAL-TO-SPECIFIC ORDERING 372.6 REMARKS ON VERSION SPACES AND CANDIDATE-ELIMINATION2.6.1 Will the CANDIDATE-ELIMINATION Algorithm Converge to theCorrect Hypothesis? 
The version space learned by the CANDIDATE-ELIMINATION algorithm will con- 
verge toward the hypothesis that correctly describes the target concept, provided 
(1) there are no errors in the training examples, and (2) there is some hypothesisin H that correctly describes the target concept. In fact, as new training examplesare observed, the version space can be monitored to determine the remaining am- 
biguity regarding the true target concept and to determine when sufficient trainingexamples have been observed to unambiguously identify the target concept. Thetarget concept is exactly learned when the S and G boundary sets converge to asingle, identical, hypothesis. 
What will happen if the training data contains errors? Suppose, for example, 
that the second training example above is incorrectly presented asa negativeexample instead ofa positive example. Unfortunately, in this case the algorithmis certain to remove the correct target concept from the version space! Because, 
it will remove every hypothesis that is inconsistent with each training example, itwill eliminate the true target concept from the version space as soon as this falsenegative example is encountered. Of course, given sufficient additional trainingdata the learner will eventually detect an inconsistency by noticing that the S andG boundary sets eventually converge toan empty version space. Such an emptyversion space indicates that there isno hypothesis inH consistent with all observedtraining examples. A similar symptom will appear when the training examples arecorrect, but the target concept cannot be described in the hypothesis representation 
(e.g., if the target concept isa disjunction of feature attributes and the hypothesisspace supports only conjunctive descriptions). We will consider such eventualitiesin greater detail later. For now, we consider only the case in which the trainingexamples are correct and the true target concept is present in the hypothesis space. 
2.6.2 What Training Example Should the Learner Request Next? 
Upto this point we have assumed that training examples are provided to thelearner by some external teacher. Suppose instead that the learner is allowed toconduct experiments in which it chooses the next instance, then obtains the correctclassification for this instance from an external oracle (e.g., nature ora teacher). 
This scenario covers situations in which the learner may conduct experiments innature (e.g., build new bridges and allow nature to classify them as stable orunstable), orin which a teacher is available to provide the correct classification 
(e.g., propose a new bridge and allow the teacher to suggest whether or not it willbe stable). We use the term query to refer to such instances constructed by thelearner, which are then classified byan external oracle. 
Consider again the version space learned from the four training examplesof the Enjoysport concept and illustrated in Figure 2.3. What would bea goodquery for the learner to pose at this point? What isa good query strategy ingeneral? Clearly, the learner should attempt to discriminate among the alternativecompeting hypotheses in its current version space. Therefore, it should choosean instance that would be classified positive by some of these hypotheses, butnegative by others. One such instance is 
(Sunny, Warm, Normal, Light, Warm, Same) 
Note that this instance satisfies three of the six hypotheses in the currentversion space (Figure 2.3). If the trainer classifies this instance asa positive ex- 
ample, the S boundary of the version space can then be generalized. Alternatively, 
if the trainer indicates that this isa negative example, the G boundary can then bespecialized. Either way, the learner will succeed in learning more about the trueidentity of the target concept, shrinking the version space from six hypotheses tohalf this number. 
In general, the optimal query strategy for a concept learner isto generateinstances that satisfy exactly half the hypotheses in the current version space. 
When this is possible, the size of the version space is reduced by half with eachnew example, and the correct target concept can therefore be found with onlyrlog2JVS11 experiments. The situation is analogous to playing the game twentyquestions, in which the goal isto ask yes-no questions to determine the correcthypothesis. The optimal strategy for playing twenty questions isto ask questionsthat evenly split the candidate hypotheses into sets that predict yes and no. Whilewe have seen that itis possible to generate an instance that satisfies preciselyhalf the hypotheses in the version space of Figure 2.3, in general it may not bepossible to construct an instance that matches precisely half the hypotheses. Insuch cases, a larger number of queries may be required than rlog21VS(1. 
2.6.3 How Can Partially Learned Concepts Be Used? 
Suppose that no additional training examples are available beyond the four inour example above, but that the learner is now required to classify new instancesthat it has not yet observed. Even though the version space of Figure 2.3 stillcontains multiple hypotheses, indicating that the target concept has not yet beenfully learned, itis possible to classify certain examples with the same degree ofconfidence asif the target concept had been uniquely identified. To illustrate, 
suppose the learner is asked to classify the four new instances shown inTa- 
ble 2.6. 9Note that although instance A was not among the training examples, it isclassified asa positive instance by every hypothesis in the current version space 
(shown in Figure 2.3). Because the hypotheses in the version space unanimouslyagree that this isa positive instance, the learner can classify instance Aas positivewith the same confidence it would have ifit had already converged to the single, 
correct target concept. Regardless of which hypothesis in the version space iseventually found tobe the correct target concept, itis already clear that it willclassify instance Aas a positive example. Notice furthermore that we need notenumerate every hypothesis in the version space in order to test whether eachCHAPTER 2 CONCEPT LEARNING AND THE GENERAL-TO-SPECIFIC ORDERING 39Instance Sky AirTemp Humidity Wind Water Forecast EnjoySport 
- 
A Sunny Warm Normal Strong Cool Change ? 
B Rainy Cold Normal Light Warm Same ? 
C Sunny Warm Normal Light Warm Same ? 
D Sunny Cold Normal Strong Warm Same ? 
TABLE 2.6New instances tobe classified. 
classifies the instance as positive. This condition will be met if and only if theinstance satisfies every member ofS (why?). The reason is that every other hy- 
pothesis in the version space isat least as general as some member ofS. By ourdefinition of more-general~han, if the new instance satisfies all members ofS itmust also satisfy each of these more general hypotheses. 
Similarly, instance Bis classified asa negative instance by every hypothesisin the version space. This instance can therefore be safely classified as negative, 
given the partially learned concept. An efficient test for this condition is that theinstance satisfies none of the members ofG (why?). 
Instance C presents a different situation. Half of the version space hypothesesclassify itas positive and half classify itas negative. Thus, the learner cannotclassify this example with confidence until further training examples are available. 
Notice that instance Cis the same instance presented in the previous section asan optimal experimental query for the learner. This isto be expected, becausethose instances whose classification is most ambiguous are precisely the instanceswhose true classification would provide the most new information for refining theversion space. 
Finally, instance Dis classified as positive by two of the version spacehypotheses and negative by the other four hypotheses. In this case we have lessconfidence in the classification than in the unambiguous cases of instances Aand B. Still, the vote isin favor ofa negative classification, and one approachwe could take would beto output the majority vote, perhaps with a confidencerating indicating how close the vote was. Aswe will discuss in Chapter 6, if weassume that all hypotheses inH are equally probable a priori, then such a voteprovides the most probable classification of this new instance. Furthermore, theproportion of hypotheses voting positive can be interpreted as the probability thatthis instance is positive given the training data. 
2.7 INDUCTIVE BIASAs discussed above, the CANDIDATE-ELIMINATION algorithm will converge towardthe true target concept provided itis given accurate training examples and pro- 
vided its initial hypothesis space contains the target concept. What if the targetconcept is not contained in the hypothesis space? Can we avoid this difficulty byusing a hypothesis space that includes every possible hypothesis? How does thesize of this hypothesis space influence the ability of the algorithm to generalizeto unobserved instances? How does the size of the hypothesis space influence thenumber of training examples that must be observed? These are fundamental ques- 
tions for inductive inference in general. Here we examine them in the context ofthe CANDIDATE-ELIMINATION algorithm. Aswe shall see, though, the conclusionswe draw from this analysis will apply to any concept learning system that outputsany hypothesis consistent with the training data. 
2.7.1 A Biased Hypothesis SpaceSuppose we wish to assure that the hypothesis space contains the unknown tar- 
get concept. The obvious solution isto enrich the hypothesis space to includeevery possible hypothesis. To illustrate, consider again the EnjoySpor t example inwhich we restricted the hypothesis space to include only conjunctions of attributevalues. Because of this restriction, the hypothesis space is unable to representeven simple disjunctive target concepts such as "Sky = Sunny or Sky = Cloudy." 
In fact, given the following three training examples of this disjunctive hypothesis, 
our algorithm would find that there are zero hypotheses in the version space. 
Example Sky AirTemp Humidity Wind Water Forecast EnjoySport1 Sunny Warm Normal Strong Cool Change Yes2 Cloudy Warm Normal Strong Cool Change Yes3 Rainy Warm Normal Strong Cool Change NoTo see why there are no hypotheses consistent with these three examples, 
note that the most specific hypothesis consistent with the first two examples andrepresentable in the given hypothesis space H isS2 : (?, Warm, Normal, Strong, Cool, Change) 
This hypothesis, although itis the maximally specific hypothesis from H that isconsistent with the first two examples, is already overly general: it incorrectlycovers the third (negative) training example. The problem is that we have biasedthe learner to consider only conjunctive hypotheses. In this case we require a moreexpressive hypothesis space. 
2.7.2 An Unbiased LearnerThe obvious solution to the problem of assuring that the target concept isin thehypothesis space His to provide a hypothesis space capable of representing everyteachable concept; that is, itis capable of representing every possible subset of theinstances X. In general, the set of all subsets ofa set Xis called thepowerset ofX. 
In the EnjoySport learning task, for example, the size of the instance spaceX of days described by the six available attributes is 96. How many possibleconcepts can be defined over this set of instances? In other words, how large isthe power set ofX? In general, the number of distinct subsets that can be definedover a set X containing 1x1 elements (i.e., the size of the power set ofX) is 21'1. 
Thus, there are 296, or approximately distinct target concepts that could bedefined over this instance space and that our learner might be called upon to learn. 
Recall from Section 2.3 that our conjunctive hypothesis space is able to representonly 973 of these-a very biased hypothesis space indeed! 
Let us reformulate the Enjoysport learning task inan unbiased way bydefining a new hypothesis space H' that can represent every subset of instances; 
that is, let H' correspond to the power set ofX. One way to define such anH' is toallow arbitrary disjunctions, conjunctions, and negations of our earlier hypotheses. 
For instance, the target concept "Sky = Sunny or Sky = Cloudy" could then bedescribed as 
(Sunny, ?, ?, ?, ?, ?) v (Cloudy, ?, ?, ?, ?, ?) 
Given this hypothesis space, we can safely use the CANDIDATE-ELIMINATIONalgorithm without worrying that the target concept might not be expressible. How- 
ever, while this hypothesis space eliminates any problems of expressibility, itun- 
fortunately raises a new, equally difficult problem: our concept learning algorithmis now completely unable to generalize beyond the observed examples! To seewhy, suppose we present three positive examples (xl, x2, x3) and two negative ex- 
amples (x4, x5) to the learner. At this point, the S boundary of the version spacewill contain the hypothesis which is just the disjunction of the positive examplesbecause this is the most specific possible hypothesis that covers these three exam- 
ples. Similarly, the G boundary will consist of the hypothesis that rules out onlythe observed negative examplesThe problem here is that with this very expressive hypothesis representation, 
the S boundary will always be simply the disjunction of the observed positiveexamples, while the G boundary will always be the negated disjunction of theobserved negative examples. Therefore, the only examples that will be unambigu- 
ously classified byS and G are the observed training examples themselves. Inorder to converge toa single, final target concept, we will have to present everysingle instance inX asa training example! 
It might at first seem that we could avoid this difficulty by simply using thepartially learned version space and by taking a vote among the members of theversion space as discussed in Section 2.6.3. Unfortunately, the only instances thatwill produce a unanimous vote are the previously observed training examples. For, 
all the other instances, taking a vote will be futile: each unobserved instance willbe classified positive by precisely half the hypotheses in the version space andwill be classified negative by the other half (why?). To see the reason, note thatwhen His the power set ofX and xis some previously unobserved instance, 
then for any hypothesis hin the version space that covers x, there will be anoQerhypothesis h' in the power set that is identical toh except for its classification ofx. And of course ifh isin the version space, then h' will beas well, because itagrees with hon all the observed training examples. 
2.7.3 The Futility of Bias-Free LearningThe above discussion illustrates a fundamental property of inductive inference: 
a learner that makes noa priori assumptions regarding the identity of the tar- 
get concept has no rational basis for classifying any unseen instances. In fact, 
the only reason that the CANDIDATE-ELIMINATION algorithm was able to gener- 
alize beyond the observed training examples in our original formulation of theEnjoySport task is that it was biased by the implicit assumption that the targetconcept could be represented bya conjunction of attribute values. In cases wherethis assumption is correct (and the training examples are error-free), its classifica- 
tion of new instances will also be correct. If this assumption is incorrect, however, 
itis certain that the CANDIDATE-ELIMINATION algorithm will rnisclassify at leastsome instances from X. 
Because inductive learning requires some form of prior assumptions, orinductive bias, we will find it useful to characterize different learning approachesby the inductive biast they employ. Let us define this notion of inductive biasmore precisely. The key idea we wish to capture here is the policy by which thelearner generalizes beyond the observed training data, to infer the classificationof new instances. Therefore, consider the general setting in which an arbitrarylearning algorithm Lis provided an arbitrary set of training data D, = {(x, c(x))} 
of some arbitrary target concept c. After training, Lis asked to classify a newinstance xi. Let L(xi, D,) denote the classification (e.g., positive or negative) thatL assigns toxi after learning from the training data D,. We can describe thisinductive inference step performed byL as followswhere the notation y + z indicates that zis inductively inferred from y. Forexample, ifwe take Lto be the CANDIDATE-ELIMINATION algorithm, D, to bethe training data from Table 2.1, and xito be the fist instance from Table 2.6, 
then the inductive inference performed in this case concludes that L(xi, D,) = 
(EnjoySport = yes). 
Because Lis an inductive learning algorithm, the result L(xi, D,) that itin- 
fers will not in general be provably correct; that is, the classification L(xi, D,) neednot follow deductively from the training data D, and the description of the newinstance xi. However, itis interesting to ask what additional assumptions could beadded toD, r\xiso that L(xi, D,) would follow deductively. We define the induc- 
tive bias ofL as this set of additional assumptions. More precisely, we define thet~he term inductive bias here is not tobe confused with the term estimation bias commonly used instatistics. Estimation bias will be discussed in Chapter 5. 
CHAFI%R 2 CONCEPT LEARNING AND THE GENERAL-TO-SPECIFIC ORDERING 43inductive bias ofL tobe the set of assumptions B such that for all new instances xi 
(BA D, Axi) FL(xi, D,) 
where the notation yt z indicates that z follows deductively from y (i.e., that zis provable from y). Thus, we define the inductive bias ofa learner as the set ofadditional assumptions B sufficient to justify its inductive inferences as deductiveinferences. To summarize, 
Definition: Consider a concept learning algorithm L for the set of instances X. Letc bean arbitrary concept defined over X, and let D, = ((x, c(x))} bean arbitraryset of training examples ofc. Let L(xi, D,) denote the classification assigned tothe instance xiby L after training on the data D,. The inductive bias ofL is anyminimal set of assertions B such that for any target concept c and correspondingtraining examples Dc 
(Vxi EX)[(BA DcA xi) kL(xi, D,)] (2.1) 
What, then, is the inductive bias of the CANDIDATE-ELIMINATION algorithm? 
To answer this, let us specify L(xi, D,) exactly for this algorithm: given a setof data D,, the CANDIDATE-ELIMINATION algorithm will first compute the versionspace VSH,D,, then classify the new instance xiby a vote among hypotheses in thisversion space. Here let us assume that it will output a classification for xi only ifthis vote among version space hypotheses is unanimously positive or negative andthat it will not output a classification otherwise. Given this definition ofL(xi, D,) 
for the CANDIDATE-ELIMINATION algorithm, what is its inductive bias? Itis simplythe assumption cE H. Given this assumption, each inductive inference performedby the CANDIDATE-ELIMINATION algorithm can be justified deductively. 
To see why the classification L(xi, D,) follows deductively from B = {cEH), together with the data D, and description of the instance xi, consider the fol- 
lowing argument. First, notice that ifwe assume cE H then it follows deductivelythat cE VSH,Dc. This follows from cE H, from the definition of the version spaceVSH,D, as the set of all hypotheses inH that are consistent with D,, and from ourdefinition ofD, = {(x, c(x))} as training data consistent with the target conceptc. Second, recall that we defined the classification L(xi, D,) tobe the unanimousvote of all hypotheses in the version space. Thus, ifL outputs the classificationL(x,, D,), it must be the case the every hypothesis in VSH,~, also produces thisclassification, including the hypothesis cE VSHYDc. Therefore c(xi) = L(xi, D,). 
To summarize, the CANDIDATE-ELIMINATION algorithm defined in this fashion canbe characterized by the following biasInductive bias of CANDIDATE-ELIMINATION algorithm. The target concept c iscontained in the given hypothesis space H. 
Figure 2.8 summarizes the situation schematically. The inductive CANDIDATE- 
ELIMINATION algorithm at the top of the figure takes two inputs: the training exam- 
ples and a new instance tobe classified. At the bottom of the figure, a deductive44 MACHINE LEARNINGInductive systemClassification ofCandidate new instance, or Training examples Elimination "don't know" 
New instance Using HypothesisSpace HEquivalent deductive systemI I Classification ofTraining examplesI new instance, or 
"don't know" 
Theorem ProverAssertion " Hcontainsthe target concept" 
-DP Inductive biasmade explicitFIGURE 2.8Modeling inductive systems by equivalent deductive systems. The input-output behavior of theCANDIDATE-ELIMINATION algorithm using a hypothesis space His identical to that ofa deduc- 
tive theorem prover utilizing the assertion "H contains the target concept." This assertion is thereforecalled the inductive bias of the CANDIDATE-ELIMINATION algorithm. Characterizing inductive systemsby their inductive bias allows modeling them by their equivalent deductive systems. This provides away to compare inductive systems according to their policies for generalizing beyond the observedtraining data. 
theorem prover is given these same two inputs plus the assertion "H contains thetarget concept." These two systems will in principle produce identical outputs forevery possible input set of training examples and every possible new instance inX. Of course the inductive bias that is explicitly input to the theorem prover isonly implicit in the code of the CANDIDATE-ELIMINATION algorithm. Ina sense, itexists only in the eye ofus beholders. Nevertheless, itis a perfectly well-definedset of assertions. 
One advantage of viewing inductive inference systems in terms of theirinductive bias is that it provides a nonprocedural means of characterizing theirpolicy for generalizing beyond the observed data. A second advantage is that itallows comparison of different learners according to the strength of the inductivebias they employ. Consider, for example, the following three learning algorithms, 
which are listed from weakest to strongest bias. 
1. ROTE-LEARNER: Learning corresponds simply to storing each observed train- 
ing example in memory. Subsequent instances are classified by looking themCHAPTER 2 CONCEPT. LEARNING AND THE GENERAL-TO-SPECIFIC ORDERING 45up in memory. If the instance is found in memory, the stored classificationis returned. Otherwise, the system refuses to classify the new instance. 
2. CANDIDATE-ELIMINATION algorithm: New instances are classified only in thecase where all members of the current version space agree on the classifi- 
cation. Otherwise, the system refuses to classify the new instance. 
3. FIND-S: This algorithm, described earlier, finds the most specific hypothesisconsistent with the training examples. It then uses this hypothesis to classifyall subsequent instances. 
The ROTE-LEARNER has no inductive bias. The classifications it providesfor new instances follow deductively from the observed training examples, withno additional assumptions required. The CANDIDATE-ELIMINATION algorithm has astronger inductive bias: that the target concept can be represented in its hypothesisspace. Because it has a stronger bias, it will classify some instances that the ROTE- 
LEARNER will not. Of course the correctness of such classifications will dependcompletely on the correctness of this inductive bias. The FIND-S algorithm hasan even stronger inductive bias. In addition to the assumption that the targetconcept can be described in its hypothesis space, it has an additional inductivebias assumption: that all instances are negative instances unless the opposite isentailed by its other know1edge.tAs we examine other inductive inference methods, itis useful to keep inmind this means of characterizing them and the strength of their inductive bias. 
More strongly biased methods make more inductive leaps, classifying a greaterproportion of unseen instances. Some inductive biases correspond to categoricalassumptions that completely rule out certain concepts, such as the bias "the hy- 
pothesis space H includes the target concept." Other inductive biases merely rankorder the hypotheses by stating preferences such as "more specific hypotheses arepreferred over more general hypotheses." Some biases are implicit in the learnerand are unchangeable by the learner, such as the ones we have considered here. 
In Chapters 11 and 12 we will see other systems whose bias is made explicit asa set of assertions represented and manipulated by the learner. 
2.8 SUMMARY AND FURTHER READINGThe main points of this chapter include: 
Concept learning can be cast asa problem of searching through a largepredefined space of potential hypotheses. 
The general-to-specific partial ordering of hypotheses, which can be definedfor any concept learning problem, provides a useful structure for organizingthe search through the hypothesis space. 
+Notice this last inductive bias assumption involves a kind of default, or nonmonotonic reasoning. 
The FINDS algorithm utilizes this general-to-specific ordering, performinga specific-to-general search through the hypothesis space along one branchof the partial ordering, to find the most specific hypothesis consistent withthe training examples. 
The CANDIDATE-ELIMINATION algorithm utilizes this general-to-specific or- 
dering to compute the version space (the set of all hypotheses consistentwith the training data) by incrementally computing the sets of maximallyspecific (S) and maximally general (G) hypotheses. 
Because the S and G sets delimit the entire set of hypotheses consistent withthe data, they provide the learner with a description of its uncertainty regard- 
ing the exact identity of the target concept. This version space of alternativehypotheses can be examined to determine whether the learner has convergedto the target concept, to determine when the training data are inconsistent, 
to generate informative queries to further refine the version space, and todetermine which unseen instances can be unambiguously classified based onthe partially learned concept. 
Version spaces and the CANDIDATE-ELIMINATION algorithm provide a usefulconceptual framework for studying concept learning. However, this learningalgorithm is not robust to noisy data orto situations in which the unknowntarget concept is not expressible in the provided hypothesis space. Chap- 
ter 10 describes several concept learning algorithms based on the general- 
to-specific ordering, which are robust to noisy data. 
0 Inductive learning algorithms are able to classify unseen examples only be- 
cause of their implicit inductive bias for selecting one consistent hypothesisover another. The bias associated with the CANDIDATE-ELIMINATION algo- 
rithm is that the target concept can be found in the provided hypothesisspace (cE H). The output hypotheses and classifications of subsequent in- 
stances follow deductively from this assumption together with the observedtraining data. 
If the hypothesis space is enriched to the point where there isa hypoth- 
esis corresponding to every possible subset of instances (the power set ofthe instances), this will remove any inductive bias from the CANDIDATE- 
ELIMINATION algorithm. Unfortunately, this also removes the ability to clas- 
sify any instance beyond the observed training examples. An unbiased learnercannot make inductive leaps to classify unseen examples. 
The idea of concept learning and using the general-to-specific ordering havebeen studied for quite some time. Bruner etal. (1957) provided an early studyof concept learning in humans, and Hunt and Hovland (1963) an early effortto automate it. Winston's (1970) widely known Ph.D. dissertation cast conceptlearning asa search involving generalization and specialization operators. Plotkin 
(1970, 1971) provided an early formalization of the more-general-than relation, 
as well as the related notion of 8-subsumption (discussed in Chapter 10). Simonand Lea (1973) give an early account of learning as search through a hypothesisCHAFTER 2 CONCEPT LEARNING AND THE GENERALTO-SPECIFIC ORDEIUNG 47space. Other early concept learning systems include (Popplestone 1969; Michal- 
ski 1973; Buchanan 1974; Vere 1975; Hayes-Roth 1974). A very large numberof algorithms have since been developed for concept learning based on symbolicrepresentations. Chapter 10 describes several more recent algorithms for con- 
cept learning, including algorithms that learn concepts represented in first-orderlogic, algorithms that are robust to noisy training data, and algorithms whoseperformance degrades gracefully if the target concept is not representable in thehypothesis space considered by the learner. 
Version spaces and the CANDIDATE-ELIMINATION algorithm were introducedby Mitchell (1977, 1982). The application of this algorithm to inferring rules ofmass spectroscopy is described in (Mitchell 1979), and its application to learningsearch control rules is presented in (Mitchell etal. 1983). Haussler (1988) showsthat the size of the general boundary can grow exponentially in the number oftraining examples, even when the hypothesis space consists of simple conjunctionsof features. Smith and Rosenbloom (1990) show a simple change to the repre- 
sentation of the G set that can improve complexity in certain cases, and Hirsh 
(1992) shows that learning can be polynomial in the number of examples in somecases when the G set is not stored at all. Subramanian and Feigenbaum (1986) 
discuss a method that can generate efficient queries in certain cases by factoringthe version space. One of the greatest practical limitations of the CANDIDATE- 
ELIMINATION algorithm is that it requires noise-free training data. Mitchell (1979) 
describes an extension that can handle a bounded, predetermined number of mis- 
classified examples, and Hirsh (1990, 1994) describes an elegant extension forhandling bounded noise in real-valued attributes that describe the training ex- 
amples. Hirsh (1990) describes an INCREMENTAL VERSION SPACE MERGING algo- 
rithm that generalizes the CANDIDATE-ELIMINATION algorithm to handle situationsin which training information can be different types of constraints representedusing version spaces. The information from each constraint is represented by aversion space and the constraints are then combined by intersecting the versionspaces. Sebag (1994, 1996) presents what she calls a disjunctive version space ap- 
proach to learning disjunctive concepts from noisy data. A separate version spaceis learned for each positive training example, then new instances are classifiedby combining the votes of these different version spaces. She reports experimentsin several problem domains demonstrating that her approach is competitive withother widely used induction methods such as decision tree learning and k-NEARESTNEIGHBOR. 
EXERCISES2.1. Explain why the size of the hypothesis space in the EnjoySport learning task is973. How would the number of possible instances and possible hypotheses increasewith the addition of the attribute Watercurrent, which can take on the valuesLight, Moderate, or Strong? More generally, how does the number of possibleinstances and hypotheses grow with the addition ofa new attribute A that takes onk possible values? , I2.2. Give the sequence ofS and G boundary sets computed by the CANDIDATE-ELIMINA- 
TION algorithm ifit is given the sequence of training examples from Table 2.1 inreverse order. Although the final version space will be the same regardless of thesequence of examples (why?), the sets S and G computed at intermediate stageswill, of course, depend on this sequence. Can you come up with ideas for orderingthe training examples to minimize the sum of the sizes of these intermediate S andG sets for the H used in the EnjoySport example? 
2.3. Consider again the EnjoySport learning task and the hypothesis space H describedin Section 2.2. Let us define a new hypothesis space H' that consists of all painvisedisjunctions of the hypotheses inH. For example, a typical hypothesis inH' is 
(?, Cold, High, ?, ?, ?) v (Sunny, ?, High, ?, ?, Same) 
Trace the CANDIDATE-ELIMINATION algorithm for the hypothesis space H' given thesequence of training examples from Table 2.1 (i.e., show the sequence ofS and Gboundary sets.) 
2.4. Consider the instance space consisting of integer points in the x, y plane and theset of hypotheses H consisting of rectangles. More precisely, hypotheses are of theform a 5 x 5 b, c 5 y 5 d, where a, b, c, and d can be any integers. 
(a) Consider the version space with respect to the set of positive (+) and negative 
(-) training examples shown below. What is the S boundary of the version spacein this case? Write out the hypotheses and draw them inon the diagram. 
(b) What is the G boundary of this version space? Write out the hypotheses anddraw them in. 
(c) Suppose the learner may now suggest a new x, y instance and ask the trainer forits classification. Suggest a query guaranteed to reduce the size of the versionspace, regardless of how the trainer classifies it. Suggest one that will not. 
(d) Now assume you are a teacher, attempting to teach a particular target concept 
(e.g., 3 5 x 5 5,2 ( y 5 9). What is the smallest number of training examplesyou can provide so that the CANDIDATE-ELIMINATION algorithm will perfectlylearn the target concept? 
2.5. Consider the following sequence of positive and negative training examples describ- 
ing the concept "pairs of people who live in the same house." Each training exampledescribes an ordered pair of people, with each person described by their sex, hairCHAPTER 2 CONCEPT LEARNING AND THE GENERAL-TO-SPECIFIC ORDERING 49color (black, brown, or blonde), height (tall, medium, or short), and nationality (US, 
French, German, Irish, Indian, Japanese, or Portuguese). 
+ ((male brown tall US) (f emale black short US)) 
+ ((male brown short French)( female black short US)) 
- ((female brown tall German)( f emale black short Indian)) 
+ ((male brown tall Irish) (f emale brown short Irish)) 
Consider a hypothesis space defined over these instances, in which each hy- 
pothesis is represented bya pair of Ctuples, and where each attribute constraint maybe a specific value, "?," or "0," just asin the EnjoySport hypothesis representation. 
For example, the hypothesis 
((male ? tall ?)(female ? ? Japanese)) 
represents the set of all pairs of people where the first isa tall male (of any nationalityand hair color), and the second isa Japanese female (of any hair color and height). 
(a) Provide a hand trace of the CANDIDATE-ELIMINATION algorithm learning fromthe above training examples and hypothesis language. In particular, show thespecific and general boundaries of the version space after it has processed thefirst training example, then the second training example, etc. 
(b) How many distinct hypotheses from the given hypothesis space are consistentwith the following single positive training example? 
+ ((male black short Portuguese)(f emale blonde tall Indian)) 
(c) Assume the learner has encountered only the positive example from part (b), 
and that itis now allowed to query the trainer by generating any instance andasking the trainer to classify it. Give a specific sequence of queries that assuresthe learner will converge to the single correct hypothesis, whatever it may be 
(assuming that the target concept is describable within the given hypothesislanguage). Give the shortest sequence of queries you can find. How does thelength of this sequence relate to your answer to question (b)? 
(d) Note that this hypothesis language cannot express all concepts that can be definedover the instances (i.e., we can define sets of positive and negative examples forwhich there isno corresponding describable hypothesis). Ifwe were to enrichthe language so that it could express all concepts that can be defined over theinstance language, then how would your answer to (c) change? 
2.6. Complete the proof of the version space representation theorem (Theorem 2.1). 
Consider a concept learning problem in which each instance isa real number, and inwhich each hypothesis isan interval over the reals. More precisely, each hypothesisin the hypothesis space His of the form a < x < b, where a and b are any realconstants, and x refers to the instance. For example, the hypothesis 4.5 < x < 6.1classifies instances between 4.5 and 6.1 as positive, and others as negative. Explaininformally why there cannot bea maximally specific consistent hypothesis for anyset of positive training examples. Suggest a slight modification to the hypothesisrepresentation so that there will be. 'C50 MACHINE LEARNING2.8. In this chapter, we commented that given an unbiased hypothesis space (the powerset of the instances), the learner would find that each unobserved instance wouldmatch exactly half the current members of the version space, regardless of whichtraining examples had been observed. Prove this. In particular, prove that for anyinstance space X, any set of training examples D, and any instance xE X not presentin D, that ifH is the power set ofX, then exactly half the hypotheses in VSH,D willclassify xas positive and half will classify itas negative. 
2.9. Consider a learning problem where each instance is described bya conjunction ofn boolean attributes a1 . . .a,. Thus, a typical instance would be 
(al = T) A (az = F) A . . . A (a, = T) 
Now consider a hypothesis space Hin which each hypothesis isa disjunction ofconstraints over these attributes. For example, a typical hypothesis would bePropose an algorithm that accepts a sequence of training examples and outputsa consistent hypothesis if one exists. Your algorithm should run in time that ispolynomial inn and in the number of training examples. 
2.10. Implement the FIND-S algorithm. First verify that it successfully produces the trace inSection 2.4 for the Enjoysport example. Now use this program to study the numberof random training examples required to exactly learn the target concept. Implementa training example generator that generates random instances, then classifies themaccording to the target concept: 
(Sunny, Warm, ?, ?, ?, ?) 
Consider training your FIND-S program on randomly generated examples and mea- 
suring the number of examples required before the program's hypothesis is identicalto the target concept. Can you predict the average number of examples required? 
Run the experiment at least 20 times and report the mean number of examples re- 
quired. How do you expect this number to vary with the number of "?" in thetarget concept? How would it vary with the number of attributes used to describeinstances and hypotheses? 
REFERENCESBruner, J. S., Goodnow, J. J., & Austin, G. A. (1957). A study of thinking. New York: John Wiey 
& Sons. 
Buchanan, B. G. (1974). Scientific theory formation by computer. InJ. C. Simon (Ed.), ComputerOriented Learning Processes. Leyden: Noordhoff. 
Gunter, C. A., Ngair, T., Panangaden, P., & Subramanian, D. (1991). The common order-theoreticstructure of version spaces and ATMS's. Proceedings of the National Conference on ArtijicialIntelligence (pp. 500-505). Anaheim. 
Haussler, D. (1988). Quantifying inductive bias: A1 learning algorithms and Valiant's learning frame- 
work. Artijicial Intelligence, 36, 177-221. 
Hayes-Roth, F. (1974). Schematic classification problems and their solution. Pattern Recognition, 6, 
105-113. 
Hirsh, H. (1990). Incremental version space merging: A general framework for concept learning. 
Boston: Kluwer. 
Hirsh, H. (1991). Theoretical underpinnings of version spaces. Proceedings of the 12th IJCAI 
(pp. 665-670). Sydney. 
Hirsh, H. (1994). Generalizing version spaces. Machine Learning, 17(1), 546. 
Hunt, E. G., & Hovland, D. I. (1963). Programming a model of human concept formation. InE. Feigenbaum & J. Feldman (Eds.), Computers and thought (pp. 310-325). New York: Mc- 
Graw Hill. 
Michalski, R. S. (1973). AQVALI1: Computer implementation ofa variable valued logic system VL1and examples of its application to pattern recognition. Proceedings of the 1st International JointConference on Pattern Recognition (pp. 3-17). 
Mitchell, T. M. (1977). Version spaces: A candidate elimination approach to rule learning. FijlhInternational Joint Conference onAI @p. 305-310). Cambridge, MA: MIT Press. 
Mitchell, T. M. (1979). Version spaces: An approach to concept learning, (F'h.D. dissertation). Elec- 
trical Engineering Dept., Stanford University, Stanford, CA. 
Mitchell, T. M. (1982). Generalization as search. ArtQcial Intelligence, 18(2), 203-226. 
Mitchell, T. M., Utgoff, P. E., & Baneji, R. (1983). Learning by experimentation: Acquiring andmodifying problem-solving heuristics. In Michalski, Carbonell, & Mitchell (Eds.), MachineLearning (Vol. 1, pp. 163-190). Tioga Press. 
Plotkin, G. D. (1970). A note on inductive generalization. In Meltzer & Michie (Eds.), MachineIntelligence 5 (pp. 153-163). Edinburgh University Press. 
Plotkin, G. D. (1971). A further note on inductive generalization. In Meltzer & Michie (Eds.), MachineIntelligence 6 (pp. 104-124). Edinburgh University Press. 
Popplestone, R. J. (1969). An experiment in automatic induction. In Meltzer & Michie (Eds.), MachineIntelligence 5 (pp. 204-215). Edinburgh University Press. 
Sebag, M. (1994). Using constraints to build version spaces. Proceedings of the 1994 EuropeanConference on Machine Learning. Springer-Verlag. 
Sebag, M. (1996). Delaying the choice of bias: A disjunctive version space approach. Proceedings ofthe 13th International Conference on Machine Learning (pp. 444-452). San Francisco: MorganKaufmann. 
Simon, H. A,, & Lea, G. (1973). Problem solving and rule induction: A unified view. In Gregg (Ed.), 
Knowledge and Cognition (pp. 105-127). New Jersey: Lawrence Erlbaum Associates. 
Smith, B. D., & Rosenbloom, P. (1990). Incremental non-backtracking focusing: A polynomiallybounded generalization algorithm for version spaces. Proceedings of the 1990 National Con- 
ference on ArtQcial Intelligence (pp. 848-853). Boston. 
Subramanian, D., & Feigenbaum, J. (1986). Factorization in experiment generation. Proceedings ofthe I986 National Conference on ArtQcial Intelligence (pp. 518-522). Morgan Kaufmann. 
Vere, S. A. (1975). Induction of concepts in the predicate calculus. Fourth International Joint Con- 
ference onAI (pp. 281-287). Tbilisi, USSR. 
Winston, P. H. (1970). Learning structural descriptions from examples, (Ph.D. dissertation). [MITTechnical Report AI-TR-2311. 
CHAPTERDECISION TREELEARNINGDecision tree learning is one of the most widely used and practical methods forinductive inference. Itis a method for approximating discrete-valued functions thatis robust to noisy data and capable of learning disjunctive expressions. This chapterdescribes a family of decision tree learning algorithms that includes widely usedalgorithms such as ID3, ASSISTANT, and C4.5. These decision tree learning meth- 
ods search a completely expressive hypothesis space and thus avoid the difficultiesof restricted hypothesis spaces. Their inductive bias isa preference for small treesover large trees. 
3.1 INTRODUCTIONDecision tree learning isa method for approximating discrete-valued target func- 
tions, in which the learned function is represented bya decision tree. Learned treescan also bere-represented as sets ofif-then rules to improve human readability. 
These learning methods are among the most popular of inductive inference algo- 
rithms and have been successfully applied toa broad range of tasks from learningto diagnose medical cases to learning to assess credit risk of loan applicants. 
3.2 DECISION TREE REPRESENTATIONDecision trees classify instances by sorting them down the tree from the root tosome leaf node, which provides the classification of the instance. Each node in thetree specifies a test of some attribute of the instance, and each branch descendingCHAPTER 3 DECISION TREE LEARNING 53Noma1 Strong WeakNo 
\ Yes 
/ No 
\ YesFIGURE 3.1A decision tree for the concept PlayTennis. An example is classified by sorting it through the treeto the appropriate leaf node, then returning the classification associated with this leaf (in this case, 
Yes orNo). This tree classifies Saturday mornings according to whether or not they are suitable forplaying tennis. 
from that node corresponds to one of the possible values for this attribute. Aninstance is classified by starting at the root node of the tree, testing the attributespecified by this node, then moving down the tree branch corresponding to thevalue of the attribute in the given example. This process is then repeated for thesubtree rooted at the new node. 
Figure 3.1 illustrates a typical learned decision tree. This decision tree clas- 
sifies Saturday mornings according to whether they are suitable for playing tennis. 
For example, the instance 
(Outlook = Sunny, Temperature = Hot, Humidity = High, Wind = Strong) 
would be sorted down the leftmost branch of this decision tree and would thereforebe classified asa negative instance (i.e., the tree predicts that PlayTennis = no). 
This tree and the example used in Table 3.2 to illustrate the ID3 learning algorithmare adapted from (Quinlan 1986). 
In general, decision trees represent a disjunction of conjunctions of con- 
straints on the attribute values of instances. Each path from the tree root toa leafcorresponds toa conjunction of attribute tests, and the tree itself toa disjunc- 
tion of these conjunctions. For example, the decision tree shown in Figure 3.1corresponds to the expression 
(Outlook = Sunny A Humidity = Normal) 
V (Outlook = Overcast) 
v (Outlook = Rain A Wind = Weak) 
54 MACHINE LEARNWG3.3 APPROPRIATE PROBLEMS FOR DECISION TREE LEARNINGAlthough a variety of decision tree learning methods have been developed withsomewhat differing capabilities and requirements, decision tree learning is gener- 
ally best suited to problems with the following characteristics: 
Znstances are represented by attribute-value pairs. Instances are described bya fixed set of attributes (e.g., Temperature) and their values (e.g., Hot). Theeasiest situation for decision tree learning is when each attribute takes on asmall number of disjoint possible values (e.g., Hot, Mild, Cold). However, 
extensions to the basic algorithm (discussed in Section 3.7.2) allow handlingreal-valued attributes as well (e.g., representing Temperature numerically). 
The targetfunction has discrete output values. The decision tree in Figure 3.1assigns a boolean classification (e.g., yes orno) to each example. Decisiontree methods easily extend to learning functions with more than two possibleoutput values. A more substantial extension allows learning target functionswith real-valued outputs, though the application of decision trees in thissetting is less common. 
0 Disjunctive descriptions may be required. As noted above, decision treesnaturally represent disjunctive expressions. 
0 The training data may contain errors. Decision tree learning methods arerobust to errors, both errors in classifications of the training examples anderrors in the attribute values that describe these examples. 
0 The training data may contain missing attribute values. Decision tree meth- 
ods can be used even when some training examples have unknown values 
(e.g., if the Humidity of the day is known for only some of the trainingexamples). This issue is discussed in Section 3.7.4. 
Many practical problems have been found to fit these characteristics. De- 
cision tree learning has therefore been applied to problems such as learning toclassify medical patients by their disease, equipment malfunctions by their cause, 
and loan applicants by their likelihood of defaulting on payments. Such problems, 
in which the task isto classify examples into one ofa discrete set of possiblecategories, are often referred toas classijication problems. 
The remainder of this chapter is organized as follows. Section 3.4 presentsthe basic ID3 algorithm for learning decision trees and illustrates its operationin detail. Section 3.5 examines the hypothesis space search performed by thislearning algorithm, contrasting it with algorithms from Chapter 2. Section 3.6characterizes the inductive bias of this decision tree learning algorithm and ex- 
plores more generally an inductive bias called Occam's razor, which correspondsto a preference for the most simple hypothesis. Section 3.7 discusses the issue ofoverfitting the training data, as well as strategies such as rule post-pruning to dealwith this problem. This section also discusses a number of more advanced topicssuch as extending the algorithm to accommodate real-valued attributes, trainingdata with unobserved attributes, and attributes with differing costs. 
CHAPTER 3 DECISION TREE LEARMNG 553.4 THE BASIC DECISION TREE LEARNING ALGORITHMMost algorithms that have been developed for learning decision trees are vari- 
ations ona core algorithm that employs a top-down, greedy search through thespace of possible decision trees. This approach is exemplified by the ID3 algorithm 
(Quinlan 1986) and its successor C4.5 (Quinlan 1993), which form the primaryfocus of our discussion here. In this section we present the basic algorithm fordecision tree learning, corresponding approximately to the ID3 algorithm. In Sec- 
tion 3.7 we consider a number of extensions to this basic algorithm, includingextensions incorporated into C4.5 and other more recent algorithms for decisiontree learning. 
Our basic algorithm, ID3, learns decision trees by constructing them top- 
down, beginning with the question "which attribute should be tested at the rootof the tree?'To answer this question, each instance attribute is evaluated usinga statistical test to determine how well it alone classifies the training examples. 
The best attribute is selected and used as the test at the root node of the tree. 
A descendant of the root node is then created for each possible value of thisattribute, and the training examples are sorted to the appropriate descendant node 
(i.e., down the branch corresponding to the example's value for this attribute). 
The entire process is then repeated using the training examples associated witheach descendant node to select the best attribute to test at that point in the tree. 
This forms a greedy search for an acceptable decision tree, in which the algorithmnever backtracks to reconsider earlier choices. A simplified version of the algo- 
rithm, specialized to learning boolean-valued functions (i.e., concept learning), isdescribed in Table 3.1. 
3.4.1 Which Attribute Is the Best Classifier? 
The central choice in the ID3 algorithm is selecting which attribute to test ateach node in the tree. We would like to select the attribute that is most usefulfor classifying examples. What isa good quantitative measure of the worth ofan attribute? We will define a statistical property, called informution gain, thatmeasures how well a given attribute separates the training examples according totheir target classification. ID3 uses this information gain measure to select amongthe candidate attributes at each step while growing the tree. 
3.4.1.1 ENTROPY MEASURES HOMOGENEITY OF EXAMPLESIn order to define information gain precisely, we begin by defining a measure com- 
monly used in information theory, called entropy, that characterizes the (im)purityof an arbitrary collection of examples. Given a collection S, containing positiveand negative examples of some target concept, the entropy ofS relative to thisboolean classification isID3(Examples, Targetattribute, Attributes) 
Examples are the training examples. Targetattribute is the attribute whose value isto bepredicted by the tree. Attributes isa list of other attributes that may be tested by the learneddecision tree. Returns a decision tree that correctly classiJies the given Examples. 
Create a Root node for the treeIf all Examples are positive, Return the single-node tree Root, with label = + 
If all Examples are negative, Return the single-node tree Root, with label = - 
If Attributes is empty, Return the single-node tree Root, with label = most common value ofTargetattribute in ExamplesOtherwise BeginA t the attribute from Attributes that best* classifies Examples0 The decision attribute for Root c AFor each possible value, vi, ofA, 
Add a new tree branch below Root, corresponding to the test A = vi0 Let Examples,, be the subset of Examples that have value vi for AIf Examples,, is emptyThen below this new branch add a leaf node with label = most commonvalue of Target attribute in ExamplesElse below this new branch add the subtreeID3(Examples,,, Targetattribute, Attributes - (A))) 
EndReturn Root 
* The best attribute is the one with highest information gain, as defined in Equation (3.4). 
TABLE 3.1Summary of the ID3 algorithm specialized to learning boolean-valued functions. ID3 isa greedyalgorithm that grows the tree top-down, at each node selecting the attribute that best classifies thelocal training examples. This process continues until the tree perfectly classifies the training examples, 
or until all attributes have been used. 
where p, is the proportion of positive examples inS and p, is the proportion ofnegative examples inS. In all calculations involving entropy we define 0 log 0 tobe 0. 
To illustrate, suppose Sis a collection of 14 examples of some booleanconcept, including 9 positive and 5 negative examples (we adopt the notation 
[9+, 5-1 to summarize such a sample of data). Then the entropy ofS relative tothis boolean classification isNotice that the entropy is 0 if all members ofS belong to the same class. Forexample, if all members are positive (pe = I), then p, is 0, and Entropy(S) = 
-1 . log2(1) - 0 . log2 0 = -1 . 0 - 0 . log2 0 = 0. Note the entropy is 1 whenthe collection contains an equal number of positive and negative examples. Ifthe collection contains unequal numbers of positive and negative examples, theCHAPTER 3 DECISION TREE LEARNING 57FIGURE 3.2The entropy function relative toa boolean classification, 
0.0 0.5 LOas the proportion, pe, of positive examples variespe between 0 and 1. 
entropy is between 0 and 1. Figure 3.2 shows the form of the entropy functionrelative toa boolean classification, asp, varies between 0 and 1. 
One interpretation of entropy from information theory is that it specifies theminimum number of bits of information needed to encode the classification ofan arbitrary member ofS (i.e., a member ofS drawn at random with uniformprobability). For example, ifp, is 1, the receiver knows the drawn example willbe positive, sono message need be sent, and the entropy is zero. On the other hand, 
ifpe is 0.5, one bit is required to indicate whether the drawn example is positiveor negative. Ifpe is 0.8, then a collection of messages can be encoded using onaverage less than 1 bit per message by assigning shorter codes to collections ofpositive examples and longer codes to less likely negative examples. 
Thus far we have discussed entropy in the special case where the targetclassification is boolean. More generally, if the target attribute can take on cdifferent values, then the entropy ofS relative to this c-wise classification isdefined asC 
Entropy(S) - -pi log, pii=lwhere piis the proportion ofS belonging to class i. Note the logarithm is stillbase 2 because entropy isa measure of the expected encoding length measuredin bits. Note also that if the target attribute can take onc possible values, theentropy can beas large as log, c. 
3.4.1.2 INFORMATION GAIN MEASURES THE EXPECTED REDUCTIONIN ENTROPYGiven entropy asa measure of the impurity ina collection of training examples, 
we can now define a measure of the effectiveness ofan attribute in classifyingthe training data. The measure we will use, called information gain, is simply theexpected reduction in entropy caused by partitioning the examples according tothis attribute. More precisely, the information gain, Gain(S, A) ofan attribute A, 
relative toa collection of examples S, is defined asISVl Gain(S, A) I Entropy(S) - -Entropy (S,) 
IS1 
(3.4) 
veValues(A) 
where Values(A) is the set of all possible values for attribute A, and S, is thesubset ofS for which attribute A has value v (i.e., S, = {sE SIA(s) = v)). Notethe first term in Equation (3.4) is just the entropy of the original collection S, 
and the second term is the expected value of the entropy after Sis partitionedusing attribute A. The expected entropy described by this second term is simplythe sum of the entropies of each subset S,, weighted by the fraction of examplesthat belong toS,. Gain(S, A) is therefore the expected reduction in entropycaused by knowing the value of attribute A. Put another way, Gain(S, A) is theinformation provided about the target &action value, given the value of someother attribute A. The value of Gain(S, A) is the number of bits saved whenencoding the target value ofan arbitrary member ofS, by knowing the value ofattribute A. 
For example, suppose Sis a collection of training-example days described byattributes including Wind, which can have the values Weak or Strong. As before, 
assume Sis a collection containing 14 examples, [9+, 5-1. Of these 14 examples, 
suppose 6 of the positive and 2 of the negative examples have Wind = Weak, andthe remainder have Wind = Strong. The information gain due to sorting theoriginal 14 examples by the attribute Wind may then be calculated asValues(Wind) = Weak, StrongIS, l Gain(S, Wind) = Entropy(S) - -Entropy(S,) 
v~(Weak,Strong] Is1Information gain is precisely the measure used by ID3 to select the best attribute ateach step in growing the tree. The use of information gain to evaluate the relevanceof attributes is summarized in Figure 3.3. In this figure the information gain of twodifferent attributes, Humidity and Wind, is computed in order to determine whichis the better attribute for classifying the training examples shown in Table 3.2. 
CHAPTER 3 DECISION TREE LEARNING 59Which attribute is the best classifier? 
S: [9+,5-IE =0.940HumidityHigh 
[3+,4-I [6t,l-lES.985 ES.592Gain (S, Hurnidiry ) 
S: [9+,5-IES.940wx Strong 
[6+,2-I [3+,3-IES.811 E =1.00Gain (S, Wind) 
= ,940 - (8/14).811 - (6114)l.O 
= ,048FIGURE 3.3Humidity provides greater information gain than Wind, relative to the target classification. Here, Estands for entropy and S for the original collection of examples. Given an initial collection Sof 9positive and 5 negative examples, [9+, 5-1, sorting these by their Humidity produces collections of 
[3+, 4-1 (Humidity = High) and [6+, 1-1 (Humidity = Normal). The information gained by thispartitioning is .151, compared toa gain of only .048 for the attribute Wind. 
3.4.2 An Illustrative ExampleTo illustrate the operation of ID3, consider the learning task represented by thetraining examples of Table 3.2. Here the target attribute PlayTennis, which canhave values yes orno for different Saturday mornings, isto be predicted basedon other attributes of the morning in question. Consider the first step throughDay Outlook Temperature Humidity Wind PlayTennisDl Sunny Hot High Weak NoD2 Sunny Hot High Strong NoD3 Overcast Hot High Weak YesD4 Rain Mild High Weak YesD5 Rain Cool Normal Weak YesD6 Rain Cool Normal Strong NoD7 Overcast Cool Normal Strong YesD8 Sunny Mild High Weak NoD9 Sunny Cool Normal Weak YesDl0 Rain Mild Normal Weak YesDl1 Sunny Mild Normal Strong YesDl2 Overcast Mild High Strong YesDl3 Overcast Hot Normal Weak YesDl4 Rain Mild High Strong NoTABLE 3.2Training examples for the target concept PlayTennis. 
the algorithm, in which the topmost node of the decision tree is created. Whichattribute should be tested first in the tree? ID3 determines the information gain foreach candidate attribute (i.e., Outlook, Temperature, Humidity, and Wind), thenselects the one with highest information gain. The computation of informationgain for two of these attributes is shown in Figure 3.3. The information gainvalues for all four attributes areGain(S, Outlook) = 0.246Gain(S, Humidity) = 0.151Gain(S, Wind) = 0.048Gain(S, Temperature) = 0.029where S denotes the collection of training examples from Table 3.2. 
According to the information gain measure, the Outlook attribute providesthe best prediction of the target attribute, PlayTennis, over the training exam- 
ples. Therefore, Outlook is selected as the decision attribute for the root node, 
and branches are created below the root for each of its possible values (i.e., 
Sunny, Overcast, and Rain). The resulting partial decision tree is shown in Fig- 
ure 3.4, along with the training examples sorted to each new descendant node. 
Note that every example for which Outlook = Overcast is also a positive ex- 
ample of PlayTennis. Therefore, this node of the tree becomes a leaf node withthe classification PlayTennis = Yes. In contrast, the descendants corresponding toOutlook = Sunny and Outlook = Rain still have nonzero entropy, and the decisiontree will be further elaborated below these nodes. 
The process of selecting a new attribute and partitioning the training exam- 
ples is now repeated for each nontenninal descendant node, this time using onlythe training examples associated with that node. Attributes that have been incor- 
porated higher in the tree are excluded, so that any given attribute can appear atmost once along any path through the tree. This process continues for each newleaf node until either of two conditions is met: (1) every attribute has already beenincluded along this path through the tree, or (2) the training examples associatedwith this leaf node all have the same target attribute value (i.e., their entropyis zero). Figure 3.4 illustrates the computations of information gain for the nextstep in growing the decision tree. The final decision tree learned by ID3 from the14 training examples of Table 3.2 is shown in Figure 3.1. 
3.5 HYPOTHESIS SPACE SEARCH IN DECISION TREELEARNINGAs with other inductive learning methods, ID3 can be characterized as searching aspace of hypotheses for one that fits the training examples. The hypothesis spacesearched by ID3 is the set of possible decision trees. ID3 performs a simple-to- 
complex, hill-climbing search through this hypothesis space, beginning with theempty tree, then considering progressively more elaborate hypotheses in search ofa decision tree that correctly classifies the training data. The evaluation function 
{Dl, D2, ..., Dl41P+S-IWhich attribute should be tested here? 
Gain (Ssunnyj Temperaare) = ,970 - (215) 0.0 - (Y5) 1.0 - (115) 0.0 = ,570Gain (Sss,,,, Wind) = 970 - (215) 1.0 - (315) ,918 = ,019FIGURE 3.4The partially learned decision tree resulting from the first step of ID3. The training examples aresorted to the corresponding descendant nodes. The Overcast descendant has only positive examplesand therefore becomes a leaf node with classification Yes. The other two nodes will be furtherexpanded, by selecting the attribute with highest information gain relative to the new subsets ofexamples. 
that guides this hill-climbing search is the information gain measure. This searchis depicted in Figure 3.5. 
By viewing ID^ in terms of its search space and search strategy, we can getsome insight into its capabilities and limitations. 
1~3's hypothesis space of all decision trees isa complete space of finitediscrete-valued functions, relative to the available attributes. Because everyfinite discrete-valued function can be represented by some decision tree, ID3avoids one of the major risks of methods that search incomplete hypothesisspaces (such as methods that consider only conjunctive hypotheses): that thehypothesis space might not contain the target function. 
ID3 maintains only a single current hypothesis asit searches through thespace of decision trees. This contrasts, for example, with the earlier ver- 
sion space candidate-~lirninat-od, which maintains the set of allhypotheses consistent with the available training examples. By determin- 
ing only a single hypothesis, ID^ loses the capabilities that follow fromF: + - + FIGURE 3.5Hypothesis space search by ID3. 
ID3 searches throuah the mace of - possible decision trees from simplestto increasingly complex, guided by the 
... ... information gain heuristic. 
explicitly representing all consistent hypotheses. For example, it does nothave the ability to determine how many alternative decision trees are con- 
sistent with the available training data, orto pose new instance queries thatoptimally resolve among these competing hypotheses. 
0 ID3 in its pure form performs no backtracking in its search. Once it,se- 
lects an attribute to test ata particular level in the tree, it never backtracksto reconsider this choice. Therefore, itis susceptible to the usual risks ofhill-climbing search without backtracking: converging to locally optimal so- 
lutions that are not globally optimal. In the case of ID3, a locally optimalsolution corresponds to the decision tree it selects along the single searchpath it explores. However, this locally optimal solution may be less desir- 
able than trees that would have been encountered along a different branch ofthe search. Below we discuss an extension that adds a form of backtracking 
(post-pruning the decision tree). 
0 ID3 uses all training examples at each step in the search to make statisticallybased decisions regarding how to refine its current hypothesis. This contrastswith methods that make decisions incrementally, based on individual train- 
ing examples (e.g., FIND-Sor CANDIDATE-ELIMINATION). One advantage ofusing statistical properties of all the examples (e.g., information gain) is thatthe resulting search is much less sensitive to errors in individual trainingexamples. ID3 can be easily extended to handle noisy training data by mod- 
ifying its termination criterion to accept hypotheses that imperfectly fit thetraining data. 
3.6 INDUCTIVE BIAS IN DECISION TREE LEARNINGWhat is the policy by which ID3 generalizes from observed training examplesto classify unseen instances? In other words, what is its inductive bias? Recallfrom Chapter 2 that inductive bias is the set of assumptions that, together withthe training data, deductively justify the classifications assigned by the learner tofuture instances. 
Given a collection of training examples, there are typically many decisiontrees consistent with these examples. Describing the inductive bias of ID3 there- 
fore consists of describing the basis by which it chooses one of these consis- 
tent hypotheses over the others. Which of these decision trees does ID3 choose? 
It chooses the first acceptable tree it encounters in its simple-to-complex, hill- 
climbing search through the space of possible trees. Roughly speaking, then, theID3 search strategy (a) selects in favor of shorter trees over longer ones, and 
(b) selects trees that place the attributes with highest information gain closest tothe root. Because of the subtle interaction between the attribute selection heuris- 
tic used by ID3 and the particular training examples it encounters, itis difficultto characterize precisely the inductive bias exhibited by ID3. However, we canapproximately characterize its bias asa preference for short decision trees overcomplex trees. 
Approximate inductive bias of ID3: Shorter trees are preferred over larger trees. 
In fact, one could imagine an algorithm similar to ID3 that exhibits preciselythis inductive bias. Consider an algorithm that begins with the empty tree andsearches breadth Jirst through progressively more complex trees, first consideringall trees of depth 1, then all trees of depth 2, etc. Once it finds a decision treeconsistent with the training data, it returns the smallest consistent tree at thatsearch depth (e.g., the tree with the fewest nodes). Let us call this breadth-firstsearch algorithm BFS-ID3. BFS-ID3 finds a shortest decision tree and thus exhibitsprecisely the bias "shorter trees are preferred over longer trees." ID3 can beviewed asan efficient approximation to BFS-ID3, using a greedy heuristic searchto attempt to find the shortest tree without conducting the entire breadth-firstsearch through the hypothesis space. 
Because ID3 uses the information gain heuristic and a hill climbing strategy, 
it exhibits a more complex bias than BFS-ID3. In particular, it does not alwaysfind the shortest consistent tree, and itis biased to favor trees that place attributeswith high information gain closest to the root. 
A closer approximation to the inductive bias of ID3: Shorter trees are preferredover longer trees. Trees that place high information gain attributes close to the rootare preferred over those that do not. 
3.6.1 Restriction Biases and Preference BiasesThere isan interesting difference between the types of inductive bias exhibitedby ID3 and by the CANDIDATE-ELIMINATION algorithm discussed in Chapter 2. 
Consider the difference between the hypothesis space search in these two ap- 
proaches: 
ID3 searches a complete hypothesis space (i.e., one capable of expressingany finite discrete-valued function). It searches incompletely through thisspace, from simple to complex hypotheses, until its termination condition ismet (e.g., until it finds a hypothesis consistent with the data). Its inductivebias is solely a consequence of the ordering of hypotheses by its searchstrategy. Its hypothesis space introduces no additional bias. 
0 The version space CANDIDATE-ELIMINATION algorithm searches an incom- 
plete hypothesis space (i.e., one that can express only a subset of the poten- 
tially teachable concepts). It searches this space completely, finding everyhypothesis consistent with the training data. Its inductive bias is solely aconsequence of the expressive power of its hypothesis representation. Itssearch strategy introduces no additional bias. 
In brief, the inductive bias of ID3 follows from its search strategy, whereasthe inductive bias of the CANDIDATE-ELIMINATION algorithm follows from the def- 
inition of its search space. 
The inductive bias of ID3 is thus a preference for certain hypotheses overothers (e.g., for shorter hypotheses), with no hard restriction on the hypotheses thatcan be eventually enumerated. This form of bias is typically called a preferencebias (or, alternatively, a search bias). In contrast, the bias of the CANDIDATE- 
ELIMINATION algorithm isin the form ofa categorical restriction on the set ofhypotheses considered. This form of bias is typically called a restriction bias (or, 
alternatively, a language bias). 
Given that some form of inductive bias is required in order to generalizebeyond the training data (see Chapter 2), which type of inductive bias shall weprefer; a preference bias or restriction bias? 
Typically, a preference bias is more desirable than a restriction bias, be- 
cause it allows the learner to work within a complete hypothesis space that isassured to contain the unknown target function. In contrast, a restriction bias thatstrictly limits the set of potential hypotheses is generally less desirable, becauseit introduces the possibility of excluding the unknown target function altogether. 
Whereas ID3 exhibits a purely preference bias and CANDIDATE-ELIMINATIONa purely restriction bias, some learning systems combine both. Consider, for ex- 
ample, the program described in Chapter 1 for learning a numerical evaluationfunction for game playing. In this case, the learned evaluation function is repre- 
sented bya linear combination ofa fixed set of board features, and the learningalgorithm adjusts the parameters of this linear combination to best fit the availabletraining data. In this case, the decision to use a linear function to represent the eval- 
uation function introduces a restriction bias (nonlinear evaluation functions cannotbe represented in this form). At the same time, the choice ofa particular parametertuning method (the LMS algorithm in this case) introduces a preference bias stem- 
ming from the ordered search through the space of all possible parameter values. 
3.6.2 Why Prefer Short Hypotheses? 
Is ID3's inductive bias favoring shorter decision trees a sound basis for generaliz- 
ing beyond the training data? Philosophers and others have debated this questionfor centuries, and the debate remains unresolved to this day. William of Occamwas one of the first to discusst the question, around the year 1320, so this biasoften goes by the name of Occam's razor. 
Occam's razor: Prefer the simplest hypothesis that fits the data. 
Of course giving an inductive bias a name does not justify it. Why should oneprefer simpler hypotheses? Notice that scientists sometimes appear to follow thisinductive bias. Physicists, for example, prefer simple explanations for the motionsof the planets, over more complex explanations. Why? One argument is thatbecause there are fewer short hypotheses than long ones (based on straightforwardcombinatorial arguments), itis less likely that one will find a short hypothesis thatcoincidentally fits the training data. In contrast there are often many very complexhypotheses that fit the current training data but fail to generalize correctly tosubsequent data. Consider decision tree hypotheses, for example. There are manymore 500-node decision trees than 5-node decision trees. Given a small set of20 training examples, we might expect tobe able to find many 500-node deci- 
sion trees consistent with these, whereas we would be more surprised ifa 5-nodedecision tree could perfectly fit this data. We might therefore believe the 5-nodetree is less likely tobe a statistical coincidence and prefer this hypothesis overthe 500-node hypothesis. 
Upon closer examination, it turns out there isa major difficulty with theabove argument. By the same reasoning we could have argued that one shouldprefer decision trees containing exactly 17 leaf nodes with 11 nonleaf nodes, thatuse the decision attribute A1 at the root, and test attributes A2 through All, innumerical order. There are relatively few such trees, and we might argue (by thesame reasoning as above) that our a priori chance of finding one consistent withan arbitrary set of data is therefore small. The difficulty here is that there are verymany small sets of hypotheses that one can define-most of them rather arcane. 
Why should we believe that the small set of hypotheses consisting of decisiontrees with short descriptions should be any more relevant than the multitude ofother small sets of hypotheses that we might define? 
A second problem with the above argument for Occam's razor is that the sizeof a hypothesis is determined by the particular representation used internally bythe learner. Two learners using different internal representations could thereforeanive at different hypotheses, both justifying their contradictory conclusions byOccam's razor! For example, the function represented by the learned decisiontree in Figure 3.1 could be represented asa tree with just one decision node, by alearner that uses the boolean attribute XYZ, where we define the attribute XYZ to 
~~prentl~ while shaving. 
be true for instances that are classified positive by the decision tree in Figure 3.1and false otherwise. Thus, two learners, both applying Occam's razor, wouldgeneralize in different ways if one used the XYZ attribute to describe its examplesand the other used only the attributes Outlook, Temperature, Humidity, and Wind. 
This last argument shows that Occam's razor will produce two differenthypotheses from the same training examples when itis applied by two learnersthat perceive these examples in terms of different internal representations. On thisbasis we might be tempted to reject Occam's razor altogether. However, considerthe following scenario that examines the question of which internal representa- 
tions might arise from a process of evolution and natural selection. Imagine apopulation of artificial learning agents created bya simulated evolutionary pro- 
cess involving reproduction, mutation, and natural selection of these agents. Letus assume that this evolutionary process can alter the perceptual systems of theseagents from generation to generation, thereby changing the internal attributes bywhich they perceive their world. For the sake of argument, let us also assume thatthe learning agents employ a fixed learning algorithm (say ID3) that cannot bealtered by evolution. Itis reasonable to assume that over time evolution will pro- 
duce internal representation that make these agents increasingly successful withintheir environment. Assuming that the success ofan agent depends highly on itsability to generalize accurately, we would therefore expect evolution to developinternal representations that work well with whatever learning algorithm and in- 
ductive bias is present. If the species of agents employs a learning algorithm whoseinductive bias is Occam's razor, then we expect evolution to produce internal rep- 
resentations for which Occam's razor isa successful strategy. The essence of theargument here is that evolution will create internal representations that make thelearning algorithm's inductive bias a self-fulfilling prophecy, simply because itcan alter the representation easier than it can alter the learning algorithm. 
For now, we leave the debate regarding Occam's razor. We will revisit it inChapter 6, where we discuss the Minimum Description Length principle, a versionof Occam's razor that can be interpreted within a Bayesian framework. 
3.7 ISSUES IN DECISION TREE LEARNINGPractical issues in learning decision trees include determining how deeply to growthe decision tree, handling continuous attributes, choosing an appropriate attributeselection measure, andling training data with missing attribute values, handlingattributes with differing costs, and improving computational efficiency. Belowwe discuss each of these issues and extensions to the basic ID3 algorithm thataddress them. ID3 has itself been extended to address most of these issues, withthe resulting system renamed C4.5 (Quinlan 1993). 
3.7.1 Avoiding Overfitting the DataThe algorithm described in Table 3.1 grows each branch of the tree just deeplyenough to perfectly classify the training examples. While this is sometimes areasonable strategy, in fact it can lead to difficulties when there is noise in the data, 
or when the number of training examples is too small to produce a representativesample of the true target function. In either of these cases, this simple algorithmcan produce trees that overjt the training examples. 
We will say that a hypothesis overfits the training examples if some otherhypothesis that fits the training examples less well actually performs better over theentire distribution of instances (i.e., including instances beyond the training set). 
Definition: Given a hypothesis space H, a hypothesis hE His said to overlit thetraining data if there exists some alternative hypothesis h' EH, such that h hassmaller error than h' over the training examples, but h' has a smaller error than hover the entire distribution of instances. 
Figure 3.6 illustrates the impact of overfitting ina typical application of deci- 
sion tree learning. In this case, the ID3 algorithm is applied to the task of learningwhich medical patients have a form of diabetes. The horizontal axis of this plotindicates the total number of nodes in the decision tree, as the tree is being con- 
structed. The vertical axis indicates the accuracy of predictions made by the tree. 
The solid line shows the accuracy of the decision tree over the training examples, 
whereas the broken line shows accuracy measured over an independent set of testexamples (not included in the training set). Predictably, the accuracy of the treeover the training examples increases monotonically as the tree is grown. How- 
ever, the accuracy measured over the independent test examples first increases, 
then decreases. As can be seen, once the tree size exceeds approximately 25 nodes, 
On training data - 
On test data ---- iSize of tree (number of nodes) 
FIGURE 3.6Overfitting in decision tree learning. As ID3 adds new nodes to grow the decision tree, the accuracy ofthe tree measured over the training examples increases monotonically. However, when measured overa set of test examples independent of the training examples, accuracy first increases, then decreases. 
Software and data for experimenting with variations on this plot are available on the World WideWeb at http://www.cs.cmu.edu/-torn/mlbook.html. 
further elaboration of the tree decreases its accuracy over the test examples despiteincreasing its accuracy on the training examples. 
How can itbe possible for tree hto fit the training examples better than h', 
but for itto perform more poorly over subsequent examples? One way this canoccur is when the training examples contain random errors or noise. To illustrate, 
consider the effect of adding the following positive training example, incorrectlylabeled as negative, to the (otherwise correct) examples in Table 3.2. 
(Outlook = Sunny, Temperature = Hot, Humidity = Normal, 
Wind = Strong, PlayTennis = No) 
Given the original error-free data, ID3 produces the decision tree shown in Fig- 
ure 3.1. However, the addition of this incorrect example will now cause ID3 toconstruct a more complex tree. In particular, the new example will be sorted intothe second leaf node from the left in the learned tree of Figure 3.1, along with theprevious positive examples D9 and Dl 1. Because the new example is labeled asa negative example, ID3 will search for further refinements to the tree below thisnode. Of course as long as the new erroneous example differs in some arbitraryway from the other examples affiliated with this node, ID3 will succeed in findinga new decision attribute to separate out this new example from the two previouspositive examples at this tree node. The result is that ID3 will output a decisiontree (h) that is more complex than the original tree from Figure 3.1 (h'). Of courseh will fit the collection of training examples perfectly, whereas the simpler h' willnot. However, given that the new decision node is simply a consequence of fittingthe noisy training example, we expect hto outperform h' over subsequent datadrawn from the same instance distribution. 
The above example illustrates how random noise in the training examplescan lead to overfitting. In fact, overfitting is possible even when the training dataare noise-free, especially when small numbers of examples are associated with leafnodes. In this case, itis quite possible for coincidental regularities to occur, inwhich some attribute happens to partition the examples very well, despite beingunrelated to the actual target function. Whenever such coincidental regularitiesexist, there isa risk of overfitting. 
Overfitting isa significant practical difficulty for decision tree learning andmany other learning methods. For example, in one experimental study of ID3involving five different learning tasks with noisy, nondeterministic data (Mingers1989b), overfitting was found to decrease the accuracy of learned decision treesby 10-25% on most problems. 
There are several approaches to avoiding overfitting in decision tree learning. 
These can be grouped into two classes: 
approaches that stop growing the tree earlier, before it reaches the pointwhere it perfectly classifies the training data, 
0 approaches that allow the tree to overfit the data, and then post-prune thetree. 
Although the first of these approaches might seem.more direct, the secondapproach of post-pruning overfit trees has been found tobe more successful inpractice. This is due to the difficulty in the first approach of estimating preciselywhen to stop growing the tree. 
Regardless of whether the correct tree size is found by stopping early orby post-pruning, a key question is what criterion isto be used to determine thecorrect final tree size. Approaches include: 
0 Use a separate set of examples, distinct from the training examples, to eval- 
uate the utility of post-pruning nodes from the tree. 
0 Use all the available data for training, but apply a statistical test to estimatewhether expanding (or pruning) a particular node is likely to produce animprovement beyond the training set. For example, Quinlan (1986) uses achi-square test to estimate whether further expanding a node is likely toimprove performance over the entire instance distribution, or only on thecurrent sample of training data. 
0 Use an explicit measure of the complexity for encoding the training exam- 
ples and the decision tree, halting growth of the tree when this encodingsize is minimized. This approach, based ona heuristic called the MinimumDescription Length principle, is discussed further in Chapter 6, as well asin Quinlan and Rivest (1989) and Mehta etal. (199.5). 
The first of the above approaches is the most common and is often referredto asa training and validation set approach. We discuss the two main variants ofthis approach below. In this approach, the available data are separated into twosets of examples: a training set, which is used to form the learned hypothesis, anda separate validation set, which is used to evaluate the accuracy of this hypothesisover subsequent data and, in particular, to evaluate the impact of pruning thishypothesis. The motivation is this: Even though the learner may be misled byrandom errors and coincidental regularities within the training set, the validationset is unlikely to exhibit the same random fluctuations. Therefore, the validationset can be expected to provide a safety check against overfitting the spuriouscharacteristics of the training set. Of course, itis important that the validation setbe large enough to itself provide a statistically significant sample of the instances. 
One common heuristic isto withhold one-third of the available examples for thevalidation set, using the other two-thirds for training. 
3.7.1.1 REDUCED ERROR PRUNINGHow exactly might we use a validation set to prevent overfitting? One approach, 
called reduced-error pruning (Quinlan 1987), isto consider each of the decisionnodes in the.tree tobe candidates for pruning. Pruning a decision node consists ofremoving the subtree rooted at that node, making ita leaf node, and assigning itthe most common classification of the training examples affiliated with that node. 
Nodes are removed only if the resulting pruned tree performs no worse than-theoriginal over the validation set. This has the effect that any leaf node added dueto coincidental regularities in the training set is likely tobe pruned because thesesame coincidences are unlikely to occur in the validation set. Nodes are prunediteratively, always choosing the node whose removal most increases the decisiontree accuracy over the validation set. Pruning of nodes continues until furtherpruning is harmful (i.e., decreases accuracy of the tree over the validation set). 
The impact of reduced-error pruning on the accuracy of the decision treeis illustrated in Figure 3.7. Asin Figure 3.6, the accuracy of the tree is shownmeasured over both training examples and test examples. The additional line inFigure 3.7 shows accuracy over the test examples as the tree is pruned. Whenpruning begins, the tree isat its maximum size and lowest accuracy over the testset. As pruning proceeds, the number of nodes is reduced and accuracy over thetest set increases. Here, the available data has been split into three subsets: thetraining examples, the validation examples used for pruning the tree, and a set oftest examples used to provide an unbiased estimate of accuracy over future unseenexamples. The plot shows accuracy over the training and test sets. Accuracy overthe validation set used for pruning is not shown. 
Using a separate set of data to guide pruning isan effective approach pro- 
vided a large amount of data is available. The major drawback of this approachis that when data is limited, withholding part ofit for the validation set reduceseven further the number of examples available for training. The following sectionpresents an alternative approach to pruning that has been found useful in manypractical situations where data is limited. Many additional techniques have beenproposed as well, involving partitioning the available data several different times in7 
---..--..._.._.._~~ 
" .------.------- 2--... -, .--.. -..... -... .-... 
\_____.. 
--... -.... --... -.___._..___...-_-------- 
On training data - 
On test data ---- 
On test data (during pruning) - - - - - 
0 10 20 30 40 50 60 70 80 90 100Size of tree (number of nodes) 
FIGURE 3.7Effect of reduced-error pruning in decision tree learning. This plot shows the same curves of trainingand test set accuracy asin Figure 3.6. In addition, it shows the impact of reduced error pruning ofthe tree produced by ID3. Notice the increase in accuracy over the test set as nodes are pruned fromthe tree. Here, the validation set used for pruning is distinct from both the training and test sets. 
multiple ways, then averaging the results. Empirical evaluations of alternative treepruning methods are reported by Mingers (1989b) and by Malerba etal. (1995). 
3.7.1.2 RULE POST-PRUNINGIn practice, one quite successful method for finding high accuracy hypotheses isa technique we shall call rule post-pruning. A variant of this pruning method isused by C4.5 (Quinlan 1993), which isan outgrowth of the original ID3 algorithm. 
Rule post-pruning involves the following steps: 
1. Infer the decision tree from the training set, growing the tree until the trainingdata is fit as well as possible and allowing overfitting to occur. 
2. Convert the learned tree into an equivalent set of rules by creating one rulefor each path from the root node toa leaf node. 
3. Prune (generalize) each rule by removing any preconditions that result inimproving its estimated accuracy. 
4. Sort the pruned rules by their estimated accuracy, and consider them in thissequence when classifying subsequent instances. 
To illustrate, consider again the decision tree in Figure 3.1. In rule post- 
pruning, one rule is generated for each leaf node in the tree. Each attribute testalong the path from the root to the leaf becomes a rule antecedent (precondition) 
and the classification at the leaf node becomes the rule consequent (postcondition). 
For example, the leftmost path of the tree in Figure 3.1 is translated into the ruleIF (Outlook = Sunny) A (Humidity = High) 
THEN PlayTennis = NoNext, each such rule is pruned by removing any antecedent, or precondi- 
tion, whose removal does not worsen its estimated accuracy. Given the aboverule, for example, rule post-pruning would consider removing the preconditions 
(Outlook = Sunny) and (Humidity = High). It would select whichever of thesepruning steps produced the greatest improvement in estimated rule accuracy, thenconsider pruning the second precondition asa further pruning step. No pruningstep is performed ifit reduces the estimated rule accuracy. 
As noted above, one method to estimate rule accuracy isto use a validationset of examples disjoint from the training set. Another method, used by C4.5, 
isto evaluate performance based on the training set itself, using a pessimisticestimate to make up for the fact that the training data gives an estimate biasedin favor of the rules. More precisely, C4.5 calculates its pessimistic estimate bycalculating the rule accuracy over the training examples to which it applies, thencalculating the standard deviation in this estimated accuracy assuming a binomialdistribution. For a given confidence level, the lower-bound estimate is then takenas the measure of rule performance (e.g., for a 95% confidence interval, ruleaccuracy is pessimistically estimated by the observed accuracy over the trainingset, minus 1.96 times the estimated standard deviation). The net effect is that forlarge data sets, the pessimistic estimate is very close to the observed accuracy 
(e.g., the standard deviation is very small), whereas it grows further from theobserved accuracy as the size of the data set decreases. Although this heuristicmethod is not statistically valid, it has nevertheless been found useful in practice. 
See Chapter 5 for a discussion of statistically valid approaches to estimating meansand confidence intervals. 
Why convert the decision tree to rules before pruning? There are three mainadvantages. 
Converting to rules allows distinguishing among the different contexts inwhich a decision node is used. Because each distinct path through the deci- 
sion tree node produces a distinct rule, the pruning decision regarding thatattribute test can be made differently for each path. In contrast, if the treeitself were pruned, the only two choices would beto remove the decisionnode completely, orto retain itin its original form. 
Converting to rules removes the distinction between attribute tests that occurnear the root of the tree and those that occur near the leaves. Thus, we avoidmessy bookkeeping issues such as how to reorganize the tree if the root nodeis pruned while retaining part of the subtree below this test. 
Converting to rules improves readability. Rules are often easier forto understand. 
3.7.2 Incorporating Continuous-Valued AttributesOur initial definition of ID3 is restricted to attributes that take ona discrete setof values. First, the target attribute whose value is predicted by the learned treemust be discrete valued. Second, the attributes tested in the decision nodes ofthe tree must also be discrete valued. This second restriction can easily bere- 
moved so that continuous-valued decision attributes can be incorporated into thelearned tree. This can be accomplished by dynamically defining new discrete- 
valued attributes that partition the continuous attribute value into a discrete setof intervals. In particular, for an attribute A that is continuous-valued, the algo- 
rithm can dynamically create a new boolean attribute A, that is true ifA < cand false otherwise. The only question is how to select the best value for thethreshold c. 
Asan example, suppose we wish to include the continuous-valued attributeTemperature in describing the training example days in the learning task ofTa- 
ble 3.2. Suppose further that the training examples associated with a particularnode in the decision tree have the following values for Temperature and the targetattribute PlayTennis. 
Temperature: 40 48 60 72 80 90PlayTennis: NoNo Yes Yes Yes NOCHAPTER 3 DECISION TREE LEARNING 73What threshold-based boolean attribute should be defined based on Temper- 
ature? Clearly, we would like to pick a threshold, c, that produces the greatestinformation gain. By sorting the examples according to the continuous attributeA, then identifying adjacent examples that differ in their target classification, wecan generate a set of candidate thresholds midway between the correspondingvalues ofA. It can be shown that the value ofc that maximizes information gainmust always lie at such a boundary (Fayyad 1991). These candidate thresholdscan then be evaluated by computing the information gain associated with each. 
In the current example, there are two candidate thresholds, corresponding to thevalues of Temperature at which the value of PlayTennis changes: (48 + 60)/2, 
and (80 + 90)/2. The information gain can then be computed for each of thecandidate attributes, Temperat~re,~~ and Temperat~re,~~, and the best can beselected (Temperat~re,~~). This dynamically created boolean attribute can thencompete with the other discrete-valued candidate attributes available for growingthe decision tree. Fayyad and Irani (1993) discuss an extension to this approachthat splits the continuous attribute into multiple intervals rather than just two in- 
tervals based ona single threshold. Utgoff and Brodley (1991) and Murthy etal. 
( 1994) discuss approaches that define features by thresholding linear combinationsof several continuous-valued attributes. 
3.7.3 Alternative Measures for Selecting AttributesThere isa natural bias in the information gain measure that favors attributes withmany values over those with few values. Asan extreme example, consider theattribute Date, which has a very large number of possible values (e.g., March 4, 
1979). Ifwe were to add this attribute to the data in Table 3.2, it would havethe highest information gain of any of the attributes. This is because Date aloneperfectly predicts the target attribute over the training data. Thus, it would beselected as the decision attribute for the root node of the tree and lead toa (quitebroad) tree of depth one, which perfectly classifies the training data. Of course, 
this decision tree would fare poorly on subsequent examples, because itis not auseful predictor despite the fact that it perfectly separates the training data. 
What is wrong with the attribute Date? Simply put, it has so many possiblevalues that itis bound to separate the training examples into very small subsets. 
Because of this, it will have a very high information gain relative to the trainingexamples, despite being a very poor predictor of the target function over unseeninstances. 
One way to avoid this difficulty isto select decision attributes based on somemeasure other than information gain. One alternative measure that has been usedsuccessfully is the gain ratio (Quinlan 1986). The gain ratio measure penalizesattributes such as Date by incorporating a term, called split informution, that issensitive to how broadly and uniformly the attribute splits the data: 
74 MACHINE LEARNINGwhere S1 through S, are the c subsets of examples resulting from partitioning Sby the c-valued attribute A. Note that Splitlnfomzation is actually the entropy ofS with respect to the values of attribute A. This isin contrast to our previoususes of entropy, in which we considered only the entropy ofS with respect to thetarget attribute whose value isto be predicted by the learned tree. 
The Gain Ratio measure is defined in terms of the earlier Gain measure, aswell as this Splitlnfomzation, as followsGain (S, A) 
GainRatio(S, A) rSplit Inf ormation(S, A) 
Notice that the Splitlnfomzation term discourages the selection of attributes withmany uniformly distributed values. For example, consider a collection ofn ex- 
amples that are completely separated by attribute A (e.g., Date). In this case, theSplitlnfomzation value will be log, n. In contrast, a boolean attribute B that splitsthe same n examples exactly in half will have Splitlnfomzation of 1. If attributesA and B produce the same information gain, then clearly B will score higheraccording to the Gain Ratio measure. 
One practical issue that arises in using GainRatio in place of Gain toselect attributes is that the denominator can be zero or very small when ISi 1 x IS1for one of the Si. This either makes the GainRatio undefined or very large forattributes that happen to have the same value for nearly all members ofS. Toavoid selecting attributes purely on this basis, we can adopt some heuristic suchas first calculating the Gain of each attribute, then applying the GainRatio testonly considering those attributes with above average Gain (Quinlan 1986). 
An alternative to the GainRatio, designed to directly address the abovedifficulty, isa distance-based measure introduced by Lopez de Mantaras (1991). 
This measure is based on defining a distance metric between partitions of'thedata. Each attribute is evaluated based on the distance between the data partitionit creates and the perfect partition (i.e., the partition that perfectly classifies thetraining data). The attribute whose partition is closest to the perfect partition ischosen. Lopez de Mantaras (1991) defines this distance measure, proves that itis not biased toward attributes with large numbers of values, and reports experi- 
mental studies indicating that the predictive accuracy of the induced trees is notsignificantly different from that obtained with the Gain and Gain Ratio measures. 
However, this distance measure avoids the practical difficulties associated with theGainRatio measure, and in his experiments it produces significantly smaller treesin the case of data sets whose attributes have very different numbers of values. 
A variety of other selection measures have been proposed as well (e.g., 
see Breiman etal. 1984; Mingers 1989a; Kearns and Mansour 1996; Dietterichet al. 1996). Mingers (1989a) provides an experimental analysis of the relativeeffectiveness of several selection measures over a variety of problems. He reportssignificant differences in the sizes of the unpruned trees produced by the differentselection measures. However, in his experimental domains the choice of attributeselection measure appears to have a smaller impact on final accuracy than doesthe extent and method of post-pruning. 
CHAPTER 3 DECISION TREE LEARNING 753.7.4 Handling Training Examples with Missing Attribute ValuesIn certain cases, the available data may be missing values for some attributes. 
For example, ina medical domain in which we wish to predict patient outcomebased on various laboratory tests, it may be that the lab test Blood-Test-Result isavailable only for a subset of the patients. In such cases, itis common to estimatethe missing attribute value based on other examples for which this attribute has aknown value. 
Consider the situation in which Gain(S, A) isto be calculated at node n inthe decision tree to evaluate whether the attribute Ais the best attribute to testat this decision node. Suppose that (x, c(x)) is one of the training examples in Sand that the value A(x) is unknown. 
One strategy for dealing with the missing attribute value isto assign it thevalue that is most common among training examples at node n. Alternatively, wemight assign it the most common value among examples at node n that have theclassification c(x). The elaborated training example using this estimated value forA(x) can then be used directly by the existing decision tree learning algorithm. 
This strategy is examined by Mingers (1989a). 
A second, more complex procedure isto assign a probability to each of thepossible values ofA rather than simply assigning the most common value toA(x). 
These probabilities can be estimated again based on the observed frequencies ofthe various values for A among the examples at node n. For example, given aboolean attribute A, if node n contains six known examples with A = 1 and fourwith A = 0, then we would say the probability that A(x) = 1 is 0.6, and theprobability that A(x) = 0 is 0.4. A fractional 0.6 of instance xis now distributeddown the branch for A = 1, and a fractional 0.4 ofx down the other tree branch. 
These fractional examples are used for the purpose of computing informationGain and can be further subdivided at subsequent branches of the tree ifa secondmissing attribute value must be tested. This same fractioning of examples canalso be applied after learning, to classify new instances whose attribute valuesare unknown. In this case, the classification of the new instance is simply themost probable classification, computed by summing the weights of the instancefragments classified in different ways at the leaf nodes of the tree. This methodfor handling missing attribute values is used in C4.5 (Quinlan 1993). 
3.7.5 Handling Attributes with Differing CostsIn some learning tasks the instance attributes may have associated costs. Forexample, in learning to classify medical diseases we might describe patients interms of attributes such as Temperature, BiopsyResult, Pulse, BloodTestResults, 
etc. These attributes vary significantly in their costs, both in terms of monetarycost and cost to patient comfort. In such tasks, we would prefer decision trees thatuse low-cost attributes where possible, relying on high-cost attributes only whenneeded to produce reliable classifications. 
ID3 can be modified to take into account attribute costs by introducing a costterm into the attribute selection measure. For example, we might divide the Gpinby the cost of the attribute, so that lower-cost attributes would be preferred. Whilesuch cost-sensitive measures do not guarantee finding an optimal cost-sensitivedecision tree, they do bias the search in favor of low-cost attributes. 
Tan and Schlimmer (1990) and Tan (1993) describe one such approach andapply itto a robot perception task in which the robot must learn to classify dif- 
ferent objects according to how they can be grasped by the robot's manipulator. 
In this case the attributes correspond to different sensor readings obtained by amovable sonar on the robot. Attribute cost is measured by the number of secondsrequired to obtain the attribute value by positioning and operating the sonar. Theydemonstrate that more efficient recognition strategies are learned, without sacri- 
ficing classification accuracy, by replacing the information gain attribute selectionmeasure by the following measureCost (A) 
Nunez (1988) describes a related approach and its application to learningmedical diagnosis rules. Here the attributes are different symptoms and laboratorytests with differing costs. His system uses a somewhat different attribute selectionmeasure2GaWS.A) - 1 
(Cost(A) + 
where wE [0, 11 isa constant that determines the relative importance of costversus information gain. Nunez (1991) presents an empirical comparison of thesetwo approaches over a range of tasks. 
3.8 SUMMARY AND FURTHER READINGThe main points of this chapter include: 
Decision tree learning provides a practical method for concept learning andfor learning other discrete-valued functions. The ID3 family of algorithmsinfers decision trees by growing them from the root downward, greedilyselecting the next best attribute for each new decision branch added to thetree. 
ID3 searches a complete hypothesis space (i.e., the space of decision treescan represent any discrete-valued function defined over discrete-valued in- 
stances). It thereby avoids the major difficulty associated with approachesthat consider only restricted sets of hypotheses: that the target function mightnot be present in the hypothesis space. 
The inductive bias implicit in ID3 includes a preference for smaller trees; 
that is, its search through the hypothesis space grows the tree only as largeas needed in order to classify the available training examples. 
Overfitting the training data isan important issue in decision tree learning. 
Because the training examples are only a sample of all possible instances, 
CHAFER 3 DECISION TREE LEARNING 77it is possible to add branches to the tree that improve performance on thetraining examples while decreasing performance on other instances outsidethis set. Methods for post-pruning the decision tree are therefore importantto avoid overfitting in decision tree learning (and other inductive inferencemethods that employ a preference bias). 
A large variety of extensions to the basic ID3 algorithm has been developedby different researchers. These include methods for post-pruning trees, han- 
dling real-valued attributes, accommodating training examples with miss- 
ing attribute values, incrementally refining decision trees as new trainingexamples become available, using attribute selection measures other thaninformation gain, and considering costs associated with instance attributes. 
Among the earliest work on decision tree learning is Hunt's Concept Learn- 
ing System (CLS) (Hunt etal. 1966) and Friedman and Breiman's work resultingin the CART system (Friedman 1977; Breiman etal. 1984). Quinlan's ID3 sys- 
tem (Quinlan 1979, 1983) forms the basis for the discussion in this chapter. Otherearly work on decision tree learning includes ASSISTANT (Kononenko etal. 1984; 
Cestnik etal. 1987). Implementations of decision tree induction algorithms arenow commercially available on many computer platforms. 
For further details on decision tree induction, an excellent book by Quinlan 
(1993) discusses many practical issues and provides executable code for C4.5. 
Mingers (1989a) and Buntine and Niblett (1992) provide two experimental studiescomparing different attribute-selection measures. Mingers (1989b) and Malerba etal. (1995) provide studies of different pruning strategies. Experiments comparingdecision tree learning and other learning methods can be found in numerouspapers, including (Dietterich etal. 1995; Fisher and McKusick 1989; Quinlan1988a; Shavlik etal. 1991; Thrun etal. 1991; Weiss and Kapouleas 1989). 
EXERCISESGive decision trees to represent the following boolean functions: 
(a) AA -B 
(b) AV [BA C] 
(c) A XOR B 
(d) [AA B] v [CA DlConsider the following set of training examples: 
Instance Classification a1 a2 
(a) What is the entropy of this collection of training examples with respect to thetarget function classification? 
(b) What is the information gain of a2 relative to these training examples? 
3.3. True or false: If decision tree D2 isan elaboration of tree Dl, then Dlis more- 
general-than D2. Assume Dl and D2 are decision trees representing arbitrary booleanfunctions, and that D2 isan elaboration ofDl if ID3 could extend Dl into D2. If true, 
give a proof; if false, a counterexample. (More-general-than is defined in Chapter 2.) 
3.4. ID3 searches for just one consistent hypothesis, whereas the CANDIDATE- 
ELIMINATION algorithm finds all consistent hypotheses. Consider the correspondencebetween these two learning algorithms. 
(a) Show the decision tree that would be learned by ID3 assuming itis given thefour training examples for the Enjoy Sport? target concept shown in Table 2.1of Chapter 2. 
(b) What is the relationship between the learned decision tree and the version space 
(shown in Figure 2.3 of Chapter 2) that is learned from these same examples? 
Is the learned tree equivalent to one of the members of the version space? 
(c) Add the following training example, and compute the new decision tree. Thistime, show the value of the information gain for each candidate attribute at eachstep in growing the tree. 
Sky Air-Temp Humidity Wind Water Forecast Enjoy-Sport? 
Sunny Warm Normal Weak Warm Same No 
(d) Suppose we wish to design a learner that (like ID3) searches a space of decisiontree hypotheses and (like CANDIDATE-ELIMINATION) finds all hypotheses con- 
sistent with the data. In short, we wish to apply the CANDIDATE-ELIMINATIONalgorithm to searching the space of decision tree hypotheses. Show the S andG sets that result from the first training example from Table 2.1. Note S mustcontain the most specific decision trees consistent with the data, whereas G mustcontain the most general. Show how the S and G sets are refined by thesecondtraining example (you may omit syntactically distinct trees that describe the sameconcept). What difficulties do you foresee in applying CANDIDATE-ELIMINATIONto a decision tree hypothesis space? 
REFERENCESBreiman, L., Friedman, J. H., Olshen, R. A., & Stone, P. 1. (1984). ClassiJication and regressiontrees. Belmont, CA: Wadsworth International Group. 
Brodley, C. E., & Utgoff, P. E. (1995). Multivariate decision trees. Machine Learning, 19, 45-77. 
Buntine, W., & Niblett, T. (1992). A further comparison of splitting rules for decision-tree induction. 
Machine Learning, 8, 75-86. 
Cestnik, B., Kononenko, I., & Bratko, I. (1987). ASSISTANT-86: A knowledge-elicitation tool forsophisticated users. InI. Bratko & N. LavraE (Eds.), Progress in machine learning. Bled, 
Yugoslavia: Sigma Press. 
Dietterich, T. G., Hild, H., & Bakiri, G. (1995). A comparison of ID3 and BACKPROPAGATION forEnglish text-to-speech mapping. Machine Learning, 18(1), 51-80. 
Dietterich, T. G., Kearns, M., & Mansour, Y. (1996). Applying the weak learning framework tounderstand and improve C4.5. Proceedings of the 13th International Conference on MachineLearning (pp. 96104). San Francisco: Morgan Kaufmann. 
Fayyad, U. M. (1991). On the induction of decision trees for multiple concept leaning, (Ph.D. dis- 
sertation). EECS Department, University of Michigan. 
Cm 3 DECISION TREE LEARNING 79Fayyad, U. M., & Irani, K. B. (1992). On the handling of continuous-valued attributes in decisiontree generation. Machine Learning, 8, 87-102. 
Fayyad, U. M., & Irani, K. B. (1993). Multi-interval discretization of continuous-valued attributesfor classification learning. InR. Bajcsy (Ed.), Proceedings of the 13th International JointConference on ArtiJcial Intelligence (pp. 1022-1027). Morgan-Kaufmann. 
Fayyad, U. M., Weir, N., & Djorgovski, S. (1993). SKICAT: A machine learning system for auto- 
mated cataloging of large scale sky surveys. Proceedings of the Tenth International Conferenceon Machine Learning (pp. 112-1 19). Amherst, MA: Morgan Kaufmann. 
Fisher, D. H., and McKusick, K. B. (1989). An empirical comparison of ID3 and back-propagation. 
Proceedings of the Eleventh International Joint Conference on A1 (pp. 788-793). MorganKaufmann. 
Fnedman, J. H. (1977). A recursive partitioning decision rule for non-parametric classification. IEEETransactions on Computers @p. 404408). 
Hunt, E. B. (1975). Art$cial Intelligence. New Yorc Academic Press. 
Hunt, E. B., Marin, J., & Stone, P. J. (1966). Experiments in Induction. New York: Academic Press. 
Kearns, M., & Mansour, Y. (1996). On the boosting ability of top-down decision tree learningalgorithms. Proceedings of the 28th ACM Symposium on the Theory of Computing. New York: 
ACM Press. 
Kononenko, I., Bratko, I., & Roskar, E. (1984). Experiments in automatic learning of medical diag- 
nostic rules (Technical report). Jozef Stefan Institute, Ljubljana, Yugoslavia. 
Lopez de Mantaras, R. (1991). A distance-based attribute selection measure for decision tree induc- 
tion. Machine Learning, 6(1), 81-92. 
Malerba, D., Floriana, E., & Semeraro, G. (1995). A further comparison of simplification methods fordecision tree. induction. InD. Fisher & H. Lenz (Eds.), Learningfrom data: AI and statistics. 
Springer-Verlag. 
Mehta, M., Rissanen, J., & Agrawal, R. (1995). MDL-based decision tree pruning. Proceedings ofthe First International Conference on Knowledge Discovery and Data Mining (pp. 216-221). 
Menlo Park, CA: AAAI Press. 
Mingers, J. (1989a). An empirical comparison of selection measures for decision-tree induction. 
Machine Learning, 3(4), 319-342. 
Mingers, J. (1989b). An empirical comparison of pruning methods for decision-tree induction. 
Machine Learning, 4(2), 227-243. 
Murphy, P. M., & Pazzani, M. J. (1994). Exploring the decision forest: An empirical investigationof Occam's razor in decision tree induction. Journal of Artijicial Intelligence Research, 1, 
257-275. 
Murthy, S. K., Kasif, S., & Salzberg, S. (1994). A system for induction of oblique decision trees. 
Journal of Art$cial Intelligence Research, 2, 1-33. 
Nunez, M. (1991). The use of background knowledge in decision tree induction. Machine Learning, 
6(3), 23 1-250. 
Pagallo, G., & Haussler, D. (1990). Boolean feature discovery in empirical learning. Machine Learn- 
ing, 5, 71-100. 
Qulnlan, J. R. (1979). Discovering rules by induction from large collections of examples. InD. 
Michie (Ed.), Expert systems in the micro electronic age. Edinburgh Univ. Press. 
Qulnlan, J. R. (1983). Learning efficient classification procedures and their application to chess endgames. InR. S. Michalski, J. G. Carbonell, & T. M. Mitchell (Eds.), Machine learning: Anartificial intelligence approach. San Matw, CA: Morgan Kaufmann. 
Qulnlan, J. R. (1986). Induction of decision trees. Machine Learning, 1(1), 81-106. 
Qulnlan, J. R. (1987). Rule induction with statistical data-a comparison with multiple regression. 
Journal of the Operational Research Society, 38,347-352. 
Quinlan, J.R. (1988). An empirical comparison of genetic and decision-tree classifiers. Proceedingsof the Fifrh International Machine Learning Conference (135-141). San Matw, CA: MorganKaufmann. 
Quinlan, J.R. (1988b). Decision trees and multi-valued attributes. In Hayes, Michie, & Richards 
(Eds.), Machine Intelligence 11, (pp. 305-318). Oxford, England: Oxford University Press. 
80 MACHINE LEARNINGQuinlan, J. R., & Rivest, R. (1989). Information and Computation, (go), 227-248. 
Quinlan, J. R. (1993). C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann. 
Rissanen, J. (1983). A universal prior for integers and estimation by minimum description length. 
Annals of Statistics 11 (2), 416-431. 
Rivest, R. L. (1987). Learning decision lists. Machine Learning, 2(3), 229-246. 
Schaffer, C. (1993). Overfitting avoidance as bias. Machine Learning, 10, 113-152. 
Shavlik, J. W., Mooney, R. J., & Towell, G. G. (1991). Symbolic and neural learning algorithms: anexperimental comparison. Machine kaming, 6(2), 11 1-144. 
Tan, M. (1993). Cost-sensitive learning of classification knowledge and its applications in robotics. 
Machine Learning, 13(1), 1-33. 
Tan, M., & Schlimmer, J. C. (1990). Two case studies in cost-sensitive concept acquisition. Pro- 
ceedings of the AAAZ-90. 
Thrun, S. B. etal. (1991). The Monk's problems: Ape~ormance comparison of different learn- 
ing algorithms, (Technical report CMU-FS-91-197). Computer Science Department, CarnegieMellon Univ., Pittsburgh, PA. 
Turney, P. D. (1995). Cost-sensitive classification: empirical evaluation ofa hybrid genetic decisiontree induction algorithm. Journal of A1 Research, 2, 369409. 
Utgoff, P. E. (1989). Incremental induction of decision trees. Machine Learning, 4(2), 161-186. 
Utgoff, P. E., & Brodley, C. E. (1991). Linear machine decision trees, (COINS Technical Report91-10). University of Massachusetts, Amherst, MA. 
Weiss, S., & Kapouleas, I. (1989). An empirical comparison of pattern recognition, neural nets, 
and machine learning classification methods. Proceedings of the Eleventh IJCAI, (781-787), 
Morgan Kaufmann. 
CHAPTERARTIFICIALNEURALNETWORKSArtificial neural networks (ANNs) provide a general, practical method for learningreal-valued, discrete-valued, and vector-valued functions from examples. Algorithmssuch as BACKPROPAGATION use gradient descent to tune network parameters to bestfit a training set of input-output pairs. ANN learning is robust to errors in the trainingdata and has been successfully applied to problems such as interpreting visual scenes, 
speech recognition, and learning robot control strategies. 
4.1 INTRODUCTIONNeural network learning methods provide a robust approach to approximatingreal-valued, discrete-valued, and vector-valued target functions. For certain typesof problems, such as learning to interpret complex real-world sensor data, artificialneural networks are among the most effective learning methods currently known. 
For example, the BACKPROPAGATION algorithm described in this chapter has provensurprisingly successful in many practical problems such as learning to recognizehandwritten characters (LeCun etal. 1989), learning to recognize spoken words 
(Lang etal. 1990), and learning to recognize faces (Cottrell 1990). One survey ofpractical applications is provided by Rumelhart etal. (1994). 
4.1.1 Biological MotivationThe study of artificial neural networks (ANNs) has been inspired in part by theobservation that biological learning systems are built of very complex webs ofinterconnected neurons. In rough analogy, artificial neural networks are built outof a densely interconnected set of simple units, where each unit takes a numberof real-valued inputs (possibly the outputs of other units) and produces a singlereal-valued output (which may become the input to many other units). 
To develop a feel for this analogy, let us consider a few facts from neuro- 
biology. The human brain, for example, is estimated to contain a densely inter- 
connected network of approximately 1011 neurons, each connected, on average, tolo4 others. Neuron activity is typically excited or inhibited through connections toother neurons. The fastest neuron switching times are known tobe on the order ofloe3 seconds--quite slow compared to computer switching speeds of 10-lo sec- 
onds. Yet humans are able to make surprisingly complex decisions, surprisinglyquickly. For example, it requires approximately lo-' seconds to visually recognizeyour mother. Notice the sequence of neuron firings that can take place during this10-'-second interval cannot possibly be longer than a few hundred steps, giventhe switching speed of single neurons. This observation has led many to speculatethat the information-processing abilities of biological neural systems must followfrom highly parallel processes operating on representations that are distributedover many neurons. One motivation for ANN systems isto capture this kindof highly parallel computation based on distributed representations. Most ANNsoftware runs on sequential machines emulating distributed processes, althoughfaster versions of the algorithms have also been implemented on highly parallelmachines and on specialized hardware designed specifically for ANN applications. 
While ANNs are loosely motivated by biological neural systems, there aremany complexities to biological neural systems that are not modeled by ANNs, 
and many features of the ANNs we discuss here are known tobe inconsistentwith biological systems. For example, we consider here ANNs whose individualunits output a single constant value, whereas biological neurons output a complextime series of spikes. 
Historically, two groups of researchers have worked with artificial neuralnetworks. One group has been motivated by the goal of using ANNs to studyand model biological learning processes. A second group has been motivated bythe goal of obtaining highly effective machine learning algorithms, independent ofwhether these algorithms mirror biological processes. Within this book our interestfits the latter group, and therefore we will not dwell further on biological modeling. 
For more information on attempts to model biological systems using ANNs, see, 
for example, Churchland and Sejnowski (1992); Zornetzer etal. (1994); Gabrieland Moore (1990). 
4.2 NEURAL NETWORK REPRESENTATIONSA prototypical example of ANN learning is provided by Pomerleau's (1993) sys- 
tem ALVINN, which uses a learned ANN to steer an autonomous vehicle drivingat normal speeds on public highways. The input to the neural network isa 30 x 32grid of pixel intensities obtained from a forward-pointed camera mounted on thevehicle. The network output is the direction in which the vehicle is steered. TheANN is trained to mimic the observed steering commands ofa human driving thevehicle for approximately 5 minutes. ALVINN has used its learned networks tosuccessfully drive at speeds upto 70 miles per hour and for distances of 90 mileson public highways (driving in the left lane ofa divided public highway, withother vehicles present). 
Figure 4.1 illustrates the neural network representation used in one versionof the ALVINN system, and illustrates the kind of representation typical of manyANN systems. The network is shown on the left side of the figure, with the inputcamera image depicted below it. Each node (i.e., circle) in the network diagramcorresponds to the output ofa single network unit, and the lines entering the nodefrom below are its inputs. As can be seen, there are four units that receive inputsdirectly from all of the 30 x 32 pixels in the image. These are called "hidden" 
units because their output is available only within the network and is not availableas part of the global network output. Each of these four hidden units computes asingle real-valued output based ona weighted combination of its 960 inputs. Thesehidden unit outputs are then used as inputs toa second layer of 30 "output" units. 
Each output unit corresponds toa particular steering direction, and the outputvalues of these units determine which steering direction is recommended moststrongly. 
The diagrams on the right side of the figure depict the learned weight valuesassociated with one of the four hidden units in this ANN. The large matrix ofblack and white boxes on the lower right depicts the weights from the 30 x 32 pixelinputs into the hidden unit. Here, a white box indicates a positive weight, a blackbox a negative weight, and the size of the box indicates the weight magnitude. 
The smaller rectangular diagram directly above the large matrix shows the weightsfrom this hidden unit to each of the 30 output units. 
The network structure of ALYINN is typical of many ANNs. Here the in- 
dividual units are interconnected in layers that form a directed acyclic graph. Ingeneral, ANNs can be graphs with many types of structures-acyclic or cyclic, 
directed or undirected. This chapter will focus on the most common and practicalANN approaches, which are based on the BACKPROPAGATION algorithm. The BACK- 
PROPAGATION algorithm assumes the network isa fixed structure that correspondsto a directed graph, possibly containing cycles. Learning corresponds to choosinga weight value for each edge in the graph. Although certain types of cycles areallowed, the vast majority of practical applications involve acyclic feed-forwardnetworks, similar to the network structure used by ALVINN. 
4.3 APPROPRIATE PROBLEMS FOR NEURAL NETWORKLEARNINGANN learning is well-suited to problems in which the training data correspondsto noisy, complex sensor data, such as inputs from cameras and microphones. 
E2' StraightAhead1 1 1 30 OutputUnitsn 
30x32 SensorInput Retina1 
FIGURE 4.1Neural network learning to steer an autonomous vehicle. The ALVINN system uses BACKPROPAGA- 
TION to learn to steer an autonomous vehicle (photo at top) driving at speeds upto 70 miles per hour. 
The diagram on the left shows how the image ofa forward-mounted camera is mapped to 960 neuralnetwork inputs, which are fed forward to 4 hidden units, connected to 30 output units. Networkoutputs encode the commanded steering direction. The figure on the right shows weight values forone of the hidden units in this network. The 30 x 32 weights into the hidden unit are displayed inthe large matrix, with white blocks indicating positive and black indicating negative weights. Theweights from this hidden unit to the 30 output units are depicted by the smaller rectangular blockdirectly above the large block. As can be seen from these output weights, activation of this particularhidden unit encourages a turn toward the left. 
~tis also applicable to problems for which more symbolic representations areoften used, such as the decision tree learning tasks discussed in Chapter 3. Inthese cases ANN and decision tree learning often produce results of comparableaccuracy. See Shavlik etal. (1991) and Weiss and Kapouleas (1989) for exper- 
imental comparisons of decision tree and ANN learning. The BACKPROPAGATIONalgorithm is the most commonly used ANN learning technique. Itis appropriatefor problems with the following characteristics: 
0 Instances are represented by many attribute-value pairs. The target functionto be learned is defined over instances that can be described bya vector ofpredefined features, such as the pixel values in the ALVINN example. Theseinput attributes may be highly correlated or independent of one another. 
Input values can be any real values. 
The target function output may be discrete-valued, real-valued, ora vectorof several real- or discrete-valued attributes. For example, in the ALVINNsystem the output isa vector of 30 attributes, each corresponding toa rec- 
ommendation regarding the steering direction. The value of each output issome real number between 0 and 1, which in this case corresponds to theconfidence in predicting the corresponding steering direction. We can alsotrain a single network to output both the steering command and suggestedacceleration, simply by concatenating the vectors that encode these two out- 
put predictions. 
The training examples may contain errors. ANN learning methods are quiterobust to noise in the training data. 
Long training times are acceptable. Network training algorithms typicallyrequire longer training times than, say, decision tree learning algorithms. 
Training times can range from a few seconds to many hours, dependingon factors such as the number of weights in the network, the number oftraining examples considered, and the settings of various learning algorithmparameters. 
Fast evaluation of the learned target function may be required. AlthoughANN learning times are relatively long, evaluating the learned network, inorder to apply itto a subsequent instance, is typically very fast. For example, 
ALVINN applies its neural network several times per second to continuallyupdate its steering command as the vehicle drives forward. 
IThe ability of humans to understand the learned target function is not impor- 
tant. The weights learned by neural networks are often difficult for humans tointerpret. Learned neural networks are less easily communicated to humansthan learned rules. 
The rest of this chapter is organized as follows: We first consider severalalternative designs for the primitive units that make up artificial neural networks 
(perce~trons, linear units, and sigmoid units), along with learning algorithms fortraining single units. We then present the BACKPROPAGATION algorithm for trainingmultilayer networks of such units and consider several general issues such as therepresentational capabilities of ANNs, nature of the hypothesis space search, over- 
fitting problems, and alternatives to the BACKPROPAGATION algorithm. A detailedexample is also presented applying BACKPROPAGATION to face recognition, anddirections are provided for the reader to obtain the data and code to experimentfurther with this application. 
4.4 PERCEPTRONSOne type of ANN system is based ona unit called a perceptron, illustrated inFigure 4.2. A perceptron takes a vector of real-valued inputs, calculates a linearcombination of these inputs, then outputs a 1 if the result is greater than somethreshold and -1 otherwise. More precisely, given inputs xl through x,, the outputo(x1, . . . , x,) computed by the perceptron iso(x1,. ..,x,) = 1 ifwo + wlxl+ ~2x2 + - . + W,X, > 0 
-1 otherwisewhere each wiis a real-valued constant, or weight, that determines the contributionof input xito the perceptron output. Notice the quantity (-wO) isa threshold thatthe weighted combination of inputs wlxl + . . . + wnxn must surpass in order forthe perceptron to output a 1. 
To simplify notation, we imagine an additional constant input xo = 1, al- 
lowing usto write the above inequality asC:=o wixi > 0, orin vector form asiir ..i! > 0. For brevity, we will sometimes write the perceptron function aswhereLearning a perceptron involves choosing values for the weights wo, . . . , w,. 
Therefore, the space Hof candidate hypotheses considered in perceptron learningis the set of all possible real-valued weight vectors. 
4.4.1 Representational Power of PerceptronsWe can view the perceptron as representing a hyperplane decision surface in then-dimensional space of instances (i.e., points). The perceptron outputs a 1 forinstances lying on one side of the hyperplane and outputs a -1 for instanceslying on the other side, as illustrated in Figure 4.3. The equation for this decisionhyperplane is iir . .i! = 0. Of course, some sets of positive and negative examplescannot be separated by any hyperplane. Those that can be separated are calledlinearly separable sets of examples. 
FIGURE 43A perceptron. 
A single perceptron can be used to represent many boolean functions. Forexample, ifwe assume boolean values of 1 (true) and -1 (false), then one way touse a two-input perceptron to implement the AND function isto set the weightswo = -3, and wl = wz = .5. This perceptron can be made to represent the ORfunction instead by altering the threshold towo = -.3. In fact, AND and OR canbe viewed as special cases ofm-of-n functions: that is, functions where at leastm of the n inputs to the perceptron must be true. The OR function corresponds torn = 1 and the AND function tom = n. Any m-of-n function is easily representedusing a perceptron by setting all input weights to the same value (e.g., 0.5) andthen setting the threshold wo accordingly. 
Perceptrons can represent all of the primitive boolean functions AND, OR, 
NAND (1 AND), and NOR (1 OR). Unfortunately, however, some boolean func- 
tions cannot be represented bya single perceptron, such as the XOR functionwhose value is 1 if and only ifxl # xz. Note the set of linearly nonseparabletraining examples shown in Figure 4.3(b) corresponds to this XOR function. 
The ability of perceptrons to represent AND, OR, NAND, and NOR isimportant because every boolean function can be represented by some network ofinterconnected units based on these primitives. In fact, every boolean function canbe represented by some network of perceptrons only two levels deep, in whichFIGURE 4.3The decision surface represented bya two-input perceptron. (a) A set of training examples and thedecision surface ofa perceptron that classifies them correctly. (b) A set of training examples that isnot linearly separable (i.e., that cannot be correctly classified by any straight line). xl and x2 are thePerceptron inputs. Positive examples are indicated by "+", negative by "-". 
the inputs are fed to multiple units, and the outputs of these units are then input toa second, final stage. One way isto represent the boolean function in disjunctivenormal form (i.e., as the disjunction (OR) ofa set of conjunctions (ANDs) ofthe inputs and their negations). Note that the input toan AND perceptron can benegated simply by changing the sign of the corresponding input weight. 
Because networks of threshold units can represent a rich variety of functionsand because single units alone cannot, we will generally be interested in learningmultilayer networks of threshold units. 
4.4.2 The Perceptron Training RuleAlthough we are interested in learning networks of many interconnected units, letus begin by understanding how to learn the weights for a single perceptron. Herethe precise learning problem isto determine a weight vector that causes the per- 
ceptron to produce the correct f 1 output for each of the given training examples. 
Several algorithms are known to solve this learning problem. Here we con- 
sider two: the perceptron rule and the delta rule (a variant of the LMS rule usedin Chapter 1 for learning evaluation functions). These two algorithms are guaran- 
teed to converge to somewhat different acceptable hypotheses, under somewhatdifferent conditions. They are important to ANNs because they provide the basisfor learning networks of many units. 
One way to learn an acceptable weight vector isto begin with randomweights, then iteratively apply the perceptron to each training example, modify- 
ing the perceptron weights whenever it misclassifies an example. This process isrepeated, iterating through the training examples as many times as needed untilthe perceptron classifies all training examples correctly. Weights are modified ateach step according to the perceptron training rule, which revises the weight wiassociated with input xi according to the rulewhereHere tis the target output for the current training example, ois the output generatedby the perceptron, and qis a positive constant called the learning rate. The roleof the learning rate isto moderate the degree to which weights are changed ateach step. Itis usually set to some small value (e.g., 0.1) and is sometimes madeto decay as the number of weight-tuning iterations increases. 
Why should this update rule converge toward successful weight values? Toget an intuitive feel, consider some specific cases. Suppose the training example iscorrectly classified already by the perceptron. In this case, (t - o) is zero, makingAwi zero, so that no weights are updated. Suppose the perceptron outputs a -1, 
when the target output is + 1. To make the perceptron output a + 1 instead of - 1 inthis case, the weights must be altered to increase the value ofG.2. For example, ifxi r 0, then increasing wi will bring the perceptron closer to correctly classifyingthis example. Notice the training rule will increase w, in this case, because (t - o), 
7, and Xi are all positive. For example, ifxi = .8, q = 0.1, t = 1, and o = - 1, 
then the weight update will be Awi = q(t - o)xi = O.1(1 - (-1))0.8 = 0.16. Onthe other hand, ift = -1 and o = 1, then weights associated with positive xi willbe decreased rather than increased. 
In fact, the above learning procedure can be proven to converge within afinite number of applications of the perceptron training rule toa weight vec- 
tor that correctly classifies all training examples, provided the training examplesare linearly separable and provided a sufficiently small 7 is used (see Minskyand Papert 1969). If the data are not linearly separable, convergence is not as- 
sured. 
4.4.3 Gradient Descent and the Delta RuleAlthough the perceptron rule finds a successful weight vector when the trainingexamples are linearly separable, it can fail to converge if the examples are notlinearly separable. A second training rule, called the delta rule, is designed toovercome this difficulty. If the training examples are not linearly separable, thedelta rule converges toward a best-fit approximation to the target concept. 
The key idea behind the delta rule isto use gradient descent to search the hy- 
pothesis space of possible weight vectors to find the weights that best fit the train- 
ing examples. This rule is important because gradient descent provides the basisfor the BACKPROPAGATION algorithm, which can learn networks with many inter- 
connected units. Itis also important because gradient descent can serve as thebasis for learning algorithms that must search through hypothesis spaces contain- 
ing many different types of continuously parameterized hypotheses. 
The delta training rule is best understood by considering the task of trainingan unthresholded perceptron; that is, a linear unit for which the output ois given byThus, a linear unit corresponds to the first stage ofa perceptron, without thethreshold. 
In order to derive a weight learning rule for linear units, let us begin byspecifying a measure for the training error ofa hypothesis (weight vector), relativeto the training examples. Although there are many ways to define this error, onecommon measure that will turn out tobe especially convenient iswhere Dis the set of training examples, tdis the target output for training exampled, and odis the output of the linear unit for training example d. By this definition, 
E(6) is simply half the squared difference between the target output td and thehear unit output od, summed over all training examples. Here we characterizeE asa function of 27, because the linear unit output o depends on this weightvector. Of course E also depends on the particular set of training examples, butwe assume these are fixed during training, sowe do not bother to write Eas anexplicit function of these. Chapter 6 provides a Bayesian justification for choosingthis particular definition ofE. In particular, there we show that under certainconditions the hypothesis that minimizes Eis also the most probable hypothesisin H given the training data. 
4.4.3.1 VISUALIZING THE HYPOTHESIS SPACETo understand the gradient descent algorithm, itis helpful to visualize the entirehypothesis space of possible weight vectors and their associated E values, asillustrated in Figure 4.4. Here the axes wo and wl represent possible values forthe two weights ofa simple linear unit. The wo, wl plane therefore representsthe entire hypothesis space. The vertical axis indicates the error E relative tosome fixed set of training examples. The error surface shown in the figure thussummarizes the desirability of every weight vector in the hypothesis space (wedesire a hypothesis with minimum error). Given the way in which we chose todefine E, for linear units this error surface must always be parabolic with a singleglobal minimum. The specific parabola will depend, of course, on the particularset of training examples. 
FIGURE 4.4Error of different hypotheses. For a linear unit with two weights, the hypothesis space His thewg, wl plane. The vertical axis indicates tk error of the corresponding weight vector hypothesis, 
relative toa fixed set of training examples. The arrow shows the negated gradient at one partic- 
ular point, indicating the direction in the wo, wl plane producing steepest descent along the errorsurface. 
Gradient descent search determines a weight vector that minimizes E bystarting with an arbitrary initial weight vector, then repeatedly modifying it insmall steps. At each step, the weight vector is altered in the direction that producesthe steepest descent along the error surface depicted in Figure 4.4. This processcontinues until the global minimum error is reached. 
4.4.3.2 DERIVATION OF THE GRADIENT DESCENT RULEHow can we calculate the direction of steepest descent along the error surface? 
This direction can be found by computing the derivative ofE with respect to eachcomponent of the vector 2. This vector derivative is called the gradient ofE withrespect to 221, written ~~(iir). 
Notice VE(221) is itself a vector, whose components are the partial derivativesof E with respect to each of the wi. When interpreted asa vector in weightspace, the gradient specijies the direction that produces the steepest increase inE. The negative of this vector therefore gives the direction of steepest decrease. 
For example, the arrow in Figure 4.4 shows the negated gradient -VE(G) for aparticular point in the wo, wl plane. 
Since the gradient specifies the direction of steepest increase ofE, the train- 
ing rule for gradient descent iswhereHere r] isa positive constant called the learning rate, which determines the stepsize in the gradient descent search. The negative sign is present because we wantto move the weight vector in the direction that decreases E. This training rulecan also be written in its component formwherewhich makes it clear that steepest descent is achieved by altering each componentw, ofii in proportion toE. To construct a practical algorithm for iteratively updating weights accordingto Equation (44, we need an efficient way of calculating the gradient at eachstep. Fortunately, this is not difficult. The vector of derivatives that form thegradient can be obtained by differentiating E from Equation (4.2), aswhere xid denotes the single input component xi for training example d. We nowhave an equation that gives in terms of the linear unit inputs xid, outputsOd, and target values td associated with the training examples. Substituting Equa- 
tion (4.6) into Equation (4.5) yields the weight update rule for gradient descentTo summarize, the gradient descent algorithm for training linear units is asfollows: Pick an initial random weight vector. Apply the linear unit to all trainingexamples, then compute Awi for each weight according to Equation (4.7). Updateeach weight wiby adding Awi, then repeat this process. This algorithm is givenin Table 4.1. Because the error surface contains only a single global minimum, 
this algorithm will converge toa weight vector with minimum error, regardlessof whether the training examples are linearly separable, given a sufficiently smalllearning rate qis used. Ifr) is too large, the gradient descent search runs the riskof overstepping the minimum in the error surface rather than settling into it. Forthis reason, one common modification to the algorithm isto gradually reduce thevalue ofr) as the number of gradient descent steps grows. 
4.4.3.3 STOCHASTIC APPROXIMATION TO GRADIENT DESCENTGradient descent isan important general paradigm for learning. Itis a strategy forsearching through a large or infinite hypothesis space that can be applied whenever 
(1) the hypothesis space contains continuously parameterized hypotheses (e.g., theweights ina linear unit), and (2) the error can be differentiated with respect tothese hypothesis parameters. The key practical difficulties in applying gradientdescent are (1) converging toa local minimum can sometimes be quite slow (i.e., 
it can require many thousands of gradient descent steps), and (2) if there aremultiple local minima in the error surface, then there isno guarantee that theprocedure will find the global minimum. 
CHAF'l'ER 4 ARTIFICIAL NEURAL NETWORKS 93 
- - 
~~ADIENT-DEscENT(~~~~~~~~~x~~~~~s, q) 
Each training example isa pair of the form (2, t), where x' is the vector of input values, andt is the target output value. qis the learning rate (e.g., .05). 
. Initialize each w, to some small random value 
. Until the termination condition is met, Do0 Initialize each Awi to zero. 
0 For each (2, t) in trainingaxamples, Dow Input the instance x' to the unit and compute the output oFor each linear unit weight w,, DoFor each linear unit weight wi, DoTABLE 4.1GRADIENT DESCENT algorithm for training a linear unit. To implement the stochastic approximationto gradient descent, Equation (T4.2) is deleted, and Equation (T4.1) replaced bywi cwi +q(t -obi. 
One common variation on gradient descent intended to alleviate these diffi- 
culties is called incremental gradient descent, or alternatively stochastic gradientdescent. Whereas the gradient descent training rule presented in Equation (4.7) 
computes weight updates after summing over a22 the training examples inD, theidea behind stochastic gradient descent isto approximate this gradient descentsearch by updating weights incrementally, following the calculation of the errorfor each individual example. The modified training rule is like the training rulegiven by Equation (4.7) except that aswe iterate through each training examplewe update the weight according towhere t, o, and xi are the target value, unit output, and ith input for the trainingexample in question. To modify the gradient descent algorithm of Table 4.1 toimplement this stochastic approximation, Equation (T4.2) is simply deleted andEquation (T4.1) replaced bywi twi + v (t - o) xi. One way to view this stochasticgradient descent isto consider a distinct error function ~~(6) defined for eachindividual training example das follows1 
Ed (6) = - (td - 0d) 22 
(4.11) 
where t, and od are the target value and the unit output value for training ex- 
ample d. Stochastic gradient descent iterates over the training examples din D, 
at each iteration altering the weights according to the gradient with respect toEd(;). The sequence of these weight updates, when iterated over all trainingexamples, provides a reasonable approximation to descending the gradient withrespect to our original error function E(G). By making the value of 7 (the gradient94 MACHINE LEARNINGdescent step size) sufficiently small, stochastic gradient descent can be made toapproximate true gradient descent arbitrarily closely. The key differences betweenstandard gradient descent and stochastic gradient descent are: 
0 In standard gradient descent, the error is summed over all examples beforeupdating weights, whereas in stochastic gradient descent weights are updatedupon examining each training example. 
. Summing over multiple examples in standard gradient descent requires morecomputation per weight update step. On the other hand, because it uses thetrue gradient, standard gradient descent is often used with a larger step sizeper weight update than stochastic gradient descent. 
r, In cases where there are multiple local minima with respect toE($, stochas- 
tic gradient descent can sometimes avoid falling into these local minimabecause it uses the various VEd(G) rather than VE(6) to guide its search. 
Both stochastic and standard gradient descent methods are commonly used inpractice. 
The training rule in Equation (4.10) is known as the delta rule, or sometimesthe LMS (least-mean-square) rule, Adaline rule, or Widrow-Hoff rule (after itsinventors). In Chapter 1 we referred toit as the LMS weight-update rule whendescribing its use for learning an evaluation function for game playing. Noticethe delta rule in Equation (4.10) is similar to the perceptron training rule inEquation (4.4.2). In fact, the two expressions appear tobe identical. However, 
the rules are different because in the delta rule o refers to the linear unit outputo(2) = i;) .?, whereas for the perceptron rule o refers to the thresholded outputo(2) = sgn($ .2). 
Although we have presented the delta rule asa method for learning weightsfor unthresholded linear units, it can easily be used to train thresholded perceptronunits, as well. Suppose that o = i;) . x' is the unthresholded linear unit output asabove, and of = sgn(G.2) is the result of thresholding oas in the perceptron. Nowif we wish to train a perceptron to fit training examples with target values off 1 foro', we can use these same target values and examples to train o instead, using thedelta rule. Clearly, if the unthresholded output o can be trained to fit these valuesperfectly, then the threshold output of will fit them as well (because sgn(1) = 1, 
and sgn(-1) = -1). Even when the target values cannot be fit perfectly, thethresholded of value will correctly fit the f 1 target value whenever the linearunit output o has the correct sign. Notice, however, that while this procedure willlearn weights that minimize the error in the linear unit output o, these weightswill not necessarily minimize the number of training examples misclassified bythe thresholded output 0'. 
4.4.4 RemarksWe have considered two similar algorithms for iteratively learning perceptronweights. The key difference between these algorithms is that the perceptron train- 
CHmR 4 ARTIFICIAL NEURAL NETWORKS 95ing rule updates weights based on the error in the thresholded perceptron output, 
whereas the delta rule updates weights based on the error in the unthresholdedlinear combination of inputs. 
The difference between these two training rules is reflected in different con- 
vergence properties. The perceptron training rule converges after a finite numberof iterations toa hypothesis that perfectly classifies the training data, provided thetraining examples are linearly separable. The delta rule converges only asymp- 
totically toward the minimum error hypothesis, possibly requiring unboundedtime, but converges regardless of whether the training data are linearly sepa- 
rable. A detailed presentation of the convergence proofs can be found in Hertz etal. (1991). 
A third possible algorithm for learning the weight vector is linear program- 
ming. Linear programming isa general, efficient method for solving sets of linearinequalities. Notice each training example corresponds toan inequality of theform zZI - x' > 0 orG . x' 5 0, and their solution is the desired weight vector. Un- 
fortunately, this approach yields a solution only when the training examples arelinearly separable; however, Duda and Hart (1973, p. 168) suggest a more subtleformulation that accommodates the nonseparable case. In any case, the approachof linear programming does not scale to training multilayer networks, which isour primary concern. In contrast, the gradient descent approach, on which thedelta rule is based, can be easily extended to multilayer networks, as shown inthe following section. 
4.5 MULTILAYER NETWORKS AND THE BACKPROPAGATIONALGORITHMAs noted in Section 4.4.1, single perceptrons can only express linear decisionsurfaces. In contrast, the kind of multilayer networks learned by the BACKPROPA- 
CATION algorithm are capable of expressing a rich variety of nonlinear decisionsurfaces. For example, a typical multilayer network and decision surface isde- 
picted in Figure 4.5. Here the speech recognition task involves distinguishingamong 10 possible vowels, all spoken in the context of "h-d" (i.e., "hid," "had," 
"head," "hood," etc.). The input speech signal is represented by two numericalparameters obtained from a spectral analysis of the sound, allowing usto easilyvisualize the decision surface over the two-dimensional instance space. As shownin the figure, itis possible for the multilayer network to represent highly nonlineardecision surfaces that are much more expressive than the linear decision surfacesof single units shown earlier in Figure 4.3. 
This section discusses how to learn such multilayer networks using a gradientdescent algorithm similar to that discussed in the previous section. 
4.5.1 A Differentiable Threshold UnitWhat type of unit shall we use as the basis for constructing multilayer networks? 
At first we might be tempted to choose the linear units discussed in the previoushead hid 4 who'd hood0 bad 
. hid 
+ hodr hadr hawed 
. hoardo heedc hud 
, who'dhoodFIGURE 4.5Decision regions ofa multilayer feedforward network. The network shown here was trained torecognize 1 of 10 vowel sounds occurring in the context "hd" (e.g., "had," "hid"). The networkinput consists of two parameters, F1 and F2, obtained from a spectral analysis of the sound. The10 network outputs correspond to the 10 possible vowel sounds. The network prediction is theoutput whose value is highest. The plot on the right illustrates the highly nonlinear decision surfacerepresented by the learned network. Points shown on the plot are test examples distinct from theexamples used to train the network. (Reprinted by permission from Haung and Lippmann (1988).) 
section, for which we have already derived a gradient descent learning rule. How- 
ever, multiple layers of cascaded linear units still produce only linear functions, 
and we prefer networks capable of representing highly nonlinear functions. Theperceptron unit is another possible choice, but its discontinuous threshold makesit undifferentiable and hence unsuitable for gradient descent. What we need is aunit whose output isa nonlinear function of its inputs, but whose output is alsoa differentiable function of its inputs. One solution is the sigmoid unit-a unitvery much like a perceptron, but based ona smoothed, differentiable thresholdfunction. 
The sigmoid unit is illustrated in Figure 4.6. Like the perceptron, the sigmoidunit first computes a linear combination of its inputs, then applies a threshold tothe result. In the case of the sigmoid unit, however, the threshold output is anet = Cwi xi 1 o = @net) = - 1 + kMfFIGURE 4.6The sigmoid threshold unit. 
CHAPTER 4 ARTIFICIAL NEURAL NETWORKS 97continuous function of its input. More precisely, the sigmoid unit computes itsoutput o aswherea is often called the sigmoid function or, alternatively, the logistic function. Noteits output ranges between 0 and 1, increasing monotonically with its input (see thethreshold function plot in Figure 4.6.). Because it maps a very large input domainto a small range of outputs, itis often referred toas the squashingfunction ofthe unit. The sigmoid function has the useful property that its derivative is easilyexpressed in terms of its output [in particular, = dyO(Y) . (1 - dy))]. Aswe shall see, the gradient descent learning rule makes use of this derivative. 
Other differentiable functions with easily calculated derivatives are sometimesused in place ofa. For example, the term e-yin the sigmoid function definitionis sometimes replaced bye-k'y where kis some positive constant that determinesthe steepness of the threshold. The function tanh is also sometimes used in placeof the sigmoid function (see Exercise 4.8). 
4.5.2 The BACKPROPAGATION AlgorithmThe BACKPROPAGATION algorithm learns the weights for a multilayer network, 
given a network with a fixed set of units and interconnections. It employs gradi- 
ent descent to attempt to minimize the squared error between the network outputvalues and the target values for these outputs. This section presents the BACKPROP- 
AGATION algorithm, and the following section gives the derivation for the gradientdescent weight update rule used by BACKPROPAGATION. 
Because we are considering networks with multiple output units rather thansingle units as before, we begin by redefining Eto sum the errors over all of thenetwork output unitswhere outputs is the set of output units in the network, and tkd and OM are theI target and output values associated with the kth output unit and training example d. 
The learning problem faced by BACKPROPAGATION isto search a large hypoth- 
esis space defined by all possible weight values for all the units in the network. 
The situation can be visualized in terms ofan error surface similar to that shownfor linear units in Figure 4.4. The error in that diagram is replaced by our newdefinition ofE, and the other dimensions of the space correspond now to all ofthe weights associated with all of the units in the network. Asin the case oftraining a single unit, gradient descent can be used to attempt to find a hypothesisto minimize E. 
B~c~~~o~~GATIO~(trainingaxamp~es, q, ni, , no,, , nhidden) 
Each training example isa pair of the form (2, i ), where x' is the vector of network inputvalues, and is the vector of target network output values. 
qis the learning rate (e.g., .O5). ni, is the number of network inputs, nhidden the number ofunits in the hidden layer, and no,, the number of output units. 
The inputfiom unit i into unit jis denoted xji, and the weight from unit ito unit jis denotedwji. 
a Create a feed-forward network with ni, inputs, midden hidden units, and nour output units. 
a Initialize all network weights to small random numbers (e.g., between -.05 and .05). 
r Until the termination condition is met, Doa For each (2, i ) in trainingaxamples, DoPropagate the input forward through the network: 
1, Input the instance x' to the network and compute the output o, of every unit u inthe network. 
Propagate the errors backward through the network: 
2. For each network output unit k, calculate its error term Sk6k 4- ok(l - ok)(tk - 0k) 
3. For each hidden unit h, calculate its error term 6h4. Update each network weight wjiwhereAw.. - Jl - I 11TABLE 4.2The stochastic gradient descent version of the BACKPROPAGATION algorithm for feedforward networkscontaining two layers of sigmoid units. 
One major difference in the case of multilayer networks is that the error sur- 
face can have multiple local minima, in contrast to the single-minimum parabolicerror surface shown in Figure 4.4. Unfortunately, this means that gradient descentis guaranteed only to converge toward some local minimum, and not necessarilythe global minimum error. Despite this obstacle, in practice BACKPROPAGATION hasbeen found to produce excellent results in many real-world applications. 
The BACKPROPAGATION algorithm is presented in Table 4.2. The algorithm asdescribed here applies to layered feedforward networks containing two layers ofsigmoid units, with units at each layer connected to all units from the precedinglayer. This is the incremental, or stochastic, gradient descent version of BACK- 
PROPAGATION. The notation used here is the same as that used in earlier sections, 
with the following extensions: 
CHAPTER 4 ARTIFICIAL NEURAL NETWORKS 99An index (e.g., an integer) is assigned to each node in the network,wherea "node" is either an input to the network or the output of some unit in thenetwork. 
0 xji denotes the input from node ito unit j, and wji denotes the correspondingweight. 
0 6, denotes the error term associated with unit n. It plays a role analogousto the quantity (t - o) in our earlier discussion of the delta training rule. Aswe shall see later, 6, = - s. 
Notice the algorithm in Table 4.2 begins by constructing a network with thedesired number of hidden and output units and initializing all network weightsto small random values. Given this fixed network structure, the main loop of thealgorithm then repeatedly iterates over the training examples. For each trainingexample, it applies the network to the example, calculates the error of the networkoutput for this example, computes the gradient with respect to the error on thisexample, then updates all weights in the network. This gradient descent step isiterated (often thousands of times, using the same training examples multipletimes) until the network performs acceptably well. 
The gradient descent weight-update rule (Equation [T4.5] in Table 4.2) issimilar to the delta training rule (Equation [4.10]). Like the delta rule, it updateseach weight in proportion to the learning rate r], the input value xji to whichthe weight is applied, and the error in the output of the unit. The only differ- 
ence is that the error (t - o) in the delta rule is replaced bya more complexerror term, aj. The exact form ofaj follows from the derivation of the weight- 
tuning rule given in Section 4.5.3. To understand it intuitively, first considerhow akis computed for each network output unit k (Equation [T4.3] in the al- 
gorithm). akis simply the familiar (tk - ok) from the delta rule, multiplied bythe factor ok(l - ok), which is the derivative of the sigmoid squashing function. 
The ah value for each hidden unit h has a similar form (Equation [T4.4] in thealgorithm). However, since training examples provide target values tk only fornetwork outputs, no target values are directly available to indicate the error ofhidden units' values. Instead, the error term for hidden unit his calculated bysumming the error terms Jk for each output unit influenced byh, weighting eachof the ak'sby wkh, the weight from hidden unit hto output unit k. This weightcharacterizes the degree to which hidden unit his "responsible for" the error inoutput unit k. IThe algorithm in Table 4.2 updates weights incrementally, following the IPresentation of each training example. This corresponds toa stochastic approxi- 
mation to gradient descent. To obtain the true gradient ofE one would sum the6, x,, values over all training examples before altering weight values. 
The weight-update loop in BACKPROPAGATION may be iterated thousands oftimes ina typical application. A variety of termination conditions can be usedto halt the procedure. One may choose to halt after a fixed number of iterationsthrough the loop, or once the error on the training examples falls below somethreshold, or once the error ona separate validation set of examples meets some100 MACHINE LEARNINGcriterion. The choice of termination criterion isan important one, because too fewiterations can fail to reduce error sufficiently, and too many can lead to overfittingthe training data. This issue is discussed in greater detail in Section 4.6.5. 
4.5.2.1 ADDING MOMENTUMBecause BACKPROPAGATION is such a widely used algorithm, many variations havebeen developed. Perhaps the most common isto alter the weight-update rule inEquation (T4.5) in the algorithm by making the weight update on the nth iterationdepend partially on the update that occurred during the (n - 1)th iteration, asfollows: 
Here Awji(n) is the weight update performed during the nth iteration through themain loop of the algorithm, and 0 5 a < 1 isa constant called the momentum. 
Notice the first term on the right of this equation is just the weight-update rule ofEquation (T4.5) in the BACKPROPAGATION algorithm. The second term on the rightis new and is called the momentum term. To see the effect of this momentumterm, consider that the gradient descent search trajectory is analogous to thatof a (momentumless) ball rolling down the error surface. The effect ofa! is toadd momentum that tends to keep the ball rolling in the same direction fromone iteration to the next. This can sometimes have the effect of keeping the ballrolling through small local minima in the error surface, or along flat regions inthe surface where the ball would stop if there were no momentum. It also hasthe effect of gradually increasing the step size of the search in regions where thegradient is unchanging, thereby speeding convergence. 
4.5.2.2 LEARNING IN ARBITRARY ACYCLIC NETWORKSThe definition of BACKPROPAGATION presented in Table 4.2 applies ohy to two- 
layer networks. However, the algorithm given there easily generalizes to feedfor- 
ward networks of arbitrary depth. The weight update rule seen in Equation (T4.5) 
is retained, and the only change isto the procedure for computing 6 values. Ingeneral, the 6, value for a unit rin layer rnis computed from the 6 values at thenext deeper layer rn + 1 according toNotice this is identical to Step 3 in the algorithm of Table 4.2, so all we are reallysaying here is that this step may be repeated for any number of hidden layers inthe network. 
Itis equally straightforward to generalize the algorithm to any directedacyclic graph, regardless of whether the network units are arranged in uniformlayers aswe have assumed upto now. In the case that they are not, the rule forcalculating 6 for any internal unit (i.e., any unit that is not an output) isCHAPTER 4 ARTIFICIAL NEURAL NETWORKS 101where Downstream(r) is the set of units immediately downstream from unit r inthe network: that is, all units whose inputs include the output of unit r. Itis thisgneral form of the weight-update rule that we derive in Section 4.5.3. 
4.5.3 Derivation of the BACKPROPAGATION RuleThis section presents the derivation of the BACKPROPAGATION weight-tuning rule. 
It may be skipped ona first reading, without loss of continuity. 
The specific problem we address here is deriving the stochastic gradient de- 
scent rule implemented by the algorithm in Table 4.2. Recall from Equation (4. ll) 
that stochastic gradient descent involves iterating through the training examplesone ata time, for each training example d descending the gradient of the errorEd with respect to this single example. In other words, for each training exampled every weight wji is updated by adding toit Awjiwhere Edis the error on training example d, summed over all output units in thenetworkHere outputs is the set of output units in the network, tkis the target value of unitk for training example d, and okis the output of unit k given training example d. 
The derivation of the stochastic gradient descent rule is conceptually straight- 
forward, but requires keeping track ofa number of subscripts and variables. Wewill follow the notation shown in Figure 4.6, adding a subscript jto denote tothe jth unit of the network as follows: 
xji = the ith input to unit jwji = the weight associated with the ith input to unit jnetj = xi wjixji (the weighted sum of inputs for unit j) 
oj = the output computed by unit jt, = the target output for unit ja = the sigmoid functionoutputs = the set of units in the final layer of the networkDownstream(j) = the set of units whose immediate inputs include theoutput of unit jWe now derive an expression for 2 in order to implement the stochasticgradient descent rule seen in Equation (4:2l). To begin, notice that weight wjican influence the rest of the network only through netj. Therefore, we can use the102 MACHINE LEARNINGchain rule to writeGiven Equation (4.22), our remaining task isto derive a convenient expressionfor z. We consider two cases in turn: the case where unit jis an output unitfor the network, and the case where jis an internal unit. 
Case 1: raini in^ Rule for Output Unit Weights. Just as wji can influence therest of the network only through net,, net, can influence the network only throughoj. Therefore, we can invoke the chain rule again to writeTo begin, consider just the first term in Equation (4.23) 
The derivatives &(tk - ok12 will be zero for all output units k except when k = j. 
We therefore drop the summation over output units and simply set k = j. 
Next consider the second term in Equation (4.23). Since oj = a(netj), thederivative $ is just the derivative of the sigmoid function, which we havealready noted is equal toa(netj)(l - a(netj)). Therefore, 
Substituting expressions (4.24) and (4.25) into (4.23), we obtainand combining this with Equations (4.21) and (4.22), we have the stochasticgradient descent rule for output unitsNote this training rule is exactly the weight update rule implemented by Equa- 
tions (T4.3) and (T4.5) in the algorithm of Table 4.2. Furthermore, we can seenow that Skin Equation (T4.3) is equal to the quantity -$. In the remainderof this section we will use Sito denote the quantity -% for an arbitrary unit i. 
Case 2: Training Rule for Hidden Unit Weights. In the case where jis aninternal, or hidden unit in the network, the derivation of the training rule for wjimust take into account the indirect ways in which wji can influence the networkoutputs and hence Ed. For this reason, we will find it useful to refer to theset of all units immediately downstream of unit jin the network (i.e., all unitswhose direct inputs include the output of unit j). We denote this set of units byDownstream( j). Notice that netj can influence the network outputs (and thereforeEd) only through the units in Downstream(j). Therefore, we can writeRearranging terms and using Sjto denote -$, we haveandwhich is precisely the general rule from Equation (4.20) for updating internalunit weights in arbitrary acyclic directed graphs. Notice Equation (T4.4) fromTable 4.2 is just a special case of this rule, in which Downstream(j) = outputs. 
4.6 REMARKS ON THE BACKPROPAGATION ALGORITHM4.6.1 Convergence and Local MinimaAs shown above, the BACKPROPAGATION algorithm implements a gradient descentsearch through the space of possible network weights, iteratively reducing theerror E between the training example target values and the network outputs. 
Because the error surface for multilayer networks may contain many differentlocal minima, gradient descent can become trapped in any of these. Asa result, 
BACKPROPAGATION over multilayer networks is only guaranteed to converge towardsome local minimum inE and not necessarily to the global minimum error. 
Despite the lack of assured convergence to the global minimum error, BACK- 
PROPAGATION isa highly effective function approximation method in practice. Inmany practical applications the problem of local minima has not been found tobe as severe as one might fear. To develop some intuition here, consider thatnetworks with large numbers of weights correspond to error surfaces in very highdimensional spaces (one dimension per weight). When gradient descent falls intoa local minimum with respect to one of these weights, it will not necessarily bein a local minimum with respect to the other weights. In fact, the more weights inthe network, the more dimensions that might provide "escape routes" for gradientdescent to fall away from the local minimum with respect to this single weight. 
A second perspective on local minima can be gained by considering themanner in which network weights evolve as the number of training iterationsincreases. Notice that if network weights are initialized to values near zero, thenduring early gradient descent steps the network will represent a very smoothfunction that is approximately linear in its inputs. This is because the sigmoidthreshold function itself is approximately linear when the weights are close tozero (see the plot of the sigmoid function in Figure 4.6). Only after the weightshave had time to grow will they reach a point where they can represent highlynonlinear network functions. One might expect more local minima to exist in theregion of the weight space that represents these more complex functions. Onehopes that by the time the weights reach this point they have already movedclose enough to the global minimum that even local minima in this region areacceptable. 
Despite the above comments, gradient descent over the complex error sur- 
faces represented by ANNs is still poorly understood, and no methods are known topredict with certainty when local minima will cause difficulties. Common heuris- 
tics to attempt to alleviate the problem of local minima include: 
Add a momentum term to the weight-update rule as described in Equa- 
tion (4.18). Momentum can sometimes carry the gradient descent procedurethrough narrow local minima (though in principle it can also carry it throughnarrow global minima into other local minima!). 
Use stochastic gradient descent rather than true gradient descent. As dis- 
cussed in Section 4.4.3.3, the stochastic approximation to gradient descenteffectively descends a different error surface for each training example, re- 
CHAPTER 4 ARTIFICIAL NEURAL NETWORKS 105lying on the average of these to approximate the gradient with respect to thefull training set. These different error surfaces typically will have differentlocal minima, making it less likely that the process will get stuck in any oneof them. 
0 Train multiple networks using the same data, but initializing each networkwith different random weights. If the different training efforts lead to dif- 
ferent local minima, then the network with the best performance over aseparate validation data set can be selected. Alternatively, all networks canbe retained and treated asa "committee" of networks whose output is the 
(possibly weighted) average of the individual network outputs. 
4.6.2 Representational Power of Feedforward NetworksWhat set of functions can be represented by feedfonvard networks? Of coursethe answer depends on the width and depth of the networks. Although much isstill unknown about which function classes can be described by which types ofnetworks, three quite general results are known: 
Boolean functions. Every boolean function can be represented exactly bysome network with two layers of units, although the number of hidden unitsrequired grows exponentially in the worst case with the number of networkinputs. To see how this can be done, consider the following general schemefor representing an arbitrary boolean function: For each possible input vector, 
create a distinct hidden unit and set its weights so that it activates if and onlyif this specific vector is input to the network. This produces a hidden layerthat will always have exactly one unit active. Now implement the outputunit asan OR gate that activates just for the desired input patterns. 
0 Continuous functions. Every bounded continuous function can be approxi- 
mated with arbitrarily small error (under a finite norm) bya network withtwo layers of units (Cybenko 1989; Hornik etal. 1989). The theorem inthis case applies to networks that use sigmoid units at the hidden layer and 
(unthresholded) linear units at the output layer. The number of hidden unitsrequired depends on the function tobe approximated. 
Arbitraryfunctions. Any function can be approximated to arbitrary accuracyby a network with three layers of units (Cybenko 1988). Again, the outputlayer uses linear units, the two hidden layers use sigmoid units, and thenumber of units required at each layer is not known in general. The proofof this involves showing that any function can be approximated bya lin- 
ear combination of many localized functions that have value 0 everywhereexcept for some small region, and then showing that two layers of sigmoidunits are sufficient to produce good local approximations. 
These results show that limited depth feedfonvard networks provide a veryexpressive hypothesis space for BACKPROPAGATION. However, itis important tokeep in mind that the network weight vectors reachable by gradient descent fromthe initial weight values may not include all possible weight vectors. Hertz etal. 
(1991) provide a more detailed discussion of the above results. 
4.6.3 Hypothesis Space Search and Inductive BiasIt is interesting to compare the hypothesis space search of BACKPROPAGATION tothe search performed by other learning algorithms. For BACKPROPAGATION, everypossible assignment of network weights represents a syntactically distinct hy- 
pothesis that in principle can be considered by the learner. In other words, thehypothesis space is the n-dimensional Euclidean space of the n network weights. 
Notice this hypothesis space is continuous, in contrast to the hypothesis spacesof decision tree learning and other methods based on discrete representations. 
The fact that itis continuous, together with the fact that Eis differentiable withrespect to the continuous parameters of the hypothesis, results ina well-definederror gradient that provides a very useful structure for organizing the search forthe best hypothesis. This structure is quite different from the general-to-specificordering used to organize the search for symbolic concept learning algorithms, 
or the simple-to-complex ordering over decision trees used by the ID3 and C4.5algorithms. 
What is the inductive bias by which BACKPROPAGATION generalizes beyondthe observed data? Itis difficult to characterize precisely the inductive bias ofBACKPROPAGATION learning, because it depends on the interplay between the gra- 
dient descent search and the way in which the weight space spans the space ofrepresentable functions. However, one can roughly characterize itas smooth in- 
terpolation between data points. Given two positive training examples with nonegative examples between them, BACKPROPAGATION will tend to label points inbetween as positive examples as well. This can be seen, for example, in the de- 
cision surface illustrated in Figure 4.5, in which the specific sample of trainingexamples gives rise to smoothly varying decision regions. 
4.6.4 Hidden Layer RepresentationsOne intriguing property of BACKPROPAGATION is its ability to discover useful in- 
termediate representations at the hidden unit layers inside the network. Becausetraining examples constrain only the network inputs and outputs, the weight-tuningprocedure is free to set weights that define whatever hidden unit representation ismost effective at minimizing the squared error E. This can lead BACKPROPAGATIONto define new hidden layer features that are not explicit in the input representa- 
tion, but which capture properties of the input instances that are most relevant tolearning the target function. 
Consider, for example, the network shown in Figure 4.7. Here, the eightnetwork inputs are connected to three hidden units, which are in turn connectedto the eight output units. Because of this structure, the three hidden units willbe forced tore-represent the eight input values in some way that captures theirInputs Outputs Input100000000 100000000 100000000100000000100000000 100ooOOOo 100000000 1HiddenValues 
.89 .04 .08 + 
.15 .99 .99 + 
.01 .97 .27 + 
.99 .97 .71 + 
.03 .05 .02 + 
.01 .ll .88 + 
.80 .01 .98 + 
.60 .94 .01 + 
output100000000 100000000 100000000 100000000 100000000 100000000 100000000 1FIGURE 4.7Learned Hidden Layer Representation. This 8 x 3 x 8 network was trained to learn the identityfunction, using the eight training examples shown. After 5000 training epochs, the three hidden unitvalues encode the eight distinct inputs using the encoding shown on the right. Notice if the encodedvalues are rounded to zero or one, the result is the standard binary encoding for eight distinct values. 
relevant features, so that this hidden layer representation can be used by the outputunits to compute the correct target values. 
Consider training the network shown in Figure 4.7 to learn the simple targetfunction f (2) = 2, where 2 isa vector containing seven 0's and a single 1. Thenetwork must learn to reproduce the eight inputs at the corresponding eight outputunits. Although this isa simple function, the network in this case is constrainedto use only three hidden units. Therefore, the essential information from all eightinput units must be captured by the three learned hidden units. 
When BACKPROPAGATION is applied to this task, using each of the eight pos- 
sible vectors as training examples, it successfully learns the target function. Whathidden layer representation is created by the gradient descent BACKPROPAGATIONalgorithm? By examining the hidden unit values generated by the learned networkfor each of the eight possible input vectors, itis easy to see that the learned en- 
coding is similar to the familiar standard binary encoding of eight values usingthree bits (e.g., 000,001,010,. . . , 111). The exact values of the hidden units forone typical run of BACKPROPAGATION are shown in Figure 4.7. 
This ability of multilayer networks to automatically discover useful repre- 
sentations at the hidden layers isa key feature of ANN learning. In contrast tolearning methods that are constrained to use only predefined features provided bythe human designer, this provides an important degree of flexibility that allowsthe learner to invent features not explicitly introduced by the human designer. Ofcourse these invented features must still be computable as sigmoid unit functionsof the provided network inputs. Note when more layers of units are used in thenetwork, more complex features can be invented. Another example of hidden layerfeatures is provided in the face recognition application discussed in Section 4.7. 
In order to develop a better intuition for the operation of BACKPROPAGATIONin this example, let us examine the operation of the gradient descent procedure ingreater detailt. The network in Figure 4.7 was trained using the algorithm shownin Table 4.2, with initial weights set to random values in the interval (-0.1,0.1), 
learning rate q = 0.3, and no weight momentum (i.e., a! = 0). Similar resultswere obtained by using other learning rates and by including nonzero momentum. 
The hidden unit encoding shown in Figure 4.7 was obtained after 5000 trainingiterations through the outer loop of the algorithm (i.e., 5000 iterations through eachof the eight training examples). Most of the interesting weight changes occurred, 
however, during the first 2500 iterations. 
We can directly observe the effect of BACKPROPAGATION'S gradient descentsearch by plotting the squared output error asa function of the number of gradientdescent search steps. This is shown in the top plot of Figure 4.8. Each line inthis plot shows the squared output error summed over all training examples, forone of the eight network outputs. The horizontal axis indicates the number ofiterations through the outermost loop of the BACKPROPAGATION algorithm. As thisplot indicates, the sum of squared errors for each output decreases as the gradientdescent procedure proceeds, more quickly for some output units and less quicklyfor others. 
The evolution of the hidden layer representation can be seen in the secondplot of Figure 4.8. This plot shows the three hidden unit values computed by thelearned network for one of the possible inputs (in particular, 01000000). Again, thehorizontal axis indicates the number of training iterations. As this plot indicates, 
the network passes through a number of different encodings before converging tothe final encoding given in Figure 4.7. 
Finally, the evolution of individual weights within the network is illustratedin the third plot of Figure 4.8. This plot displays the evolution of weights con- 
necting the eight input units (and the constant 1 bias input) to one of the threehidden units. Notice that significant changes in the weight values for this hiddenunit coincide with significant changes in the hidden layer encoding and outputsquared errors. The weight that converges toa value near zero in this case is thebias weight wo. 
4.6.5 Generalization, Overfitting, and Stopping CriterionIn the description oft'le BACKPROPAGATION algorithm in Table 4.2, the terminationcondition for the algcrithm has been left unspecified. What isan appropriate con- 
dition for terrninatinp the weight update loop? One obvious choice isto continuetraining until the errcr Eon the training examples falls below some predeterminedthreshold. In fact, this isa poor strategy because BACKPROPAGATION is suscepti- 
ble to overfitting the training examples at the cost of decreasing generalizationaccuracy over other unseen examples. 
To see the dangers of minimizing the error over the training data, considerhow the error E varies with the number of weight iterations. Figure 4.9 showst~he source code to reproduce this example is available at http://www.cs.cmu.edu/-tom/mlbook.hhnl. 
Sum of squared errors for each output unitHidden unit encoding for input 01000000FIGURE 4.8Learning the 8 x 3 x 8 Network. The top plot shows the evolving sum of squared errors for each ofthe eight output units, as the number of training iterations (epochs) increases. The middle plot showsthe evolving hidden layer representation for the input string "01000000." The bottom plot shows theevolving weights for one of the three hidden units. 
Weights from inputs to one hidden unit4 
32 
1 
-1 
-2I 
.................. ...:........... ......... .:siii..... ziiii 
.. ..................... ...... .... ....-- --- .. ...---- 
....---.-- 
.... - _.. __-. .->-.------ 
/-,-.<-- 
........... ,*' ,.. ... - , . 
... .>, 
... ,'... ... ,,,.- -.. 
................................................ .. ....;, - ..< , . , ,I' ,I 
./;. /- ,/' 
&:>::.--= <, 
" -I-- 
... '.,.. ....... - . ... .. '.. 
. .:. 
- , - -. -- . - - - - - . . _ .., . . . _ . . _ . . 
.... ..................... - 
.-.. ......."... ..... -_ .._ -_ . -. 
-- _ _ _ _ _ _ _ ...... ......................................... 1110 MACHINE LEARNINGError versus weight updates (example 1) 
Validation set error0.0080.0070 5000 loo00 15000 20000Number of weight updates0 lo00 2000 3000 4000 5000 6000Number of weight updatesError versus weight updates (example 2) 
0.08 %** Ir 8FIGURE 4.9Plots of error Eas a function of the number of weight updates, for two different robot perceptiontasks. In both learning cases, error E over the training examples decreases monotonically, as gradientdescent minimizes this measure of error. Error over the separate "validation" set of examples typicallydecreases at first, then may later increase due to overfitting the training examples. The network mostIikeIy to generalize correctly to unseen data is the network with the lowest error over the validationset. Notice in the second plot, one must be careful to not stop training too soon when the validationset error begins to increase. 
0.070.06this variation for two fairly typical applications of BACKPROPAGATION. Considerfirst the top plot in this figure. The lower of the two lines shows the monotoni- 
cally decreasing error E over the training set, as the number of gradient descentiterations grows. The upper line shows the error E measured over a different vali- 
dation set of examples, distinct from the training examples. This line measures thegeneralization accuracy of the network-the accuracy with which it fits examplesbeyond the training data. 
- Training set error * - 
Validation set error + 
y+:LCHAPTER 4 ARTIFICIAL NEURAL NETWORKS 111Notice the generalization accuracy measured over the validation examplesfirst decreases, then increases, even as the error over the training examples contin- 
ues to decrease. How can this occur? This occurs because the weights are beingtuned to fit idiosyncrasies of the training examples that are not representative ofthe general distribution of examples. The large number of weight parameters inANNs provides many degrees of freedom for fitting such idiosyncrasies. 
Why does overfitting tend to occur during later iterations, but not during ear- 
lier iterations? Consider that network weights are initialized to small random val- 
ues. With weights of nearly identical value, only very smooth decision surfaces aredescribable. As training proceeds, some weights begin to grow in order to reducethe error over the training data, and the complexity of the learned decision surfaceincreases. Thus, the effective complexity of the hypotheses that can be reached byBACKPROPAGATION increases with the number of weight-tuning iterations. Givenenough weight-tuning iterations, BACKPROPAGATION will often be able to createoverly complex decision surfaces that fit noise in the training data or unrepresen- 
tative characteristics of the particular training sample. This overfitting problem isanalogous to the overfitting problem in decision tree learning (see Chapter 3). 
Several techniques are available to address the overfitting problem for BACK- 
PROPAGATION learning. One approach, known as weight decay, isto decrease eachweight by some small factor during each iteration. This is equivalent to modifyingthe definition ofE to include a penalty term corresponding to the total magnitudeof the network weights. The motivation for this approach isto keep weight valuessmall, to bias learning against complex decision surfaces. 
One of the most successful methods for overcoming the overfitting problemis to simply provide a set of validation data to the algorithm in addition to thetraining data. The algorithm monitors the error with respect to this validation set, 
while using the training set to drive the gradient descent search. In essence, thisallows the algorithm itself to plot the two curves shown in Figure 4.9. How manyweight-tuning iterations should the algorithm perform? Clearly, it should use thenumber of iterations that produces the lowest error over the validation set, sincethis is the best indicator of network performance over unseen examples. In typicalimplementations of this approach, two copies of the network weights are kept: 
one copy for training and a separate copy of the best-performing weights thus far, 
measured by their error over the validation set. Once the trained weights reach asignificantly higher error over the validation set than the stored weights, trainingis terminated and the stored weights are returned as the final hypothesis. Whenthis procedure is applied in the case of the top plot of Figure 4.9, it outputs thenetwork weights obtained after 9100 iterations. The second plot in Figure 4.9shows that itis not always obvious when the lowest error on the validation sethas been reached. In this plot, the validation set error decreases, then increases, 
then decreases again. Care must be taken to avoid the mistaken conclusion thatthe network has reached its lowest validation set error at iteration 850. 
In general, the issue of overfitting and how to overcome itis a subtle one. 
The above cross-validation approach works best when extra data are available toprovide a validation set. Unfortunately, however, the problem of overfitting is most112 MACHINE LEARNWGI 
severe for small training sets. In these cases, ak-fold cross-validation approachis sometimes used, in which cross validation is performed k different times, eachtime using a different partitioning of the data into training and validation sets, 
and the results are then averaged. In one version of this approach, the m availableexamples are partitioned into k disjoint subsets, each of size m/k. The cross- 
validation procedure is then run k times, each time using a different one of thesesubsets as the validation set and combining the other subsets for the training set. 
Thus, each example is used in the validation set for one of the experiments andin the training set for the other k - 1 experiments. On each experiment the abovecross-validation approach is used to determine the number of iterations i that yieldthe best performance on the validation set. The mean iof these estimates for iis then calculated, and a final run of BACKPROPAGATION is performed training onall n examples for i iterations, with no validation set. This procedure is closelyrelated to the procedure for comparing two learning methods based on limiteddata, described in Chapter 5. 
4.7 AN ILLUSTRATIVE EXAMPLE: FACE RECOGNITIONTo illustrate some of the practical design choices involved in applying BACKPROPA- 
GATION, this section discusses applying itto a learning task involving face recogni- 
tion. All image data and code used to produce the examples described in this sec- 
tion are available at World Wide Web site http://www.cs.cmu.edu/-tomlmlbook. 
html, along with complete documentation on how to use the code. Why not try ityourself? 
4.7.1 The TaskThe learning task here involves classifying camera images of faces of variouspeople in various poses. Images of 20 different people were collected, includingapproximately 32 images per person, varying the person's expression (happy, sad, 
angry, neutral), the direction in which they were looking (left, right, straight ahead, 
up), and whether or not they were wearing sunglasses. As can be seen from theexample images in Figure 4.10, there is also variation in the background behindthe person, the clothing worn by the person, and the position of the person'sface within the image. In total, 624 greyscale images were collected, each with aresolution of 120 x 128, with each image pixel described bya greyscale intensityvalue between 0 (black) and 255 (white). 
A variety of target functions can be learned from this image data. For ex- 
ample, given an image as input we could train an ANN to output the identity ofthe person, the direction in which the person is facing, the gender of the person, 
whether or not they are wearing sunglasses, etc. All of these target functions canbe learned to high accuracy from this image data, and the reader is encouragedto try out these experiments. In the remainder of this section we consider oneparticular task: learning the direction in which the person is facing (to their left, 
right, straight ahead, or upward). I30 x 32 resolution input imagesleft straight rightL 
Network weights after 1 iteration through each training exampleleftNetwork weights after 100 iterations through each training exampleFIGURE 4.10Learning an artificial neural network to recognize face pose. Here a 960 x 3 x 4 network is trainedon grey-level images of faces (see top), to predict whether a person is looking to their left, right, 
ahead, orup. After training on 260 such images, the network achieves an accuracy of 90% over aseparate test set. The learned network weights are shown after one weight-tuning iteration throughthe training examples and after 100 iterations. Each output unit (left, straight, right, up) has fourweights, shown by dark (negative) and light (positive) blocks. The leftmost block corresponds tothe weight wg, which determines the unit threshold, and the three blocks to the right correspond toweights on inputs from the three hidden units. The weights from the image pixels into each hiddenunit are also shown, with each weight plotted in the position of the corresponding image pixel. 
4.7.2 Design ChoicesIn applying BACKPROPAGATION to any given task, a number of design choicesmust be made. We summarize these choices below for our task of learning thedirection in which a person is facing. Although no attempt was made to determinethe precise optimal design choices for this task, the design described here learnsthe target function quite well. After training ona set of 260 images, classificationaccuracy over a separate test set is 90%. In contrast, the default accuracy achievedby randomly guessing one of the four possible face directions is 25%. 
Input encoding. Given that the ANN input isto be some representation of theimage, one key design choice is how to encode this image. For example, we couldpreprocess the image to extract edges, regions of uniform intensity, or other localimage features, then input these features to the network. One difficulty with thisdesign option is that it would lead toa variable number of features (e.g., edges) 
per image, whereas the ANN has a fixed number of input units. The design optionchosen in this case was instead to encode the image asa fixed set of 30 x 32 pixelintensity values, with one network input per pixel. The pixel intensity valuesranging from 0 to 255 were linearly scaled to range from 0 to 1 so that networkinputs would have values in the same interval as the hidden unit and output unitactivations. The 30 x 32 pixel image is, in fact, a coarse resolution summary ofthe original 120 x 128 captured image, with each coarse pixel intensity calculatedas the mean of the corresponding high-resolution pixel intensities. Using thiscoarse-resolution image reduces the number of inputs and network weights toa much more manageable size, thereby reducing computational demands, whilemaintaining sufficient resolution to correctly classify the images. Recall fromFigure 4.1 that the ALVINN system uses a similar coarse-resolution image asinput to the network. One interesting difference is that in ALVINN, each coarseresolution pixel intensity is obtained by selecting the intensity ofa single pixel atrandom from the appropriate region within the high-resolution image, rather thantaking the mean of all pixel intensities within this region. The motivation for thisic ALVINN is that it significantly reduces the computation required to produce thecoarse-resolution image from the available high-resolution image. This efficiencyis especially important when the network must be used to process many imagesper second while autonomously driving the vehicle. 
Output encoding. The ANN must output one of four values indicating the di- 
rection in which the person is looking (left, right, up, or straight). Note we couldencode this four-way classification using a single output unit, assigning outputsof, say, 0.2,0.4,0.6, and 0.8 to encode these four possible values. Instead, weuse four distinct output units, each representing one of the four possible face di- 
rections, with the highest-valued output taken as the network prediction. This isoften called a 1 -0f-n output encoding. There are two motivations for choosing the1-of-n output encoding over the single unit option. First, it provides more degreesof freedom to the network for representing the target function (i.e., there are ntimes as many weights available in the output layer of units). Second, in the 1-of-nencoding the difference between the highest-valued output and the second-highestcan be used asa measure of the confidence in the network prediction (ambiguousclassifications may result in near or exact ties). A further design choice here is 
"what should be the target values for these four output units?' One obvious choicewould beto use the four target values (1,0,0,O) to encode a face looking to theleft, (0,1,0,O) to encode a face looking straight, etc. Instead of 0 and 1 values, 
we use values of 0.1 and 0.9, so that (0.9,O. 1,0.1,0.1) is the target output vectorfor a face looking to the left. The reason for avoiding target values of 0 and 1is that sigmoid units cannot produce these output values given finite weights. Ifwe attempt to train the network to fit target values of exactly 0 and 1, gradientdescent will force the weights to grow without bound. On the other hand, valuesof 0.1 and 0.9 are achievable using a sigmoid unit with finite weights. 
Network graph structure. As described earlier, BACKPROPAGATION can beap- 
plied to any acyclic directed graph of sigmoid units. Therefore, another designchoice we face is how many units to include in the network and how to inter- 
connect them. The most common network structure isa layered network withfeedforward connections from every unit in one layer to every unit in the next. 
In the current design we chose this standard structure, using two layers of sig- 
moid units (one hidden layer and one output layer). Itis common to use one ortwo layers of sigmoid units and, occasionally, three layers. Itis not common touse more layers than this because training times become very long and becausenetworks with three layers of sigmoid units can already express a rich variety oftarget functions (see Section 4.6.2). Given our choice ofa layered feedforwardnetwork with one hidden layer, how many hidden units should we include? In theresults reported in Figure 4.10, only three hidden units were used, yielding a testset accuracy of 90%. In other experiments 30 hidden units were used, yielding atest set accuracy one to two percent higher. Although the generalization accuracyvaried only a small amount between these two experiments, the second experimentrequired significantly more training time. Using 260 training images, the trainingtime was approximately 1 hour ona Sun Sparc5 workstation for the 30 hidden unitnetwork, compared to approximately 5 minutes for the 3 hidden unit network. Inmany applications it has been found that some minimum number of hidden unitsis required in order to learn the target function accurately and that extra hiddenunits above this number do not dramatically affect generalization accuracy, pro- 
vided cross-validation methods are used to determine how many gradient descentiterations should be performed. If such methods are not used, then increasing thenumber of hidden units often increases the tendency to overfit the training data, 
thereby reducing generalization accuracy. 
Other learning algorithm parameters. In these learning experiments the learn- 
ing rate r] was set to 0.3, and the momentum a! was set to 0.3. Lower values for bothparameters produced roughly equivalent generalization accuracy, but longer train- 
ing times. If these values are set too high, training fails to converge toa networkwith acceptable error over the training set. Full gradient descent was used in allthese experiments (in contrast to the stochastic approximation to gradient descentin the algorithm of Table 4.2). Network weights in the output units were initial- 
ized to small random values. However, input unit weights were initialized to zero, 
because this yields much more intelligible visualizations of the learned weights 
(see Figure 4.10), without any noticeable impact on generalization accuracy. Thenumber of training iterations was selected by partitioning the available data intoa training set and a separate validation set. Gradient descent was used to min- 
imize the error over the training set, and after every 50 gradient descent stepsthe performance of the network was evaluated over the validation set. The finalselected network was the one with the highest accuracy over the validation set. 
See Section 4.6.5 for an explanation and justification of this procedure. The finalreported accuracy (e-g., 90% for the network in Figure 4.10) was measured overyet a third set of test examples that were not used in any way to influence training. 
4.7.3 Learned Hidden RepresentationsIt is interesting to examine the learned weight values for the 2899 weights in thenetwork. Figure 4.10 depicts the values of each of these weights after one iterationthrough the weight update for all training examples, and again after 100 iterations. 
To understand this diagram, consider first the four rectangular blocks justbelow the face images in the figure. Each of these rectangles depicts the weightsfor one of the four output units in the network (encoding left, straight, right, andup). The four squares within each rectangle indicate the four weights associatedwith this output unit-the weight wo, which determines the unit threshold (onthe left), followed by the three weights connecting the three hidden units to thisoutput. The brightness of the square indicates the weight value, with bright whiteindicating a large positive weight, dark black indicating a large negative weight, 
and intermediate shades of grey indicating intermediate weight values. For ex- 
ample, the output unit labeled "up" has a near zero wo threshold weight, a largepositive weight from the first hidden unit, and a large negative weight from thesecond hidden unit. 
The weights of the hidden units are shown directly below those for the outputunits. Recall that each hidden unit receives an input from each of the 30 x 32image pixels. The 30 x 32 weights associated with these inputs are displayed sothat each weight isin the position of the corresponding image pixel (with the wothreshold weight superimposed in the top left of the array). Interestingly, one cansee that the weights have taken on values that are especially sensitive to featuresin the region of the image in which the face and body typically appear. 
The values of the network weights after 100 gradient descent iterationsthrough each training example are shown at the bottom of the figure. Notice theleftmost hidden unit has very different weights than it had after the first iteration, 
and the other two hidden units have changed as well. Itis possible to understandto some degree the encoding in this final set of weights. For example, consider theoutput unit that indicates a person is looking to his right. This unit has a strongpositive weight from the second hidden unit and a strong negative weight fromthe third hidden unit. Examining the weights of these two hidden units, itis easyto see that if the person's face is turned to his right (i.e., our left), then his brightskin will roughly align with strong positive weights in this hidden unit, and hisdark hair will roughly align with negative weights, resulting in this unit outputtinga large value. The same image will cause the third hidden unit to output a valueclose to zero, as the bright face will tend to align with the large negative weightsin this case. 
4.8 ADVANCED TOPICS IN ARTIFICIAL NEURAL NETWORKS4.8.1 Alternative Error FunctionsAs noted earlier, gradient descent can be performed for any function E that isdifferentiable with respect to the parameterized hypothesis space. While the basicBAcWROPAGATION algorithm defines Ein terms of the sum of squared errorsof the network, other definitions have been suggested in order to incorporateother constraints into the weight-tuning rule. For each new definition ofE a newweight-tuning rule for gradient descent must be derived. Examples of alternativedefinitions ofE includea Adding a penalty term for weight magnitude. As discussed above, we canadd a term toE that increases with the magnitude of the weight vector. 
This causes the gradient descent search to seek weight vectors with smallmagnitudes, thereby reducing the risk of overfitting. One way todo this isto redefine E aswhich yields a weight update rule identical to the BACKPROPAGATION rule, 
except that each weight is multiplied by the constant (1 - 2yq) upon eachiteration. Thus, choosing this definition ofE is equivalent to using a weightdecay strategy (see Exercise 4.10.) 
a Adding a term for errors in the slope, or derivative of the target func- 
tion. In some cases, training information may be available regarding desiredderivatives of the target function, as well as desired values. For example, 
Simard etal. (1992) describe an application to character recognition in whichcertain training derivatives are used to constrain the network to learn char- 
acter recognition functions that are invariant of translation within the im- 
age. Mitchell and Thrun (1993) describe methods for calculating trainingderivatives based on the learner's prior knowledge. In both of these sys- 
tems (described in Chapter 12), the error function is modified to add a termmeasuring the discrepancy between these training derivatives and the actualderivatives of the learned network. One example of such an error function isHere x: denotes the value of the jth input unit for training example d. 
Thus, 2 is the training derivative describing how the target output value118 MACHINE LEARNINGtkd should vary with a change in the input xi. Similarly, 9 denotes theax, 
corresponding derivative of the actual learned network. The constant ,ude- 
termines the relative weight placed on fitting the training values versus thetraining derivatives. 
0 Minimizing the cross entropy of the network with respect to the target values. 
Consider learning a probabilistic function, such as predicting whether a loanapplicant will pay back a loan based on attributes such as the applicant's ageand bank balance. Although the training examples exhibit only boolean targetvalues (either a 1 or 0, depending on whether this applicant paid back theloan), the underlying target function might be best modeled by outputting theprobability that the given applicant will repay the loan, rather than attemptingto output the actual 1 and 0 value for each input instance. Given suchsituations in which we wish for the network to output probability estimates, 
it can be shown that the best (i.e., maximum likelihood) probability estimatesare given by the network that minimizes the cross entropy, defined asHere odis the probability estimate output by the network for training ex- 
ample d, and tdis the 1 or 0 target value for training example d. Chapter 6discusses when and why the most probable network hypothesis is the onethat minimizes this cross entropy and derives the corresponding gradientdescent weight-tuning rule for sigmoid units. That chapter also describesother conditions under which the most probable hypothesis is the one thatminimizes the sum of squared errors. 
0 Altering the effective error function can also be accomplished by weightsharing, or "tying together" weights associated with different units or inputs. 
The idea here is that different network weights are forced to take on identicalvalues, usually to enforce some constraint known in advance to the humandesigner. For example, Waibel etal. (1989) and Lang etal. (1990) describean application of neural networks to speech recognition, in which the net- 
work inputs are the speech frequency components at different times within a144 millisecond time window. One assumption that can be made in this ap- 
plication is that the frequency components that identify a specific sound (e.g., 
"eee") should be independent of the exact time that the sound occurs withinthe 144 millisecond window. To enforce this constraint, the various units thatreceive input from different portions of the time window are forced to shareweights. The net effect isto constrain the space of potential hypotheses, 
thereby reducing the risk of overfitting and improving the chances for accu- 
rately generalizing to unseen situations. Such weight sharing is typically im- 
plemented by first updating each of the shared weights separately within eachunit that uses the weight, then replacing each instance of the shared weight bythe mean of their values. The result of this procedure is that shared weightseffectively adapt toa different error function than do the unshared weights. 
4.8.2 Alternative Error Minimization ProceduresWhile gradient descent is one of the most general search methods for finding ahypothesis to minimize the error function, itis not always the most efficient. Itis not uncommon for BACKPROPAGATION to require tens of thousands of iterationsthrough the weight update loop when training complex networks. For this reason, 
a number of alternative weight optimization algorithms have been proposed andexplored. To see some of the other possibilities, itis helpful to think ofa weight- 
update method as involving two decisions: choosing a direction in which to alterthe current weight vector and choosing a distance to move. In BACKPROPAGATION, 
the direction is chosen by taking the negative of the gradient, and the distance isdetermined by the learning rate constant q. 
One optimization method, known as line search, involves a different ap- 
proach to choosing the distance for the weight update. In particular, once a line ischosen that specifies the direction of the update, the update distance is chosen byfinding the minimum of the error function along this line. Notice this can resultin a very large or very small weight update, depending on the position of thepoint along the line that minimizes error. A second method, that builds on theidea of line search, is called the conjugate gradient method. Here, a sequence ofline searshes is performed to search for a minimum in the error surface. On thefirst step in this sequence, the direction chosen is the negative of the gradient. 
On each subsequent step, a new direction is chosen so that the component of theerror gradient that has just been made zero, remains zero. 
While alternative error-minimization methods sometimes lead to improvedefficiency in training the network, methods such as conjugate gradient tend tohave no significant impact on the generalization error of the final network. Theonly likely impact on the final error is that different error-minimization proceduresmay fall into different local minima. Bishop (1996) contains a general discussionof several parameter optimization methods for training networks. 
4.8.3 Recurrent NetworksUp to this point we have considered only network topologies that correspondto acyclic directed graphs. Recurrent networks are artificial neural networks thatapply to time series data and that use outputs of network units at time tas theinput to other units at time t + 1. In this way, they support a form of directedcycles in the network. To illustrate, consider the time series prediction task ofpredicting the next day's stock market average y(t + 1) based on the current day'seconomic indicators x(t). Given a time series of such data, one obvious approachis to train a feedforward network to predict y(t + 1) as its output, based on theinput values x(t). Such a network is shown in Figure 4.11(a). 
One limitation of such a network is that the prediction ofy(t + 1) dependsonly onx(t) and cannot capture possible dependencies ofy (t + 1) on earlier valuesof x. This might be necessary, for example, if tomorrow's stock market average 
~(t + 1) depends on the difference between today's economic indicator valuesx(t) and yesterday's values x(t - 1). Of course we could remedy this difficultyI 120 MACHINE LEARNING 
(4 Feedforward network (b) Recurrent networkx(t - 2) c(t - 2) 
(d Recurrent networkunfolded in timeFIGURE 4.11Recurrent networks. 
by making both x(t) and x(t - 1) inputs to the feedforward network. However, 
ifwe wish the network to consider an arbitrary window of time in the past whenpredicting y(t + l), then a different solution is required. The recurrent networkshown in Figure 4.1 1(b) provides one such solution. Here, we have added a newunit bto the hidden layer, and new input unit c(t). The value ofc(t) is definedas the value of unit bat time t - 1; that is, the input value c(t) to the network atone time step is simply copied from the value of unit bon the previous time step. 
Notice this implements a recurrence relation, in which b represents informationabout the history of network inputs. Because b depends on both x(t) and onc(t), 
itis possible for bto summarize information from earlier values ofx that arearbitrarily distant in time. Many other network topologies also can be used toCHAPTER 4 ARTIFICIAL NEURAL NETWORKS 121represent recurrence relations. For example, we could have inserted several layersof units between the input and unit b, and we could have added several contextin parallel where we added the single units b and c. 
How can such recurrent networks be trained? There are several variants ofrecurrent networks, and several training methods have been proposed (see, forexample, Jordan 1986; Elman 1990; Mozer 1995; Williams and Zipser 1995). 
Interestingly, recurrent networks such as the one shown in Figure 4.1 1(b) can betrained using a simple variant of BACKPROPAGATION. TO understand how, considerFigure 4.11(c), which shows the data flow of the recurrent network "unfoldedin time. Here we have made several copies of the recurrent network, replacingthe feedback loop by connections between the various copies. Notice that thislarge unfolded network contains no cycles. Therefore, the weights in the unfoldednetwork can be trained directly using BACKPROPAGATION. Of course in practicewe wish to keep only one copy of the recurrent network and one set of weights. 
Therefore, after training the unfolded network, the final weight wji in the recurrentnetwork can be taken tobe the mean value of the corresponding wji weights inthe various copies. Mozer (1995) describes this training process in greater detail. 
In practice, recurrent networks are more difficult to train than networks with nofeedback loops and do not generalize as reliably. However, they remain importantdue to their increased representational power. 
4.8.4 Dynamically Modifying Network StructureUp to this point we have considered neural network learning asa problem ofadjusting weights within a fixed graph structure. A variety of methods have beenproposed to dynamically grow or shrink the number of network units and intercon- 
nections inan attempt to improve generalization accuracy and training efficiency. 
One idea isto begin with a network containing no hidden units, then growthe network as needed by adding hidden units until the training error is reducedto some acceptable level. The CASCADE-CORRELATION algorithm (Fahlman andLebiere 1990) is one such algorithm. CASCADE-CORRELATION begins by construct- 
ing a network with no hidden units. In the case of our face-direction learning task, 
for example, it would construct a network containing only the four output unitscompletely connected to the 30 x 32 input nodes. After this network is trained forsome time, we may well find that there remains a significant residual error dueto the fact that the target function cannot be perfectly represented bya networkwith this single-layer structure. In this case, the algorithm adds a hidden unit, 
choosing its weight values to maximize the correlation between the hidden unitvalue and the residual error of the overall network. The new unit is now installedinto the network, with its weight values held fixed, and a new connection fromthis new unit is added to each output unit. The process is now repeated. Theoriginal weights are retrained (holding the hidden unit weights fixed), the residualerror is checked, and a second hidden unit added if the residual error is still abovethreshold. Whenever a new hidden unit is added, its inputs include all of the orig- 
inal network inputs plus the outputs of any existing hidden units. The network is122 MACHINE LEARNINGgrown in this fashion, accumulating hidden units until the network residual enoris reduced to some acceptable level. Fahlman and Lebiere (1990) report cases inwhich CASCADE-CORRELATION significantly reduces training times, due to the factthat only a single layer of units is trained at each step. One practical difficultyis that because the algorithm can add units indefinitely, itis quite easy for it tooverfit the training data, and precautions to avoid overfitting must be taken. 
A second idea for dynamically altering network structure isto take theopposite approach. Instead of beginning with the simplest possible network andadding complexity, we begin with a complex network and prune itas we find thatcertain connections are inessential. One way to decide whether a particular weightis inessential isto see whether its value is close to zero. A second way, whichappears tobe more successful in practice, isto consider the effect that a smallvariation in the weight has on the error E. The effect onE of varying w (i.e., g) 
can be taken asa measure of the salience of the connection. LeCun etal. (1990) 
describe a process in which a network is trained, the least salient connectionsremoved, and this process iterated until some termination condition is met. Theyrefer to this as the "optimal brain damage" approach, because at each step thealgorithm attempts to remove the least useful connections. They report that ina character recognition application this approach reduced the number of weightsin a large network bya factor of 4, with a slight improvement in generalizationaccuracy and a significant improvement in subsequent training efficiency. 
In general, techniques for dynamically modifying network structure havemet with mixed success. It remains tobe seen whether they can reliably improveon the generalization accuracy of BACKPROPAGATION. However, they have beenshown in some cases to provide significant improvements in training times. 
4.9 SUMMARY AND FURTHER READINGMain points of this chapter include: 
0 Artificial neural network learning provides a practical method for learningreal-valued and vector-valued functions over continuous and discrete-valuedattributes, ina way that is robust to noise in the training data. The BACKPROP- 
AGATION algorithm is the most common network learning method and hasbeen successfully applied toa variety of learning tasks, such as handwritingrecognition and robot control. 
0 The hypothesis space considered by the BACKPROPAGATION algorithm is thespace of all functions that can be represented by assigning weights to thegiven, fixed network of interconnected units. Feedforward networks contain- 
ing three layers of units are able to approximate any function to arbitraryaccuracy, given a sufficient (potentially very large) number of units in eachlayer. Even networks of practical size are capable of representing a richspace of highly nonlinear functions, making feedforward networks a goodchoice for learning discrete and continuous functions whose general form isunknown in advance. 
BACKPROPAGATION searches the space of possible hypotheses using gradientdescent to iteratively reduce the error in the network fit to the trainingexamples. Gradient descent converges toa local minimum in the trainingerror with respect to the network weights. More generally, gradient descent isa potentially useful method for searching many continuously parameterizedhypothesis spaces where the training error isa differentiable function ofhypothesis parameters. 
One of the most intriguing properties of BACKPROPAGATION is its ability toinvent new features that are not explicit in the input to the network. In par- 
ticular, the internal (hidden) layers of multilayer networks learn to representintermediate features that are useful for learning the target function and thatare only implicit in the network inputs. This capability is illustrated, for ex- 
ample, by the ability of the 8 x 3 x 8 network in Section 4.6.4 to invent theboolean encoding of digits from 1 to 8 and by the image features representedby the hidden layer in the face-recognition application of Section 4.7. 
Overfitting the training data isan important issue in ANN learning. Overfit- 
ting results in networks that generalize poorly to new data despite excellentperformance over the training data. Cross-validation methods can be used toestimate an appropriate stopping point for gradient descent search and thusto minimize the risk of overfitting. 
0 Although BACKPROPAGATION is the most common ANN learning algorithm, 
many others have been proposed, including algorithms for more specializedtasks. For example, recurrent neural network methods train networks con- 
taining directed cycles, and algorithms such as CASCADE CORRELATION alterthe network structure as well as the network weights. 
Additional information on ANN learning can be found in several other chap- 
ters in this book. A Bayesian justification for choosing to minimize the sum ofsquared errors is given in Chapter 6, along with a justification for minimizingthe cross-entropy instead of the sum of squared errors in other cases. Theoreticalresults characterizing the number of training examples needed to reliably learnboolean functions and the Vapnik-Chervonenkis dimension of certain types ofnetworks can be found in Chapter 7. A discussion of overfitting and how to avoidit can be found in Chapter 5. Methods for using prior knowledge to improve thegeneralization accuracy of ANN learning are discussed in Chapter 12. 
Work on artificial neural networks dates back to the very early days ofcomputer science. McCulloch and Pitts (1943) proposed a model ofa neuronthat corresponds to the perceptron, and a good deal of work through the 1960sexplored variations of this model. During the early 1960s Widrow and Hoff (1960) 
explored perceptron networks (which they called "adelines") and the delta rule, 
and Rosenblatt (1962) proved the convergence of the perceptron training rule. 
However, by the late 1960s it became clear that single-layer perceptron networkshad limited representational capabilities, and no effective algorithms were knownfor training multilayer networks. Minsky and Papert (1969) showed that evensimple functions such as XOR could not be represented or learned with single- 
layer perceptron networks, and work on ANNs receded during the 1970s. 
During the mid-1980s work on ANNs experienced a resurgence, caused inlarge part by the invention of BACKPROPAGATION and related algorithms for train- 
ing multilayer networks (Rumelhart and McClelland 1986; Parker 1985). Theseideas can be traced to related earlier work (e.g., Werbos 1975). Since the 1980s, 
BACKPROPAGATION has become a widely used learning method, and many otherANN approaches have been actively explored. The advent of inexpensive com- 
puters during this same period has allowed experimenting with computationallyintensive algorithms that could not be thoroughly explored during the 1960s. 
A number of textbooks are devoted to the topic of neural network learning. 
An early but still useful book on parameter learning methods for pattern recog- 
nition is Duda and Hart (1973). The text by Widrow and Stearns (1985) coversperceptrons and related single-layer networks and their applications. Rumelhartand McClelland (1986) produced an edited collection of papers that helped gen- 
erate the increased interest in these methods beginning in the mid-1980s. Recentbooks on neural network learning include Bishop (1996); Chauvin and Rumelhart 
(1995); Freeman and Skapina (1991); Fu (1994); Hecht-Nielsen (1990); and Hertzet al. (1991). 
EXERCISES4.1. What are the values of weights wo, wl, and w2 for the perceptron whose decisionsurface is illustrated in Figure 4.3? Assume the surface crosses the xl axis at -1, 
and the x2 axis at 2. 
4.2. Design a two-input perceptron that implements the boolean function AA -. B. Designa two-layer network of perceptrons that implements AXO RB. 
4.3. Consider two perceptrons defined by the threshold expression wo + wlxl+ ~2x2 > 0. 
Perceptron A has weight valuesand perceptron B has the weight valuesTrue or false? Perceptron Ais more-general~han perceptron B. (more-general~hanis defined in Chapter 2.) 
4.4. Implement the delta training rule for a two-input linear unit. Train itto fit the targetconcept -2 + XI+ 2x2 > 0. Plot the error Eas a function of the number of trainingiterations. Plot the decision surface after 5, 10, 50, 100, . . . , iterations. 
(a) Try this using various constant values for 17 and using a decaying learning rateof qo/i for the ith iteration. Which works better? 
(b) Try incremental and batch learning. Which converges more quickly? Considerboth number of weight updates and total execution time. 
4.5. Derive a gradient descent training rule for a single unit with output o, where4.6. Explain informally why the delta training rule in Equation (4.10) is only an approx- 
imation to the true gradient descent rule of Equation (4.7). 
4.7. Consider a two-layer feedforward ANN with two inputs a and b, one hidden unit c, 
and one output unit d. This network has five weights (w,, web, wd, wdc, wdO), wherew,o represents the threshold weight for unit x. Initialize these weights to the values 
(. 1, .l, .l, .l, .I), then give their values after each of the first two training iterations ofthe BACKPROPAGATION algorithm. Assume learning rate 17 = .3, momentum a! = 0.9, 
incremental weight updates, and the following training examples: 
abd1010104.8. Revise the BACKPROPAGATION algorithm in Table 4.2 so that it operates on unitsusing the squashing function tanh in place of the sigmoid function. That is, assumethe output ofa single unit iso = tanh(6.x'). Give the weight update rule for outputlayer weights and hidden layer weights. Hint: tanh'(x) = 1 - tanh2(x). 
4.9. Recall the 8 x 3 x 8 network described in Figure 4.7. Consider trying to train a 8 x 1 x 8network for the same task; that is, a network with just one hidden unit. Notice theeight training examples in Figure 4.7 could be represented by eight distinct values forthe single hidden unit (e.g., 0.1,0.2, . . . ,0.8). Could a network with just one hiddenunit therefore learn the identity function defined over these training examples? Hint: 
Consider questions such as "do there exist values for the hidden unit weights thatcan create the hidden unit encoding suggested above?'"do there exist values forthe output unit weights that could correctly decode this encoding of the input?'and 
"is gradient descent likely to find such weights?' 
4.10. Consider the alternative error function described in Section 4.8.1Derive the gradient descent update rule for this definition ofE. Show that it can beimplemented by multiplying each weight by some constant before performing thestandard gradient descent update given in Table 4.2. 
4.11. Apply BACKPROPAGATION to the task of face recognition. See World Wide WebURL http://www.cs.cmu.edu/-tomlbook.html for details, including face-image data, 
BACKPROPAGATION code, and specific tasks. 
4.12. Consider deriving a gradient descent algorithm to learn target concepts correspondingto rectangles in the x, y plane. Describe each hypothesis by the x and y coordinatesof the lower-left and upper-right comers of the rectangle - Ilx, Ily, urn, and uryrespectively. An instance (x, y) is labeled positive by hypothesis (llx, lly, urx, ury) 
if and only if the point (x, y) lies inside the corresponding rectangle. Define errorE asin the chapter. Can you devise a gradient descent algorithm to learn suchrectangle hypotheses? Notice that Eis not a continuous function of llx, Ily, urx, 
and ury, just asin the case of perceptron learning. (Hint: Consider the two solutionsused for perceptrons: (1) changing the classification rule to make output predictionscontinuous functions of the inputs, and (2) defining an alternative error-such asdistance to the rectangle center-asin using the delta rule to train perceptrons.) 
Does your algorithm converge to the minimum error hypothesis when the positiveand negative examples are separable bya rectangle? When they are not? Do youhave problems with local minima? How does your algorithm compare to symbolicmethods for learning conjunctions of feature constraints? 
REFERENCESBishop, C. M. (1996). Neural networks for pattern recognition. Oxford, England: Oxford UniversityPress. 
Chauvin, Y., & Rumelhart, D. (1995). BACKPROPAGATION: Theory, architectures, and applications 
(edited collection). Hillsdale, NJ: Lawrence Erlbaum Assoc. 
Churchland, P. S., & Sejnowski, T. J. (1992). The computational brain. Cambridge, MA: The MITPress. 
Cyhenko, G. (1988). Continuous valued neural networks with two hidden layers are sufficient (Tech- 
nical Report). Department of Computer Science, Tufts University, Medford, MA. 
Cybenko, G. (1989). Approximation by superpositions ofa sigmoidal function. Mathematics of Con- 
trol, Signals, and Systems, 2, 303-3 14. 
Cottrell, G. W. (1990). Extracting features from faces using compression networks: Face, identity, 
emotion and gender recognition using holons. InD. Touretzky (Ed.), Connection Models: 
Proceedings of the 1990 Summer School. San Mateo, CA: Morgan Kaufmann. 
Dietterich, T. G., Hild, H., & Bakiri, G. (1995). A comparison of ID3 and BACKPROPAGATION forEnglish text-to-speech mapping. Machine Learning, 18(1), 51-80. 
Duda, R., & Hart, P. (1973). Pattern class@cation and scene analysis. New York: John Wiley & 
Sons. 
Elman, J. L. (1990). Finding structure in time. Cognitive Science, 14, 179-21 1. 
Fahlman, S., & Lebiere, C. (1990). The CASCADE-CORRELATION learning architecture (TechnicalReport CMU-CS-90-100). Computer Science Department, Carnegie Mellon University, Pitts- 
burgh, PA. 
Freeman, J. A., & Skapura, D. M. (1991). Neural networks. Reading, MA: Addison Wesley. 
Fu, L. (1994). Neural networks in computer intelligence. New York: McGraw Hill. 
Gabriel, M. & Moore, J. (1990). Learning and computational neuroscience: Foundations of adaptivenetworks (edited collection). Cambridge, MA: The MIT Press. 
Hecht-Nielsen, R. (1990). Neurocomputing. Reading, MA: Addison Wesley. 
Hertz, J., Krogh, A., & Palmer, R.G. (1991). Introduction to the theory of neural computation. Read- 
ing, MA: Addison Wesley. 
Homick, K., Stinchcombe, M., & White, H. (1989). Multilayer feedforward networks are universalapproximators. Neural Networks, 2, 359-366. 
Huang, W. Y., & Lippmann, R. P. (1988). Neural net and traditional classifiers. In Anderson (Ed.), 
Neural Information Processing Systems (pp. 387-396). 
Jordan, M. (1986). Attractor dynamics and parallelism ina connectionist sequential machine. Pro- 
ceedings of the Eighth Annual Conference of the Cognitive Science Society (pp. 531-546). 
Kohonen, T. (1984). Self-organization and associative memory. Berlin: Springer-Verlag. 
Lang, K. J., Waibel, A. H., & Hinton, G. E. (1990). A time-delay neural network architecture forisolated word recognition. Neural Networks, 3, 3343. 
LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., & Jackel, L.D. 
(1989). BACKPROPAGATION applied to handwritten zip code recognition. Neural Computa- 
tion, l(4). 
LeCun, Y., Denker, J. S., & Solla, S. A. (1990). Optimal brain damage. InD. Touretzky (Ed.), 
Advances in Neural Information Processing Systems (Vol. 2, pp. 598405). San Mateo, CA: 
Morgan Kaufmann. 
Manke, S., Finke, M. & Waibel, A. (1995). NPEN++: a writer independent, large vocabulary on- 
line cursive handwriting recognition system. Proceedings of the International Conference onDocument Analysis and Recognition. Montreal, Canada: IEEE Computer Society. 
McCulloch, W. S., & Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. 
Bulletin of Mathematical Biophysics, 5, 115-133. 
Mitchell, T. M., & Thrun, S. B. (1993). Explanation-based neural network learning for robot control. 
In Hanson, Cowan, & Giles (Eds.), Advances in neural informution processing systems 5 
(pp. 287-294). San Francisco: Morgan Kaufmann. 
Mozer, M. (1995). A focused BACKPROPAGATION algorithm for temporal pattern recognition. InY. Chauvin & D. Rumelhart (Eds.), Backpropagation: Theory, architectures, and applications 
(pp. 137-169). Hillsdale, NJ: Lawrence Erlbaum Associates. 
Minsky, M., & Papert, S. (1969). Perceptrons. Cambridge, MA: MIT Press. 
Nilsson, N. J. (1965). Learning machines. New York: McGraw Hill. 
Parker, D. (1985). Learning logic (MIT Technical Report TR-47). MIT Center for Research inComputational Economics and Management Science. 
pomerleau, D. A. (1993). Knowledge-based training of artificial neural networks for autonomousrobot driving. InJ. Come11 & S. Mahadevan (Eds.), Robot Learning (pp. 19-43). Boston: 
Kluwer Academic Publishers. 
Rosenblatt, F. (1959). The perceptron: a probabilistic model for information storage and organizationin the brain. Psychological Review, 65, 386-408. 
Rosenblatt, F. (1962). Principles of neurodynamics. New York: Spartan Books. 
Rumelhart, D. E., & McClelland, J. L. (1986). Parallel distributed processing: exploration in themicrostructure of cognition (Vols. 1 & 2). Cambridge, MA: MIT Press. 
Rumelhart, D., Widrow, B., & Lehr, M. (1994). The basic ideas in neural networks. Communicationsof the ACM, 37(3), 87-92. 
Shavlik, J. W., Mooney, R. J., & Towell, G. G. (1991). Symbolic and neural learning algorithms: 
An experimental comparison. Machine Learning, 6(2), 11 1-144. 
Simard, P. S., Victorri, B., LeCun, Y., & Denker, J. (1992). Tangent prop--A formalism for specifyingselected invariances inan adaptive network. In Moody, etal. (Eds.), Advances in NeuralInformation Processing Systems 4 (pp. 895-903). San Francisco: Morgan Kaufmann. 
Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., & Lang, K. (1989). Phoneme recognition usingtime-delay neural networks. ZEEE Transactions on Acoustics, Speech and Signal Processing. 
Weiss, S., & Kapouleas, I. (1989). An empirical comparison of pattern recognition, neural nets, andmachine learning classification methods. Proceedings of the Eleventh ZJCAI @p. 781-787). 
San Francisco: Morgan Kaufmann. 
Werbos, P. (1975). Beyond regression: New tools for prediction and analysis in the behavioral sciences 
(Ph.D. dissertation). Harvard University. 
Widrow, B., & Hoff, M. E. (1960). Adaptive switching circuits. IRE WESCON Convention Record, 
4,96104. 
Widrow, B., & Stearns, S. D. (1985). Adaptive signalprocessing. Signal Processing Series. EnglewoodCliffs, NJ: Prentice Hall. 
Williams, R., & Zipser, D. (1995). Gradient-based learning algorithms for recurrent networks and theircomputational complexity. InY. Chauvin & D. Rumelhart (Eds.), Backpropagation: Theory, 
architectures, and applications (pp. 433-486). Hillsdale, NJ: Lawrence Erlbaum Associates. 
Zometzer, S. F., Davis, J. L., & Lau, C. (1994). An introduction to neural and electronic neiworks 
(edited collection) (2nd ed.). New York: Academic Press. 
CHAPTEREVALUATINGHYPOTHESESEmpirically evaluating the accuracy of hypotheses is fundamental to machine learn- 
ing. This chapter presents an introduction to statistical methods for estimating hy- 
pothesis accuracy, focusing on three questions. First, given the observed accuracyof a hypothesis over a limited sample of data, how well does this estimate its ac- 
curacy over additional examples? Second, given that one hypothesis outperformsanother over some sample of data, how probable isit that this hypothesis is moreaccurate in general? Third, when data is limited what is the best way to use thisdata to both learn a hypothesis and estimate its accuracy? Because limited samplesof data might misrepresent the general distribution of data, estimating true accuracyfrom such samples can be misleading. Statistical methods, together with assump- 
tions about the underlying distributions of data, allow one to bound the differencebetween observed accuracy over the sample of available data and the true accuracyover the entire distribution of data. 
5.1 MOTIVATIONIn many cases itis important to evaluate the performance of learned hypothesesas precisely as possible. One reason is simply to understand whether to use thehypothesis. For instance, when learning from a limited-size database indicatingthe effectiveness of different medical treatments, itis important to understand asprecisely as possible the accuracy of the learned hypotheses. A second reason isthat evaluating hypotheses isan integral component of many learning methods. 
For example, in post-pruning decision trees to avoid overfitting, we must evaluatethe impact of possible pruning steps on the accuracy of the resulting decision tree. 
Therefore itis important to understand the likely errors inherent in estimating theaccuracy of the pruned and unpruned tree. 
Estimating the accuracy ofa hypothesis is relatively straightforward whendata is plentiful. However, when we must learn a hypothesis and estimate itsfuture accuracy given only a limited set of data, two key difficulties arise: 
Bias in the estimate. First, the observed accuracy of the learned hypothesisover the training examples is often a poor estimator of its accuracy overfuture examples. Because the learned hypothesis was derived from theseexamples, they will typically provide an optimistically biased estimate ofhypothesis accuracy over future examples. This is especially likely whenthe learner considers a very rich hypothesis space, enabling itto overfit thetraining examples. To obtain an unbiased estimate of future accuracy, wetypically test the hypothesis on some set of test examples chosen indepen- 
dently of the training examples and the hypothesis. 
a Variance in the estimate. Second, even if the hypothesis accuracy is mea- 
sured over an unbiased set of test examples independent of the trainingexamples, the measured accuracy can still vary from the true accuracy, de- 
pending on the makeup of the particular set of test examples. The smallerthe set of test examples, the greater the expected variance. 
This chapter discusses methods for evaluating learned hypotheses, methodsfor comparing the accuracy of two hypotheses, and methods for comparing theaccuracy of two learning algorithms when only limited data is available. Muchof the discussion centers on basic principles from statistics and sampling theory, 
though the chapter assumes no special background in statistics on the part of thereader. The literature on statistical tests for hypotheses is very large. This chapterprovides an introductory overview that focuses only on the issues most directlyrelevant to learning, evaluating, and comparing hypotheses. 
5.2 ESTIMATING HYPOTHESIS ACCURACYWhen evaluating a learned hypothesis we are most often interested in estimatingthe accuracy with which it will classify future instances. At the same time, wewould like to know the probable error in this accuracy estimate (i.e., what errorbars to associate with this estimate). 
Throughout this chapter we consider the following setting for the learningproblem. There is some space of possible instances X (e.g., the set of all people) 
over which various target functions may be defined (e.g., people who plan topurchase new skis this year). We assume that different instances inX may been- 
countered with different frequencies. A convenient way to model this isto assumethere is some unknown probability distribution D that defines the probability ofencountering each instance inX (e-g., 23 might assign a higher probability toen- 
countering 19-year-old people than 109-year-old people). Notice 23 says nothingabout whether xis a positive or negative example; it only detennines the proba- 
bility that x will be encountered. The learning task isto learn the target conceptor target function fby considering a space Hof possible hypotheses. Trainingexamples of the target function f are provided to the learner bya trainer whodraws each instance independently, according to the distribution D, and who thenforwards the instance x along with its correct target value f (x) to the learner. 
To illustrate, consider learning the target function "people who plan to pur- 
chase new skis this year," given a sample of training data collected by surveyingpeople as they arrive ata ski resort. In this case the instance space Xis the spaceof all people, who might be described by attributes such as their age, occupation, 
how many times they skied last year, etc. The distribution D specifies for eachperson x the probability that x will be encountered as the next person arriving atthe ski resort. The target function f : X + {O,1) classifies each person accordingto whether or not they plan to purchase skis this year. 
Within this general setting we are interested in the following two questions: 
1. Given a hypothesis h and a data sample containing n examples drawn atrandom according to the distribution D, what is the best estimate of theaccuracy ofh over future instances drawn from the same distribution? 
2. What is the probable error in this accuracy estimate? 
5.2.1 Sample Error and True ErrorTo answer these questions, we need to distinguish carefully between two notionsof accuracy or, equivalently, error. One is the error rate of the hypothesis over thesample of data that is available. The other is the error rate of the hypothesis overthe entire unknown distribution Dof examples. We will call these the sampleerror and the true error respectively. 
The sample error ofa hypothesis with respect to some sample Sof instancesdrawn from Xis the fraction ofS that it misclassifies: 
Definition: The sample error (denoted errors(h)) of hypothesis h with respect totarget function f and data sample S isWhere nis the number of examples inS, and the quantity S(f (x), h(x)) is 1 iff (x) # h(x), and 0 otherwise. 
The true error ofa hypothesis is the probability that it will misclassify asingle randomly drawn instance from the distribution D. 
Definition: The true error (denoted errorv(h)) of hypothesis h with respect to targetfunction f and distribution D, is the probability that h will misclassify an instancedrawn at random according toD. 
errorv (h) = Pr [ f (x) # h(x)] 
XEDHere the notation Pr denotes that the probability is taken over the instanceXGVdistribution V. 
What we usually wish to know is the true error errorv(h) of the hypothesis, 
because this is the error we can expect when applying the hypothesis to futureexamples. All we can measure, however, is the sample error errors(h) of thehypothesis for the data sample S that we happen to have in hand. The mainquestion considered in this section is "How good an estimate of errorD(h) isprovided by errors (h)?" 
5.2.2 Confidence Intervals for Discrete-Valued HypothesesHere we give an answer to the question "How good an estimate of errorv(h) isprovided by errors(h)?' for the case in which his a discrete-valued hypothesis. 
More specifically, suppose we wish to estimate the true error for some discrete- 
valued hypothesis h, based on its observed sample error over a sample S, where0 the sample S contains n examples drawn independent of one another, andindependent ofh, according to the probability distribution V0 nz300 hypothesis h commits r errors over these n examples (i.e., errors(h) = rln). 
Under these conditions, statistical theory allows usto make the following asser- 
tions: 
1. Given no other information, the most probable value of errorD(h) is errors(h) 
2. With approximately 95% probability, the true error errorv(h) lies in theintervalerrors(h)(l - errors (h)) 
errors(h) f 1.96 7To illustrate, suppose the data sample S contains n = 40 examples and thathypothesis h commits r = 12 errors over this data. In this case, the sample errorerrors(h) = 12/40 = .30. Given no other information, the best estimate of the trueerror errorD(h) is the observed sample error .30. However, wedo not expect thisto bea perfect estimate of the true error. Ifwe were to collect a second sampleS' containing 40 new randomly drawn examples, we might expect the sampleerror errors,(h) to vary slightly from the sample error errors(h). We expect adifference due to the random differences in the makeup ofS and S'. In fact, ifwe repeated this experiment over and over, each time drawing a new sampleS, containing 40 new examples, we would find that for approximately 95% ofthese experiments, the calculated interval would contain the true error. For thisreason, we call this interval the 95% confidence interval estimate for errorv(h). 
In the current example, where r = 12 and n = 40, the 95% confidence interval is, 
according to the above expression, 0.30 f (1.96 - .07) = 0.30 f .14. 
ConfidencelevelN%: 50% 68% 80% 90% 95% 98% 99% 
Constant ZN: 0.67 1.00 1.28 1.64 1.96 2.33 2.58TABLE 5.1Values ofz~ for two-sided N% confidence intervals. 
The above expression for the 95% confidence interval can be generalized toany desired confidence level. The constant 1.96 is used in case we desire a 95% 
confidence interval. A different constant, ZN, is used to calculate the N% confi- 
dence interval. The general expression for approximate N% confidence intervalsfor errorv(h) iswhere the constant ZNis chosen depending on the desired confidence level, usingthe values ofz~ given in Table 5.1. 
Thus, just aswe could calculate the 95% confidence interval for errorv(h) tobe 0.305 (1.96. .07) (when r = 12, n = 40), we can calculate the 68% confidenceinterval in this case tobe 0.30 f (1.0 - .07). Note it makes intuitive sense that the68% confidence interval is smaller than the 95% confidence interval, because wehave reduced the probability with which we demand that errorv(h) fall into theinterval. 
Equation (5.1) describes how to calculate the confidence intervals, or errorbars, for estimates of errorv(h) that are based on errors(h). In using this ex- 
pression, itis important to keep in mind that this applies only to discrete-valuedhypotheses, that it assumes the sample Sis drawn at random using the samedistribution from which future data will be drawn, and that it assumes the datais independent of the hypothesis being tested. We should also keep in mind thatthe expression provides only an approximate confidence interval, though the ap- 
proximation is quite good when the sample contains at least 30 examples, anderrors(h) is not too close to 0 or 1. A more accurate rule of thumb is that theabove approximation works well whenAbove we summarized the procedure for calculating confidence intervals fordiscrete-valued hypotheses. The following section presents the underlying statis- 
tical justification for this procedure. 
5.3 BASICS OF SAMPLING THEORYThis section introduces basic notions from statistics and sampling theory, in- 
cluding probability distributions, expected value, variance, Binomial and Normaldistributions, and two-sided and one-sided intervals. A basic familiarity with thesea A random variable can be viewed as the name ofan experiment with a probabilistic outcome. Itsvalue is the outcome of the experiment. 
A probability distribution for a random variable Y specifies the probability Pr(Y = yi) that Y willtake on the value yi, for each possible value yi. 
The expected value, or mean, ofa random variable Yis E[Y] = Ciyi Pr(Y = yi). The symbolp)~ is commonly used to represent E[Y]. 
The variance ofa random variable is Var(Y) = E[(Y - p~)~]. The variance characterizes thewidth or dispersion of the distribution about its mean. 
a The standard deviation ofY is JVar(Y). The symbol uyis often used used to represent thestandard deviation ofY. 
The Binomial distribution gives the probability of observing r heads ina series ofn independentcoin tosses, if the probability of heads ina single toss isp. 
a The Normal distribution isa bell-shaped probability distribution that covers many naturalphenomena. 
The Central Limit Theorem isa theorem stating that the sum ofa large number of independent, 
identically distributed random variables approximately follows a Normal distribution. 
An estimator isa random variable Y used to estimate some parameter pof an underlying popu- 
lation. 
a The estimation bias ofY asan estimator for pis the quantity (E[Y] - p). An unbiased estimatoris one for which the bias is zero. 
aA N% conjidence interval estimate for parameter pis an interval that includes p with probabil- 
ity N%. 
TABLE 5.2 , 
Basic definitions and facts from statistics. 
concepts is important to understanding how to evaluate hypotheses and learningalgorithms. Even more important, these same notions provide an important con- 
ceptual framework for understanding machine learning issues such as overfittingand the relationship between successful generalization and the number of trainingexamples considered. The reader who is already familiar with these notions mayskip or skim this section without loss of continuity. The key concepts introducedin this section are summarized in Table 5.2. 
5.3.1 Error Estimation and Estimating Binomial ProportionsPrecisely how does the deviation between sample error and true error dependon the size of the data sample? This question isan instance ofa well-studiedproblem in statistics: the problem of estimating the proportion ofa population thatexhibits some property, given the observed proportion over some random sampleof the population. In our case, the property of interest is that h misclassifies theexample. 
The key to answering this question isto note that when we measure thesample error we are performing an experiment with a random outcome. We firstcollect a random sample Sof n independently drawn instances from the distribu- 
tion D, and then measure the sample error errors(h). As noted in the previoussection, ifwe were to repeat this experiment many times, each time drawing adifferent random sample Siof size n, we would expect to observe different valuesfor the various errors,(h), depending on random differences in the makeup ofthe various Si. We say in such cases that errors, (h), the outcome of the ith suchexperiment, isa random variable. In general, one can think ofa random variableas the name ofan experiment with a random outcome. The value of the randomvariable is the observed outcome of the random experiment. 
Imagine that we were to run k such random experiments, measuring the ran- 
dom variables errors, (h), errors, (h) . . . errors, (h). Imagine further that we thenplotted a histogram displaying the frequency with which we observed each possi- 
ble error value. Aswe allowed kto grow, the histogram would approach the formof the distribution shown in Table 5.3. This table describes a particular probabilitydistribution called the Binomial distribution. 
Binomial dishibution for n = 40, p =0.30.140.120.10.08 
'F 0.060.040.020 
0 5 10 15 20 25 30 35 40A Binomial distribution gives the probability of observing r heads ina sample ofn independentcoin tosses, when the probability of heads ona single coin toss isp. Itis defined by the probabilityfunctionn! 
P(r) = - pr(l - p)"-' 
r!(n - r)! 
If the random variable X follows a Binomial distribution, then: 
0 The probability Pr(X = r) that X will take on the value ris given byP(r) 
0 The expected, or mean value ofX, E[X], is0 The variance ofX, Var(X), isVar (X) = np(1- p) 
0 The standard deviation ofX, ax, isFor sufficiently large values ofn the Binomial distribution is closely approximated bya Normaldistribution (see Table 5.4) with the same mean and variance. Most statisticians recommend usingthe Normal approximation only when np(1- p) 2 5. 
TABLE 53The Binomial distribution. 
5.3.2 The Binomial DistributionA good way to understand the Binomial distribution isto consider the followingproblem. You are given a worn and bent coin and asked to estimate the probabilitythat the coin will turn up heads when tossed. Let us call this unknown probabilityof heads p. You toss the coin n times and record the number of times r that itturns up heads. A reasonable estimate ofp is rln. Note that if the experimentwere rerun, generating a new set ofn coin tosses, we might expect the numberof heads rto vary somewhat from the value measured in the first experiment, 
yielding a somewhat different estimate for p. The Binomial distribution describesfor each possible value ofr (i.e., from 0 ton), the probability of observing exactlyr heads given a sample ofn independent tosses ofa coin whose true probabilityof heads isp. 
Interestingly, estimating p from a random sample of coin tosses is equivalentto estimating errorv(h) from testing hon a random sample of instances. A singletoss of the coin corresponds to drawing a single random instance from 23 anddetermining whether itis misclassified byh. The probability p that a single randomcoin toss will turn up heads corresponds to the probability that a single instancedrawn at random will be misclassified (i.e., p corresponds to errorv(h)). Thenumber rof heads observed over a sample ofn coin tosses corresponds to thenumber of misclassifications observed over n randomly drawn instances. Thus rlncorresponds to errors(h). The problem of estimating p for coins is identical tothe problem of estimating errorv(h) for hypotheses. The Binomial distributiongives the general form of the probability distribution for the random variable r, 
whether it represents the number of heads inn coin tosses or the number ofhypothesis errors ina sample ofn examples. The detailed form of the Binomialdistribution depends on the specific sample size n and the specific probability por errorv(h). 
The general setting to which the Binomial distribution applies is: 
1. There isa base, or underlying, experiment (e.g., toss of the coin) whoseoutcome can be described bya random variable, say Y. The random variableY can take on two possible values (e.g., Y = 1 if heads, Y = 0 if tails). 
2. The probability that Y = 1 on any single trial of the underlying experimentis given by some constant p, independent of the outcome of any otherexperiment. The probability that Y = 0 is therefore (1 - p). Typically, p isnot known in advance, and the problem isto estimate it. 
3. A series ofn independent trials of the underlying experiment is performed 
(e.g., n independent coin tosses), producing the sequence of independent, 
identically distributed random variables Yl, Yz, . . . , Yn. Let R denote thenumber of trials for which Yi = 1 in this series ofn experiments4. The probability that the random variable R will take ona specific value r 
(e.g., the probability of observing exactly r heads) is given by the Binomialdistributionn! 
Pr(R = r) = pr(l - p)"-' r!(n - r)! 
A plot of this probability distribution is shown in Table 5.3. 
The Binomial distribution characterizes the probability of observing r heads fromn coin flip experiments, as well as the probability of observing r errors ina datasample containing n randomly drawn instances. 
5.3.3 Mean and VarianceTwo properties ofa random variable that are often of interest are its expectedvalue (also called its mean value) and its variance. The expected value is_theaverage of the values taken onby repeatedly sampling the random variable. MorepreciselyDefinition: Consider a random variable Y that takes on the possible values yl, . . . yn. 
The expected value ofY, E[Y], isFor example, ifY takes on the value 1 with probability .7 and the value 2 withprobability .3, then its expected value is (1 .0.7 + 2.0.3 = 1.3). In case the randomvariable Yis governed bya Binomial distribution, then it can be shown thatE [Y] = np (5.4) 
where n and p are the parameters of the Binomial distribution defined in Equa- 
tion (5.2). 
A second property, the variance, captures the "width or "spread" of theprobability distribution; that is, it captures how far the random variable is expectedto vary from its mean value. 
Definition: The variance ofa random variable Y, Var[Y], isVar[Y] = E[(Y - E[Y])~] (5.5) 
The variance describes the expected squared error in using a single obser- 
vation ofY to estimate its mean E[Y]. The square root of the variance is calledthe standard deviation ofY, denoted oy . 
Definition: The standard deviation ofa random variable Y, uy, isIn case the random variable Yis governed bya Binomial distribution, then thevariance and standard deviation are given by5.3.4 Estimators, Bias, and VarianceNow that we have shown that the random variable errors(h) obeys a Binomialdistribution, we return to our primary question: What is the likely differencebetween errors(h) and the true error errorv(h)? 
Let us describe errors(h) and errorv(h) using the terms in Equation (5.2) 
defining the Binomial distribution. We then havewhere nis the number of instances in the sample S, ris the number of instancesfrom S misclassified byh, and pis the probability of misclassifying a singleinstance drawn from 23. 
Statisticians call errors(h) an estimator for the true error errorv(h). Ingeneral, an estimator is any random variable used to estimate some parameter ofthe underlying population from which the sample is drawn. An obvious questionto ask about any estimator is whether on average it gives the right estimate. Wedefine the estimation bias tobe the difference between the expected value of theestimator and the true value of the parameter. 
Definition: The estimation bias ofan estimator Y for an arbitrary parameter p isIf the estimation bias is zero, we say that Yis an unbiased estimator for p. Noticethis will be the case if the average of many random values ofY generated byrepeated random experiments (i.e., E[Y]) converges toward p. 
Is errors(h) an unbiased estimator for errorv(h)? Yes, because for aBi- 
nomial distribution the expected value ofr is equal tonp (Equation r5.41). Itfollows, given that nis a constant, that the expected value of rln isp. 
Two quick remarks are in order regarding the estimation bias. First, whenwe mentioned at the beginning of this chapter that testing the hypothesis on thetraining examples provides an optimistically biased estimate of hypothesis error, 
itis exactly this notion of estimation bias to which we were referring. In order forerrors(h) to give an unbiased estimate of errorv(h), the hypothesis h and sampleS must be chosen independently. Second, this notion of estimation bias shouldnot be confused with the inductive bias ofa learner introduced in Chapter 2. Theestimation bias isa numerical quantity, whereas the inductive bias isa set ofassertions. 
A second important property of any estimator is its variance. Given a choiceamong alternative unbiased estimators, it makes sense to choose the one withleast variance. By our definition of variance, this choice will yield the smallestexpected squared error between the estimate and the true value of the parameter. 
To illustrate these concepts, suppose we test a hypothesis and find that itcommits r = 12 errors ona sample ofn = 40 randomly drawn test examples. 
Then an unbiased estimate for errorv(h) is given by errors(h) = rln = 0.3. 
The variance in this estimate arises completely from the variance inr, becausen isa constant. Because ris Binomially distributed, its variance is given byEquation (5.7) asnp(1 - p). Unfortunately pis unknown, but we can substituteour estimate rln for p. This yields an estimated variance inr of 40. 0.3(1 - 
0.3) = 8.4, ora corresponding standard deviation ofa ;j: 2.9. his impliesthat the standard deviation in errors(h) = rln is approximately 2.9140 = .07. Tosummarize, errors(h) in this case is observed tobe 0.30, with a standard deviationof approximately 0.07. (See Exercise 5.1 .) 
In general, given r errors ina sample ofn independently drawn test exam- 
ples, the standard deviation for errors(h) is given bywhich can be approximated by substituting rln = errors(h) for p5.3.5 Confidence IntervalsOne common way to describe the uncertainty associated with an estimate is togive an interval within which the true value is expected to fall, along with theprobability with which itis expected to fall into this interval. Such estimates arecalled conjdence interval estimates. 
Definition: AnN% confidence interval for some parameter pis an interval that isexpected with probability N% to contain p. 
For example, ifwe observe r = 12 errors ina sample ofn = 40 independentlydrawn examples, we can say with approximately 95% probability that the interval0.30 f 0.14 contains the true error errorv(h). 
How can we derive confidence intervals for errorv(h)? The answer lies inthe fact that we know the Binomial probability distribution governing the estima- 
tor errors(h). The mean value of this distribution is errorV(h), and the standarddeviation is given by Equation (5.9). Therefore, to derive a 95% confidence in- 
terval, we need only find the interval centered around the mean value errorD(h), 
which is wide enough to contain 95% of the total probability under this distribu- 
tion. This provides an interval surrounding errorv(h) into which errors(h) mustfall 95% of the time. Equivalently, it provides the size of the interval surroundingerrordh) into which errorv(h) must fall 95% of the time. 
For a given value ofN how can we find the size of the interval that con- 
tains N% of the probability mass? Unfortunately, for the Binomial distributionthis calculation can be quite tedious. Fortunately, however, an easily calculatedand very good approximation can be found in most cases, based on the fact thatfor sufficiently large sample sizes the Binomial distribution can be closely ap- 
proximated by the Normal distribution. The Normal distribution, summarized inTable 5.4, is perhaps the most well-studied probability distribution in statistics. 
As illustrated in Table 5.4, itis a bell-shaped distribution fully specified by itsNormal dismbution with mean 0. standard deviation I3 -2 -1 0 1 2 3A Normal distribution (also called a Gaussian distribution) isa bell-shaped distribution defined bythe probability density functionA Normal distribution is fully determined by two parameters in the above formula: p and a. 
If the random variable X follows a normal distribution, then: 
0 The probability that X will fall into the interval (a, 6) is given byThe expected, or mean value ofX, E[X], isThe variance ofX, Var(X), isVar(X) = a20 The standard deviation ofX, ax, isax = aThe Central Limit Theorem (Section 5.4.1) states that the sum ofa large number of independent, 
identically distributed random variables follows a distribution that is approximately Normal. 
TABLE 5.4The Normal or Gaussian distribution. 
mean p and standard deviation a. For large n, any Binomial distribution is veryclosely approximated bya Normal distribution with the same mean and variance. 
One reason that we prefer to work with the Normal distribution is that moststatistics references give tables specifying the size of the interval about the meanthat contains N% of the probability mass under the Normal distribution. This isprecisely the information needed to calculate our N% confidence interval. In fact, 
Table 5.1 is such a table. The constant ZN given in Table 5.1 defines the widthof the smallest interval about the mean that includes N% of the total probabilitymass under the bell-shaped Normal distribution. More precisely, ZN gives half thewidth of the interval (i.e., the distance from the mean in either direction) measuredin standard deviations. Figure 5.l(a) illustrates such an interval for z.80. 
To summarize, ifa random variable Y obeys a Normal distribution withmean p and standard deviation a, then the measured random value yof Y willfall into the following interval N% of the timeEquivalently, the mean p will fall into the following interval N% of the timeWe can easily combine this fact with earlier facts to derive the generalexpression for N% confidence intervals for discrete-valued hypotheses given inEquation (5.1). First, we know that errors(h) follows a Binomial distribution withmean value error~(h) and standard deviation as given in Equation (5.9). Second, 
we know that for sufficiently large sample size n, this Binomial distribution iswell approximated bya Normal distribution. Third, Equation (5.1 1) tells us howto find the N% confidence interval for estimating the mean value ofa Normaldistribution. Therefore, substituting the mean and standard deviation of errors(h) 
into Equation (5.1 1) yields the expression from Equation (5.1) for N% confidenceFIGURE 5.1A Normal distribution with mean 0, standard deviation 1. (a) With 80% confidence, the value ofthe random variable will lie in the two-sided interval [-1.28,1.28]. Note 2.80 = 1.28. With 10% 
confidence it will lie to the right of this interval, and with 10% confidence it will lie to the left. 
(b) With 90% confidence, it will lie in the one-sided interval [-oo, 1.281. 
intervals for discrete-valued hypothesesJ errors(h)(l - errors(h)) 
errors(h) zt ZNn 
Recall that two approximations were involved in deriving this expression, namely: 
1. in estimating the standard deviation aof errors(h), we have approximatederrorv(h) by errors(h) [i.e., in going from Equation (5.8) to (5.9)], and2. the Binomial distribution has been approximated by the Normal distribution. 
The common rule of thumb in statistics is that these two approximations are verygood as long asn 2 30, or when np(1- p) 2 5. For smaller values ofn itis wiseto use a table giving exact values for the Binomial distribution. 
5.3.6 Two-sided and One-sided BoundsNotice that the above confidence interval isa two-sided bound; that is, it boundsthe estimated quantity from above and from below. In some cases, we will beinterested only ina one-sided bound. For example, we might be interested in thequestion "What is the probability that errorz,(h) isat most U?' This kind of one- 
sided question is natural when we are only interested in bounding the maximumerror ofh and do not mind if the true error is much smaller than estimated. 
There isan easy modification to the above procedure for finding such one- 
sided error bounds. It follows from the fact that the Normal distribution is syrnrnet- 
ric about its mean. Because of this fact, any two-sided confidence interval based ona Normal distribution can be converted toa corresponding one-sided interval withtwice the confidence (see Figure 5.l(b)). That is, a 100(1- a)% confidence inter- 
val with lower bound L and upper bound U implies a 100(1- a/2)% confidenceinterval with lower bound L and no upper bound. It also implies a 100(1 -a/2)% 
confidence interval with upper bound U and no lower bound. Here a correspondsto the probability that the correct value lies outside the stated interval. In otherwords, ais the probability that the value will fall into the unshaded region inFigure 5.l(a), and a/2 is the probability that it will fall into the unshaded regionin Figure 5.l(b). 
To illustrate, consider again the example in which h commits r = 12 errorsover a sample ofn = 40 independently drawn examples. As discussed above, 
this leads toa (two-sided) 95% confidence interval of 0.30 f 0.14. In this case, 
100(1 - a) = 95%, soa! = 0.05. Thus, we can apply the above rule to say with100(1 - a/2) = 97.5% confidence that errorv(h) isat most 0.30 + 0.14 = 0.44, 
making no assertion about the lower bound on errorv(h). Thus, we have a one- 
sided error bound on errorv(h) with double the confidence that we had in thecorresponding two-sided bound (see Exercise 5.3). 
142 MACHINE LEARNING5.4 A GENERAL APPROACH FOR DERIVING CONFIDENCEINTERVALSThe previous section described in detail how to derive confidence interval es- 
timates for one particular case: estimating errorv(h) for a discrete-valued hy- 
pothesis h, based ona sample ofn independently drawn instances. The approachdescribed there illustrates a general approach followed in many estima6on prob- 
lems. In particular, we can see this asa problem of estimating the mean (expectedvalue) ofa population based on the mean ofa randomly drawn sample of size n. 
The general process includes the following steps: 
1. Identify the underlying population parameter pto be estimated, for example, 
errorv(h). 
2. Define the estimator Y (e.g., errors(h)). Itis desirable to choose a minimum- 
variance, unbiased estimator. 
3. Determine the probability distribution Vy that governs the estimator Y, in- 
cluding its mean and variance. 
4. Determine the N% confidence interval by finding thresholds L and U suchthat N% of the mass in the probability distribution Vy falls between L and U. 
In later sections of this chapter we apply this general approach to sev- 
eral other estimation problems common in machine learning. First, however, letus discuss a fundamental result from estimation theory called the Central LimitTheorem. 
5.4.1 Central Limit TheoremOne essential fact that simplifies attempts to derive confidence intervals is theCentral Limit Theorem. Consider again our general setting, in which we observethe values ofn independently drawn random variables Yl . . . Yn that obey the sameunknown underlying probability distribution (e.g., n tosses of the same coin). Letp denote the mean of the unknown distribution governing each of the Yi and leta denote the standard deviation. We say that these variables Yi are independent, 
identically distributed random variables, because they describe independent exper- 
iments, each obeying the same underlying probability distribution. Inan attemptto estimate the mean pof the distribution governing the Yi, we calculate the sam- 
ple mean = '& Yi (e.g., the fraction of heads among the n coin tosses). 
The Central Limit Theorem states that the probability distribution governing Fnapproaches a Normal distribution asn + co, regardless of the distribution thatgoverns the underlying random variables Yi. Furthermore, the mean of the dis- 
tribution governing Yn approaches p and the standard deviation approaches k. 
More precisely, 
Theorem 5.1. Central Limit Theorem. Consider a set of independent, identicallydistributed random variables Yl . . . Y, governed byan arbitrary probability distribu- 
tion with mean p and finite variance a2. Define the sample mean, = xy=, Yi. 
Then asn + co, the distribution governing5 
approaches a Normal distribution, with zero mean and standard deviation equal to 1. 
This isa quite surprising fact, because it states that we know the form ofthe distribution that governs the sample mean ? even when wedo not know theform of the underlying distribution that governs the individual Yi that are beingobserved! Furthermore, the Central Limit Theorem describes how the mean andvariance ofY can be used to determine the mean and variance of the individual Yi. 
The Central Limit Theorem isa very useful fact, because it implies thatwhenever we define an estimator that is the mean of some sample (e.g., errors(h) 
is the mean error), the distribution governing this estimator can be approximatedby a Normal distribution for sufficiently large n. Ifwe also know the variancefor this (approximately) Normal distribution, then we can use Equation (5.1 1) tocompute confidence intervals. A common rule of thumb is that we can use theNormal approximation when n 2 30. Recall that in the preceding section we usedsuch a Normal distribution to approximate the Binomial distribution that moreprecisely describes errors (h) . 
5.5 DIFFERENCE IN ERROR OF TWO HYPOTHESESConsider the case where we have two hypotheses hl and h2 for some discrete- 
valued target function. Hypothesis hl has been tested ona samj4e S1 containingnl randomly drawn examples, and ha has been tested onan indi:pendent sampleS2 containing n2 examples drawn from the same distribution. Suppose we wishto estimate the difference d between the true errors of these two hypotheses. 
We will use the generic four-step procedure described at the beginning ofSection 5.4 to derive a confidence interval estimate for d. Having identified d asthe parameter tobe estimated, we next define an estimator. The obvious choicefor an estimator in this case is the difference between the sample errors, whichwe denote by 2 
,. d = errors, (hl) - errors, (h2) 
Although we will not prove it here, it can be shown that 2 gives an unbiasedestimate ofd; that isE[C? ] = d. 
What is the probability distribution governing the random variable 2? Fromearlier sections, we know that for large nl and n2 (e.g., both 2 30), both errors, (hl) 
and error& (hz) follow distributions that are approximately Normal. Because thedifference of two Normal distributions is also a Normal distribution, 2 will also144 MACHINE LEARNINGr 
follow a distribution that is approximately Normal, with mean d. It can alsobe shown that the variance of this distribution is the sum of the variances oferrors, (hl) and errors2(h2). Using Equation (5.9) to obtain the approximate vari- 
ance of each of these distributions, we haveerrorS, (hl)(l - errors, (hl)) errors2 (h2)(1 - errors,(h2)) 02 , ci + (5.12) 
n 1 n2Now that we have determined the probability distribution that governs the esti- 
mator 2, itis straightforward to derive confidence intervals that characterize thelikely error in employing 2 to estimate d. For a random variable 2 obeying aNormal distribution with mean d and variance a2, the N% confidence intervalestimate for dis 2 fz~a. Using the approximate variance a; given above, thisapproximate N% confidence interval estimate for d isJ errors, (hl)(l - errors, (h 1)) errors2 (h2)(1 - errors2(h2)) 
dfz~ + (5.13) 
nl n2where zNis the same constant described in Table 5.1. The above expression givesthe general two-sided confidence interval for estimating the difference betweenerrors of two hypotheses. In some situations we might be interested in one-sidedbounds--either bounding the largest possible difference in errors or the smallest, 
with some confidence level. One-sided confidence intervals can be obtained bymodifying the above expression as described in Section 5.3.6. 
Although the above analysis considers the case in which hl and h2 are testedon independent data samples, itis often acceptable to use the confidence intervalseen in Equation (5.13) in the setting where h 1 and h2 are tested ona single sampleS (where Sis still independent ofhl and h2). In this later case, we redefine 2 asThe variance in this new 2 will usually be smaller than the variance given byEquation (5.12), when we set S1 and S2 toS. This is because using a singlesample S eliminates the variance due to random differences in the compositionsof S1 and S2. In this case, the confidence interval given by Equation (5.13) willgenerally bean overly conservative, but still correct, interval. 
5.5.1 Hypothesis TestingIn some cases we are interested in the probability that some specific conjecture istrue, rather than in confidence intervals for some parameter. Suppose, for example, 
that we are interested in the question "what is the probability that errorv(h1) > 
errorv(h2)?' Following the setting in the previous section, suppose we measurethe sample errors for hl and h2 using two independent samples S1 and S2 of size100 and find that errors, (hl) = .30 and errors2(h2) = -20, hence the observeddifference is 2 = .lo. Of course, due to random variation in the data sample, 
we might observe this difference in the sample errors even when errorv(hl) 5errorv(h2). What is the probability that errorv(hl) > errorv(h2), given theobserved difference in sample errors 2 = .10 in this case? Equivalently, what isthe probability that d > 0, given that we observed 2 = .lo? 
Note the probability Pr(d > 0) is equal to the probability that 2 has notoverestimated dby more than .lo. Put another way, this is the probability that 2falls into the one-sided interval 2 < d + .lo. Since dis the mean of the distributiongoverning 2, we can equivalently express this one-sided interval as 2 < p2 + .lo. 
To summarize, the probability Pr(d > 0) equals the probability that 2 fallsinto the one-sided interval 2 < pa + .lo. Since we already calculated the ap- 
proximate distribution governing 2 in the previous section, we can determine theprobability that 2 falls into this one-sided interval by calculating the probabilitymass of the 2 distribution within this interval. 
Let us begin this calculation byre-expressing the interval 2 < pi + .10 interms of the number of standard deviations it allows deviating from the mean. 
Using Equation (5.12) we find that 02 FZ .061, sowe can re-express the intervalas approximatelyWhat is the confidence level associated with this one-sided interval for a Normaldistribution? Consulting Table 5.1, we find that 1.64 standard deviations about themean corresponds toa two-sided interval with confidence level 90%. Therefore, 
the one-sided interval will have an associated confidence level of 95%. 
Therefore, given the observed 2 = .lo, the probability that errorv(h1) > 
errorv(h2) is approximately .95. In the terminology of the statistics literature, wesay that we accept the hypothesis that "errorv(hl) > errorv(h2)" with confidence0.95. Alternatively, we may state that we reject the opposite hypothesis (oftencalled the null hypothesis) ata (1 - 0.95) = .05 level of significance. 
5.6 COMPARING LEARNING ALGORITHMSOften we are interested in comparing the performance of two learning algorithmsLA and LB, rather than two specific hypotheses. What isan appropriate test forcomparing learning algorithms, and how can we determine whether an observeddifference between the algorithms is statistically significant? Although there isactive debate within the machine-learning research community regarding the bestmethod for comparison, we present here one reasonable approach. A discussionof alternative methods is given by Dietterich (1996). 
As usual, we begin by specifying the parameter we wish to estimate. Supposewe wish to determine which ofLA and LBis the better learning method on averagefor learning some particular target function f. A reasonable way to define "onaverage" isto consider the relative performance of these two algorithms averagedover all the training sets of size n that might be drawn from the underlyinginstance distribution V. In other words, we wish to estimate the expected valueof the difference in their errorswhere L(S) denotes the hypothesis output by learning method L when giventhe sample Sof training data and where the subscript Sc V indicates thatthe expected value is taken over samples S drawn according to the underlyinginstance distribution V. The above expression describes the expected value of thedifference in errors between learning methods LA and LB. 
Of course in practice we have only a limited sample Doof data whencomparing learning methods. In such cases, one obvious approach to estimatingthe above quantity isto divide Do into a training set So and a disjoint test set To. 
The training data can be used to train both LA and LB, and the test data can beused to compare the accuracy of the two learned hypotheses. In other words, wemeasure the quantityNotice two key differences between this estimator and the quantity in Equa- 
tion (5.14). First, we are using errorTo(h) to approximate errorv(h). Second, weare only measuring the difference in errors for one training set So rather than tak- 
ing the expected value of this difference over all samples S that might be drawnfrom the distribution 2). 
One way to improve on the estimator given by Equation (5.15) isto repeat- 
edly partition the data Do into disjoint training and test sets and to take the meanof the test set errors for these different experiments. This leads to the procedureshown in Table 5.5 for estimating the difference between errors of two learningmethods, based ona fixed sample Doof available data. This procedure first parti- 
tions the data into k disjoint subsets of equal size, where this size isat least 30. Itthen trains and tests the learning algorithms k times, using each of the k subsetsin turn as the test set, and using all remaining data as the training set. In thisway, the learning algorithms are tested onk independent test sets, 'and the meandifference in errors 8 is returned asan estimate of the difference between the twolearning algorithms. 
The quantity 8 returned by the procedure of Table 5.5 can be taken as anestimate of the desired quantity from Equation 5.14. More appropriately, we canview 8 asan estimate of the quantitywhere S represents a random sample of size ID01 drawn uniformly from Do. 
The only difference between this expression and our original expression in Equa- 
tion (5.14) is that this new expression takes the expected value over subsets ofthe available data Do, rather than over subsets drawn from the full instance dis- 
tribution 2). 
1. Partition the available data Do into k disjoint subsets TI, T2, . . . , Tkof equal size, where this sizeis at least 30. 
2. For i from 1 tok, douse Ti for the test set, and the remaining data for training set Si0 Sic {Do - Ti} 
hAC LA(Si) 
h~ + L~(si) 
0 Sit errorq (hA) - errorz (hB) 
3. Return the value 6, whereTABLE 5.5A procedure to estimate the difference in error between two learning methods LA and LB. Approxi- 
mate confidence intervals for this estimate are given in the text. 
The approximate N% confidence interval for estimating the quantity in Equa- 
tion (5.16) using 8 is given bywhere tN,k-lis a constant that plays a role analogous to that ofZN in our ear- 
lier confidence interval expressions, and where s,- isan estimate of the standarddeviation of the distribution governing 8. In particular, sgis defined asNotice the constant t~,k-lin Equation (5.17) has two subscripts. The firstspecifies the desired confidence level, asit did for our earlier constant ZN. Thesecond parameter, called the number of degrees of freedom and usually denoted byv, is related to the number of independent random events that go into producingthe value for the random variable 8. In the current setting, the number of degreesof freedom isk - 1. Selected values for the parameter t are given in Table 5.6. 
Notice that ask + oo, the value oft~,k-l approaches the constant ZN. 
Note the procedure described here for comparing two learning methods in- 
volves testing the two learned hypotheses on identical test sets. This contrasts withthe method described in Section 5.5 for comparing hypotheses that have been eval- 
uated using two independent test sets. Tests where the hypotheses are evaluatedover identical samples are called paired tests. Paired tests typically produce tighterconfidence intervals because any differences in observed errors ina paired testare due to differences between the hypotheses. In contrast, when the hypothesesare tested on separate data samples, differences in the two sample errors might bepartially attributable to differences in the makeup of the two samples. 
Confidence level N90% 95% 98% 99% 
TABLE 5.6Values oft^," for two-sided confidence intervals. Asv + w, t~," approaches ZN. 
5.6.1 Paired t TestsAbove we described one procedure for comparing two learning methods given afixed set of data. This section discusses the statistical justification for this proce- 
dure, and for the confidence interval defined by Equations (5.17) and (5.18). Itcan be skipped or skimmed ona first reading without loss of continuity. 
The best way to understand the justification for the confidence interval es- 
timate given by Equation (5.17) isto consider the following estimation problem: 
00 
aThisWe are given the observed values ofa set of independent, identically dis- 
tributed random variables YI, Y2, . . . , Yk. 
We wish to estimate the mean pof the probability distribution governingthese Yi. 
The estimator we will use is the sample mean Yproblem of estimating the distribution mean p based on the sample meanY is quite general. For example, it covers the problem discussed earlier of usingerrors(h) to estimate errorv(h). (In that problem, the Yi are 1 or 0 to indicatewhether h commits an error onan individual example from S, and errorv(h) is themean pof the underlying distribution.) The t test, described by Equations (5.17) 
and (5.18), applies toa special case of this problem-the case in which theindividual Yi follow a Normal distribution. 
Now consider the following idealization of the method in Table 5.5 for com- 
paring learning methods. Assume that instead of having a fixed sample of data Do, 
we can request new training examples drawn according to the underlying instancedistribution. In particular, in this idealized method we modify the procedure ofTable 5.5 so that on each iteration through the loop it generates a new randomtraining set Si and new random test set Tiby drawing from this underlying instancedistribution instead of drawing from the fixed sample Do. This idealized methodperfectly fits the form of the above estimation problem. In particular, the Si mea- 
sured by the procedure now correspond to the independent, identically distributedrandom variables Yi. The mean pof their distribution corresponds to the expecteddifference in error between the two learning methods [i.e., Equation (5.14)]. Thesample mean Yis the quantity 6 computed by this idealized version of the method. 
We wish to answer the question "how good an estimate ofp is provided bys?' 
First, note that the size of the test sets has been chosen to contain at least30 examples. Because of this, the individual Si will each follow an approximatelyNormal distribution (due to the Central Limit Theorem). Hence, we have a specialcase in which the Yi are governed byan approximately Normal distribution. Itcan be shown in general that when the individual Yi each follow a Normal dis- 
tribution, then the sample mean Y follows a Normal distribution as well. Giventhat Yis Normally distributed, we might consider using the earlier expression forconfidence intervals (Equation [5.11]) that applies to estimators governed by Nor- 
mal distributions. Unfortunately, that equation requires that we know the standarddeviation of this distribution, which wedo not. 
The t test applies to precisely these situations, in which the task isto esti- 
mate the sample mean ofa collection of independent, identically and Normallydistributed random variables. In this case, we can use the confidence interval givenby Equations (5.17) and (5.18), which can be restated using our current notation aswhere spis the estimated standard deviation of the sample meanand where tN,k-lis a constant analogous to our earlier ZN. In fact, the constantt~,k-l characterizes the area under a probability distribution known as the t distri- 
bution, just as the constant ZN characterizes the area under a Normal distribution. 
The t distribution isa bell-shaped distribution similar to the Normal distribu- 
tion, but wider and shorter to reflect the greater variance introduced by using spto approximate the true standard deviation ap. The t distribution approaches theNormal distribution (and therefore tN,k-l approaches zN) ask approaches infinity. 
This is intuitively satisfying because we expect spto converge toward the truestandard deviation apas the sample size k grows, and because we can use ZNwhen the standard deviation is known exactly. 
5.6.2 Practical ConsiderationsNote the above discussion justifies the use of the confidence interval estimategiven by Equation (5.17) in the case where we wish to use the sample meanY to estimate the mean ofa sample containing k independent, identically andNormally distributed random variables. This fits the idealized method describedabove, in which we assume unlimited access to examples of the target function. Inpractice, given a limited set of data Do and the more practical method describedby Table 5.5, this justification does not strictly apply. In practice, the problem isthat the only way to generate new Siis to resample Do, dividing it into trainingand test sets in different ways. The 6i are not independent of one another in thiscase, because they are based on overlapping sets of training examples drawn fromthe limited subset Doof data, rather than from the full distribution 'D. 
When only a limited sample of data Dois available, several methods can beused to resample Do. Table 5.5 describes ak-fold method in which Dois parti- 
tioned into k disjoint, equal-sized subsets. In this k-fold approach, each examplefrom Dois used exactly once ina test set, and k - 1 times ina training set. Asecond popular approach isto randomly choose a test set ofat least 30 examplesfrom Do, use the remaining examples for training, then repeat this process asmany times as desired. This randomized method has the advantage that it can berepeated an indefinite number of times, to shrink the confidence interval to thedesired width. In contrast, the k-fold method is limited by the total number ofexamples, by the use of each example only once ina test set, and by our desireto use samples of size at least 30. However, the randomized method has the dis- 
advantage that the test sets no longer qualify as being independently drawn withrespect to the underlying instance distribution D. In contrast, the test sets gener- 
ated byk-fold cross validation are independent because each instance is includedin only one test set. 
To summarize, no single procedure for comparing learning methods basedon limited data satisfies all the constraints we would like. Itis wise to keep inmind that statistical models rarely fit perfectly the practical constraints in testinglearning algorithms when available data is limited. Nevertheless, they do pro- 
vide approximate confidence intervals that can beof great help in interpretingexperimental comparisons of learning methods. 
5.7 SUMMARY AND FURTHER READINGThe main points of this chapter include: 
0 Statistical theory provides a basis for estimating the true error (errorv(h)) 
ofa hypothesis h, based on its observed error (errors(h)) over a sample S ofdata. For example, ifh isa discrete-valued hypothesis and the data sampleS contains n 2 30 examples drawn independently ofh and of one another, 
then the N% confidence interval for errorv(h) is approximatelywhere values for zN are given in Table 5.1. 
0 In general, the problem of estimating confidence intervals is approached byidentifying the parameter tobe estimated (e.g., errorD(h)) and an estimatorCHAFER 5 EVALUATING HYPOTHESES 151 
(e.g., errors(h)) for this quantity. Because the estimator isa random variable 
(e.g., errors(h) depends on the random sample S), it can be characterizedby the probability distribution that governs its value. Confidence intervalscan then be calculated by determining the interval that contains the desiredprobability mass under this distribution. 
0 One possible cause of errors in estimating hypothesis accuracy is estimationbias. IfY isan estimator for some parameter p, the estimation bias of Yis the difference between p and the expected value ofY. For example, if Sis the training data used to formulate hypothesis h, then errors(h) gives anoptimistically biased estimate of the true error errorD(h). 
0 A second cause of estimation error is variance in the estimate. Even with anunbiased estimator, the observed value of the estimator is likely to vary fromone experiment to another. The variance a2 of the distribution governing theestimator characterizes how widely this estimate is likely to vary from thecorrect value. This variance decreases as the size of the data sample isincreased. 
0 Comparing the relative effectiveness of two learning algorithms isan esti- 
mation problem that is relatively easy when data and time are unlimited, butmore difficult when these resources are limited. One possible approach de- 
scribed in this chapter isto run the learning algorithms on different subsetsof the available data, testing the learned hypotheses on the remaining data, 
then averaging the results of these experiments. 
0 In most cases considered here, deriving confidence intervals involves makinga number of assumptions and approximations. For example, the above confi- 
dence interval for errorv(h) involved approximating a Binomial distributionby a Normal distribution, approximating the variance of this distribution, andassuming instances are generated bya fixed, unchanging probability distri- 
bution. While intervals based on such approximations are only approximateconfidence intervals, they nevertheless provide useful guidance for designingand interpreting experimental results in machine learning. 
The key statistical definitions presented in this chapter are summarized inTable 5.2. 
An ocean of literature exists on the topic of statistical methods for estimatingmeans and testing significance of hypotheses. While this chapter introduces thebasic concepts, more detailed treatments of these issues can be found in manybooks and articles. Billingsley etal. (1986) provide a very readable introductionto statistics that elaborates on the issues discussed here. Other texts on statisticsinclude DeGroot (1986); Casella and Berger (1990). Duda and Hart (1973) providea treatment of these issues in the context of numerical pattern recognition. 
Segre etal. (1991, 1996), Etzioni and Etzioni (1994), and Gordon andSegre (1996) discuss statistical significance tests for evaluating learning algo- 
rithms whose performance is measured by their ability to improve computationalefficiency. 
Geman etal. (1992) discuss the tradeoff involved in attempting to minimizebias and variance simultaneously. There is ongoing debate regarding the best wayto learn and compare hypotheses from limited data. For example, Dietterich (1996) 
discusses the risks of applying the paired-difference t test repeatedly to differenttrain-test splits of the data. 
EXERCISES5.1. Suppose you test a hypothesis h and find that it commits r = 300 errors ona sampleS ofn = 1000 randomly drawn test examples. What is the standard deviation inerrors(h)? How does this compare to the standard deviation in the example at theend of Section 5.3.4? 
5.2. Consider a learned hypothesis, h, for some boolean concept. When his tested on aset of 100 examples, it classifies 83 correctly. What is the standard deviation andthe 95% confidence interval for the true error rate for Errorv(h)? 
5.3. Suppose hypothesis h commits r = 10 errors over a sample ofn = 65 independentlydrawn examples. What is the 90% confidence interval (two-sided) for the true errorrate? What is the 95% one-sided interval (i.e., what is the upper bound U such thaterrorv(h) 5 U with 95% confidence)? What is the 90% one-sided interval? 
5.4. You are about to test a hypothesis h whose errorV(h) is known tobe in the rangebetween 0.2 and 0.6. What is the minimum number of examples you must collectto assure that the width of the two-sided 95% confidence interval will be smallerthan 0.1? 
5.5. Give general expressions for the upper and lower one-sided N% confidence intervalsfor the difference in errors between two hypotheses tested on different samples ofdata. Hint: Modify the expression given in Section 5.5. 
5.6. Explain why the confidence interval estimate given in Equation (5.17) applies toestimating the quantity in Equation (5.16), and not the quantity in Equation (5.14). 
REFERENCESBillingsley, P., Croft, D. J., Huntsberger, D. V., & Watson, C. J. (1986). Statistical inference formanagement and economics. Boston: Allyn and Bacon, Inc. 
Casella, G., & Berger, R. L. (1990). Statistical inference. Pacific Grove, CA: Wadsworth andBrooksICole. 
DeGroot, M. H. (1986). Probability and statistics. (2d ed.) Reading, MA: Addison Wesley. 
Dietterich, T. G. (1996). Proper statistical tests for comparing supervised classiJication learningalgorithms (Technical Report). Department of Computer Science, Oregon State University, 
Cowallis, OR. 
Dietterich, T. G., & Kong, E. B. (1995). Machine learning bias, statistical bias, and statisticalvariance of decision tree algorithms (Technical Report). Department of Computer Science, 
Oregon State University, Cowallis, OR. 
Duda, R., & Hart, P. (1973). Pattern classiJication and scene analysis. New York: John Wiley & 
Sons. 
Efron, B., & Tibshirani, R. (1991). Statistical data analysis in the computer age. Science, 253, 390- 
395. 
Etzioni, O., & Etzioni, R. (1994). Statistical methods for analyzing speedup learning experiments. 
Machine Learning, 14, 333-347. 
Geman, S., Bienenstock, E., & Doursat, R. (1992). Neural networks and the biadvariance dilemma. 
Neural Computation, 4, 1-58. 
Gordon, G., & Segre, A.M. (1996). Nonpararnetric statistical methods for experimental evaluations ofspeedup learning. Proceedings of the Thirteenth International Conference on Machine Leam- 
ing, Bari, Italy. 
Maisel, L. (1971). Probability, statistics, and random processes. Simon and Schuster Tech Outlines. 
New York: Simon and Schuster. 
Segre, A., Elkan, C., & Russell, A. (1991). A critical look at experimental evaluations of EBL. 
Machine Learning, 6(2). 
Segre, A.M, Gordon G., & Elkan, C. P. (1996). Exploratory analysis of speedup learning data usingexpectation maximization. Artificial Intelligence, 85, 301-3 19. 
Speigel, M. R. (1991). Theory and problems of probability and statistics. Schaum's Outline Series. 
New York: McGraw Hill. 
Thompson, M.L., & Zucchini, W. (1989). On the statistical analysis of ROC curves. Statistics inMedicine, 8, 1277-1290. 
White, A. P., & Liu, W. Z. (1994). Bias in information-based measures in decision tree induction. 
Machine Learning, 15, 321-329. 
CHAPTERBAYESIANLEARNINGBayesian reasoning provides a probabilistic approach to inference. Itis based onthe assumption that the quantities of interest are governed by probability distri- 
butions and that optimal decisions can be made by reasoning about these proba- 
bilities together with observed data. Itis important to machine learning becauseit provides a quantitative approach to weighing the evidence supporting alterna- 
tive hypotheses. Bayesian reasoning provides the basis for learning algorithmsthat directly manipulate probabilities, as well asa framework for analyzing theoperation of other algorithms that do not explicitly manipulate probabilities. 
6.1 INTRODUCTIONBayesian learning methods are relevant to our study of machine learning fortwo different reasons. First, Bayesian learning algorithms that calculate explicitprobabilities for hypotheses, such as the naive Bayes classifier, are among the mostpractical approaches to certain types of learning problems. For example, Michieet al. (1994) provide a detailed study comparing the naive Bayes classifier toother learning algorithms, including decision tree and neural network algorithms. 
These researchers show that the naive Bayes classifier is competitive with theseother learning algorithms in many cases and that in some cases it outperformsthese other methods. In this chapter we describe the naive Bayes classifier andprovide a detailed example of its use. In particular, we discuss its application tothe problem of learning to classify text documents such as electronic news articles. 
CHAFER 6 BAYESIAN LEARNING 155For such learning tasks, the naive Bayes classifier is among the most effectivealgorithms known. 
The second reason that Bayesian methods are important to our study ofma- 
chine learning is that they provide a useful perspective for understanding manylearning algorithms that do not explicitly manipulate probabilities. For exam- 
ple, in this chapter we analyze algorithms such as the FIND-S and CANDIDATE- 
ELIMINATION algorithms of Chapter 2 to determine conditions under which theyoutput the most probable hypothesis given the training data. We also use aBayesian analysis to justify a key design choice in neural network learning al- 
gorithms: choosing to minimize the sum of squared errors when searching thespace of possible neural networks. We also derive an alternative error function, 
cross entropy, that is more appropriate than sum of squared errors when learn- 
ing target functions that predict probabilities. We use a Bayesian perspective toanalyze the inductive bias of decision tree learning algorithms that favor shortdecision trees and examine the closely related Minimum Description Length prin- 
ciple. A basic familiarity with Bayesian methods is important to understandingU and characterizing the operation of many algorithms in machine learning. 
Features of Bayesian learning methods include: 
0 Each observed training example can incrementally decrease or increase theestimated probability that a hypothesis is correct. This provides a moreflexible approach to learning than algorithms that completely eliminate ahypothesis ifit is found tobe inconsistent with any single example. 
0 Prior knowledge can be combined with observed data to determine the finalprobability ~fa hypothesis. In Bayesian learning, prior knowledge is pro- 
vided by asserting (1) a prior probability for each candidate hypothesis, and 
(2) a probability distribution over observed data for each possible hypothesis. 
Bayesian methods can accommodate hypotheses that make probabilistic pre- 
dictions (e.g., hypotheses such as "this pneumonia patient has a 93% chanceof complete recovery"). 
0 New instances can be classified by combining the predictions of multiplehypotheses, weighted by their probabilities. 
0 Even in cases where Bayesian methods prove computationally intractable, 
they can provide a standard of optimal decision making against which otherpractical methods can be measured. 
One practical difficulty in applying Bayesian methods is that they typicallyrequire initial knowledge of many probabilities. When these probabilities are notknown in advance they are often estimated based on background knowledge, pre- 
viously available data, and assumptions about the form of the underlying distribu- 
tions. A second practical difficulty is the significant computational cost required todetermine the Bayes optimal hypothesis in the general case (linear in the numberof candidate hypotheses). In certain specialized situations, this computational costcan be significantly reduced. 
The remainder of this chapter is organized as follows. Section 6.2 intro- 
duces Bayes theorem and defines maximum likelihood and maximum a posterioriprobability hypotheses. The four subsequent sections then apply this probabilisticframework to analyze several issues and learning algorithms discussed in earlierchapters. For example, we show that several previously described algorithms out- 
put maximum likelihood hypotheses, under certain assumptions. The remainingsections then introduce a number of learning algorithms that explicitly manip- 
ulate probabilities. These include the Bayes optimal classifier, Gibbs algorithm, 
and naive Bayes classifier. Finally, we discuss Bayesian belief networks, a rela- 
tively recent approach to learning based on probabilistic reasoning, and the EMalgorithm, a widely used algorithm for learning in the presence of unobservedvariables. 
6.2 BAYES THEOREMIn machine learning we are often interested in determining the best hypothesisfrom some space H, given the observed training data D. One way to specifywhat we mean by the best hypothesis isto say that we demand the most probablehypothesis, given the data D plus any initial knowledge about the prior probabil- 
ities of the various hypotheses inH. Bayes theorem provides a direct method forcalculating such probabilities. More precisely, Bayes theorem provides a way tocalculate the probability ofa hypothesis based on its prior probability, the proba- 
bilities of observing various data given the hypothesis, and the observed data itself. 
To define Bayes theorem precisely, let us first introduce a little notation. Weshall write P(h) to denote the initial probability that hypothesis h holds, before wehave observed the training data. P(h) is often called the priorprobability ofh andmay reflect any background knowledge we have about the chance that his a correcthypothesis. Ifwe have no such prior knowledge, then we might simply assignthe same prior probability to each candidate hypothesis. Similarly, we will writeP(D) to denote the prior probability that training data D will be observed (i.e., 
the probability ofD given no knowledge about which hypothesis holds). Next, 
we will write P(D1h) to denote the probability of observing data D given someworld in which hypothesis h holds. More generally, we write P(xly) to denotethe probability ofx given y. In machine learning problems we are interested inthe probability P (h 1 D) that h holds given the observed training data D. P (h 1 D) iscalled the posteriorprobability ofh, because it reflects our confidence that h holdsafter we have seen the training data D. Notice the posterior probability P(h1D) 
reflects the influence of the training data D, in contrast to the prior probabilityP(h) , which is independent ofD. 
Bayes theorem is the cornerstone of Bayesian learning methods becauseit provides a way to calculate the posterior probability P(hlD), from the priorprobability P(h), together with P(D) and P(D(h). 
Bayes theorem: 
CHAPTER 6 BAYESIAN LEARNING 157As one might intuitively expect, P(hID) increases with P(h) and with P(D1h) 
according to Bayes theorem. Itis also reasonable to see that P(hlD) decreases asP(D) increases, because the more probable itis that D will be observed indepen- 
dent ofh, the less evidence D provides in support ofh. 
In many learning scenarios, the learner considers some set of candidatehypotheses H and is interested in finding the most probable hypothesis hE Hgiven the observed data D (orat least one of the maximally probable if thereare several). Any such maximally probable hypothesis is called a maximum aposteriori (MAP) hypothesis. We can determine the MAP hypotheses by usingBayes theorem to calculate the posterior probability of each candidate hypothesis. 
More precisely, we will say that MAP isa MAP hypothesis providedh~~p = argmax P(hlD) 
h€ H 
= argmax P(D 1 h) P (h) 
h€ H 
(6.2) 
Notice in the final step above we dropped the term P(D) because itis a constantindependent ofh. 
In some cases, we will assume that every hypothesis inH is equally probablea priori (P(hi) = P(h;) for all hi and h; inH). In this case we can furthersimplify Equation (6.2) and need only consider the term P(D1h) to find the mostprobable hypothesis. P(Dlh) is often called the likelihood of the data D given h, 
and any hypothesis that maximizes P(Dlh) is called a maximum likelihood (ML) 
hypothesis, hML. 
hML = argmax P(Dlh) 
h€ HIn order to make clear the connection to machine learning problems, weintroduced Bayes theorem above by referring to the data Das training examples ofsome target function and referring toH as the space of candidate target functions. 
In fact, Bayes theorem is much more general than suggested by this discussion. Itcan be applied equally well to any set Hof mutually exclusive propositions whoseprobabilities sum to one (e.g., "the sky is blue," and "the sky is not blue"). In thischapter, we will at times consider cases where His a hypothesis space containingpossible target functions and the data D are training examples. At other times wewill consider cases where His some other set of mutually exclusive propositions, 
and Dis some other kind of data. 
6.2.1 An ExampleTo illustrate Bayes rule, consider a medical diagnosis problem in which there aretwo alternative hypotheses: (1) that the patien; has a- articular form of cancer. 
and (2) that the patient does not. The avaiiable data is from a particular laboratorytest with two possible outcomes: $ (positive) and 8 (negative). We have priorknowledge that over the entire population of people only .008 have this disease. 
Furthermore, the lab test is only an imperfect indicator of the disease. The testreturns a correct positive result in only 98% of the cases in which the disease isactually present and a correct negative result in only 97% of the cases in whichthe disease is not present. In other cases, the test returns the opposite result. Theabove situation can be summarized by the following probabilities: 
Suppose we now observe a new patient for whom the lab test returns a positiveresult. Should we diagnose the patient as having cancer or not? The maximum aposteriori hypothesis can be found using Equation (6.2): 
Thus, h~~p = -cancer. The exact posterior hobabilities can also be determinedby normalizing the above quantities so that they sum to 1 (e.g., P(cancer($) = 
.00;~~298 = .21). This step is warranted because Bayes theorem states that theposterior probabilities are just the above quantities divided by the probability ofthe data, P(@). Although P($) was not provided directly as part of the problemstatement, we can calculate itin this fashion because we know that P(cancerl$) 
and P(-cancerl$) must sum to 1 (i.e., either the patient has cancer or they donot). Notice that while the posterior probability of cancer is significantly higherthan its prior probability, the most probable hypothesis is still that the patient doesnot have cancer. 
As this example illustrates, the result of Bayesian inference depends stronglyon the prior probabilities, which must be available in order to apply the methoddirectly. Note also that in this example the hypotheses are not completely acceptedor rejected, but rather become more or less probable as more data is observed. 
Basic formulas for calculating probabilities are summarized in Table 6.1. 
6.3 BAYES THEOREM AND CONCEPT LEARNINGWhat is the relationship between Bayes theorem and the problem of concept learn- 
ing? Since Bayes theorem provides a principled way to calculate the posteriorprobability of each hypothesis given the training data, we can use itas the basisfor a straightforward learning algorithm that calculates the probability for eachpossible hypothesis, then outputs the most probable. This section considers sucha brute-force Bayesian concept learning algorithm, then compares itto conceptlearning algorithms we considered in Chapter 2. Aswe shall see, one interestingresult of this comparison is that under certain conditions several algorithms dis- 
cussed in earlier chapters output the same hypotheses as this brute-force BayesianCHAPTER 6 BAYESIAN LEARNING 159 
- 
. Product rule: probability P(AA B) ofa conjunction of two events A and BSum rule: probability ofa disjunction of two events A and BBayes theorem: the posterior probability P(hlD) ofh given D 
. Theorem of totalprobability: if events A1,. . . , A, are mutually exclusive with xy=lP(Ai) = 1, 
thenTABLE 6.1Summary of basic probability formulas. 
11t algorithm, despite the fact that they do not explicitly manipulate probabilities andare considerably more efficient. 
6.3.1 Brute-Force Bayes Concept LearningConsider the concept learning problem first introduced in Chapter 2. In particular, 
assume the learner considers some finite hypothesis space H defined over theinstance space X, in which the task isto learn some target concept c : X + {0,1}. 
As usual, we assume that the learner is given some sequence of training examples 
((x~, dl) . . . (xm, dm)) where xiis some instance from X and where diis the targetvalue ofxi (i.e., di = c(xi)). To simplify the discussion in this section, we assumethe sequence of instances (xl . . . xm) is held fixed, so that the training data D canbe written simply as the sequence of target values D = (dl . . . dm). It can be shown 
(see Exercise 6.4) that this simplification does not alter the main conclusions ofthis section. 
We can design a straightforward concept learning algorithm to output themaximum a posteriori hypothesis, based on Bayes theorem, as follows: 
BRUTE-FORCE MAP LEARNING algorithm1. For each hypothesis hin H, calculate the posterior probability2. Output the hypothesis hMAP with the highest posterior probability160 MACHINE LEARNINGThis algorithm may require significant computation, because it applies Bayes theo- 
rem to each hypothesis inH to calculate P(hJD). While this may prove impracticalfor large hypothesis spaces, the algorithm is still of interest because it provides astandard against which we may judge the performance of other concept learningalgorithms. 
In order specify a Iearning problem for the BRUTE-FORCE MAP LEARNINGalgorithm we must specify what values are tobe used for P(h) and for P(D1h) 
(aswe shall see, P(D) will be determined once we choose the other two). Wemay choose the probability distributions P(h) and P(D1h) in any way we wish, 
to describe our prior knowledge about the learning task. Here let us choose themto be consistent with the following assumptions: 
1. The training data Dis noise free (i.e., di = c(xi)). 
2. The target concept cis contained in the hypothesis space H3. We have noa priori reason to believe that any hypothesis is more probablethan any other. 
Given these assumptions, what values should we specify for P(h)? Given noprior knowledge that one hypothesis is more likely than another, itis reasonable toassign the same prior probability to every hypothesis hin H. Furthermore, becausewe assume the target concept is contained inH we should require that these priorprobabilities sum to 1. Together these constraints imply that we should choose1 
P(h) = - for all hin HIHIWhat choice shall we make for P(Dlh)? P(D1h) is the probability ofob- 
serving the target values D = (dl . . .dm) for the fixed set of instances (XI . . . x,), 
given a world in which hypothesis h holds (i.e., given a world in which his thecorrect description of the target concept c). Since we assume noise-free trainingdata, the probability of observing classification di given his just 1 ifdi = h(xi) 
and 0 ifdi # h(xi). Therefore, 
1 ifdi = h(xi) for all diin DP(D1h) = (6.4) 
0 otherwiseIn other words, the probability of data D given hypothesis his 1 ifD is consistentwith h, and 0 otherwise. 
Given these choices for P(h) and for P(Dlh) we now have a fully-definedproblem for the above BRUTE-FORCE MAP LEARNING algorithm. Let us consider thefirst step of this algorithm, which uses Bayes theorem to compute the posteriorprobability P(h1D) of each hypothesis h given the observed training data D. 
CHAPTER 6 BAYESIAN LEARNING 161Recalling Bayes theorem, we haveFirst consider the case where his inconsistent with the training data D. SinceEquation (6.4) defines P(D)h) tobe 0 when his inconsistent with D, we haveP(~(D) = - ' P(h) - - oif his inconsistent with DP(D) 
The posterior probability ofa hypothesis inconsistent with Dis zero. 
Now consider the case where his consistent with D. Since Equation (6.4) 
defines P(Dlh) tobe 1 when his consistent with D, we have 
- 1 -- ifh is consistent with DIVSH,DIwhere VSH,~ is the subset of hypotheses from H that are consistent with D (i.e., 
VSH,~ is the version space ofH with respect toD as defined in Chapter 2). Itis easy to verify that P(D) = above, because the sum over all hypothesesof P(hID) must be one and because the number of hypotheses from H consistentwith Dis by definition IVSH,DI. Alternatively, we can derive P(D) from thetheorem of total probability (see Table 6.1) and the fact that the hypotheses aremutually exclusive (i.e., (Vi # j)(P(hiA hj) = 0)) 
To summarize, Bayes theorem implies that the posterior probability P(hID) 
under our assumed P(h) and P(D1h) isif his consistent with DP(hlD) = (6.30 otherwisewhere IVSH,DIis the number of hypotheses from H consistent with D. The evo- 
lution of probabilities associated with hypotheses is depicted schematically inFigure 6.1. Initially (Figure 6.1~) all hypotheses have the same probability. Astraining data accumulates (Figures 6.1 b and 6. lc), the posterior probability forinconsistent hypotheses becomes zero while the total probability summing to oneis shared equally among the remaining consistent hypotheses. 
The above analysis implies that under our choice for P(h) and P(Dlh), everyconsistent hypothesis has posterior probability (1 /IV SH, I), and every inconsistenthypothesis has posterior probability 0. Every consistent hypothesis is, therefore, 
a MAP hypothesis. 
6.3.2 MAP Hypotheses and Consistent LearnersThe above analysis shows that in the given setting, every hypothesis consistentwith Dis a MAP hypothesis. This statement translates directly into an interestingstatement about a general class of learners that we might call consistent learners. 
We will say that a learning algorithm isa consistent learner provided it outputs ahypothesis that commits zero errors over the training examples. Given the aboveanalysis, we can conclude that every consistent learner outputs a MAP hypothesis, 
ifwe assume a uniform prior probability distribution over H (i.e., P(hi) = P(hj) 
for all i, j), and ifwe assume deterministic, noise free training data (i.e., P(DIh) = 
1 ifD and h are consistent, and 0 otherwise). 
Consider, for example, the concept learning algorithm FIND-S discussed inChapter 2. FIND-S searches the hypothesis space H from specific to general hy- 
potheses, outputting a maximally specific consistent hypothesis (i.e., a maximallyspecific member of the version space). Because FIND-S outputs a consistent hy- 
pothesis, we know that it will output a MAP hypothesis under the probabilitydistributions P(h) and P(D1h) defined above. Of course FIND-S does not explic- 
itly manipulate probabilities at all-it simply outputs a maximally specific memberhypotheses hypotheses 
(a) (4hypotheses 
(c) 
FIGURE 6.1Evolution of posterior probabilities P(hlD) with increasing training data. (a) Uniform priors assignequal probability to each hypothesis. As training data increases first toDl (b), then toDl A 02 
(c), the posterior probability of inconsistent hypotheses becomes zero, while posterior probabilitiesincrease for hypotheses remaining in the version space. 
CHAPTER 6 BAYESIAN LEARNING 163of the version space. However, by identifying distributions for P(h) and P(D(h) 
under which its output hypotheses will be MAP hypotheses, we have a useful wayof characterizing the behavior of FIND-S. 
Are there other probability distributions for P(h) and P(D1h) under whichFIND-S outputs MAP hypotheses? Yes. Because FIND-S outputs a maximally spe- 
cz$c hypothesis from the version space, its output hypothesis will bea MAPhypothesis relative to any prior probability distribution that favors more specifichypotheses. More precisely, suppose 3-1 is any probability distribution P(h) overH that assigns P(h1) 2 P(hz) ifhl is more specific than h2. Then it can be shownthat FIND-S outputs a MAP hypothesis assuming the prior distribution 3-1 and thesame distribution P(D1h) discussed above. 
To summarize the above discussion, the Bayesian framework allows oneway to characterize the behavior of learning algorithms (e.g., FIND-S), even whenthe learning algorithm does not explicitly manipulate probabilities. By identifyingprobability distributions P(h) and P(Dlh) under which the algorithm outputsoptimal (i.e., MAP) hypotheses, we can characterize the implicit assumptions 
, under which this algorithm behaves optimally. 
( Using the Bayesian perspective to characterize learning algorithms in thisway is similar in spirit to characterizing the inductive bias of the learner. Recallthat in Chapter 2 we defined the inductive bias ofa learning algorithm to bethe set of assumptions B sufficient to deductively justify the inductive inferenceperformed by the learner. For example, we described the inductive bias of theCANDIDATE-ELIMINATION algorithm as the assumption that the target concept c isincluded in the hypothesis space H. Furthermore, we showed there that the outputof this learning algorithm follows deductively from its inputs plus this implicitinductive bias assumption. The above Bayesian interpretation provides an alter- 
native way to characterize the assumptions implicit in learning algorithms. Here, 
instead of modeling the inductive inference method byan equivalent deductivesystem, we model itby an equivalent probabilistic reasoning system based onBayes theorem. And here the implicit assumptions that we attribute to the learnerare assumptions of the form "the prior probabilities over H are given by thedistribution P(h), and the strength of data in rejecting or accepting a hypothesisis given byP(Dlh)." The definitions ofP(h) and P(D(h) given in this sectioncharacterize the implicit assumptions of the CANDIDATE-ELIMINATION and FIND-Salgorithms. A probabilistic reasoning system based on Bayes theorem will exhibitinput-output behavior equivalent to these algorithms, provided itis given theseassumed probability distributions. 
The discussion throughout this section corresponds toa special case ofBayesian reasoning, because we considered the case where P(D1h) takes on val- 
ues of only 0 and 1, reflecting the deterministic predictions of hypotheses and theassumption of noise-free training data. Aswe shall see in the next section, wecan also model learning from noisy training data, by allowing P(D1h) to take onvalues other than 0 and 1, and by introducing into P(D1h) additional assumptionsabout the probability distributions that govern the noise. 
6.4 MAXIMUM LIKELIHOOD AND LEAST-SQUARED ERRORHYPOTHESESAs illustrated in the above section, Bayesian analysis can sometimes be used toshow that a particular learning algorithm outputs MAP hypotheses even though itmay not explicitly use Bayes rule or calculate probabilities in any form. 
In this section we consider the problem of learning a continuous-valuedtarget function-a problem faced by many learning approaches such as neuralnetwork learning, linear regression, and polynomial curve fitting. A straightfor- 
ward Bayesian analysis will show that under certain assumptions any learningalgorithm that minimizes the squared error between the output hypothesis pre- 
dictions and the training data will output a maximum likelihood hypothesis. Thesignificance of this result is that it provides a Bayesian justification (under cer- 
tain assumptions) for many neural network and other curve fitting methods thatattempt to minimize the sum of squared errors over the training data. 
Consider the following problem setting. Learner L considers an instancespace X and a hypothesis space H consisting of some class of real-valued functionsdefined over X (i.e., each hin His a function of the form h : X -+ 8, where8 represents the set of real numbers). The problem faced byL isto learn anunknown target function f : X -+ 8 drawn from H. A set ofm training examplesis provided, where the target value of each example is corrupted by randomnoise drawn according toa Normal probability distribution. More precisely, eachtraining example isa pair of the form (xi, di) where di = f (xi) + ei. Here f (xi) isthe noise-free value of the target function and eiis a random variable represent- 
ing the noise. Itis assumed that the values of the ei are drawn independently andthat they are distributed according toa Normal distribution with zero mean. Thetask of the learner isto output a maximum likelihood hypothesis, or, equivalently, 
a MAP hypothesis assuming all hypotheses are equally probable a priori. 
A simple example of such a problem is learning a linear function, though ouranalysis applies to learning arbitrary real-valued functions. Figure 6.2 illustratesFIGURE 6.2Learning a real-valued function. The targetfunction f corresponds to the solid line. 
The training examples (xi, di ) are assumedto have Normally distributed noise ei withzero mean added to the true target valuef (xi). The dashed line corresponds to thelinear function that minimizes the sum ofsquared errors. Therefore, itis the maximumI likelihood hypothesis ~ML, given these fivex training examples. 
CHAPTER 6 BAYESIAN LEARNING 165a linear target function f depicted by the solid line, and a set of noisy trainingexamples of this target function. The dashed line corresponds to the hypothesishML with least-squared training error, hence the maximum likelihood hypothesis. 
Notice that the maximum likelihood hypothesis is not necessarily identical to thecorrect hypothesis, f, because itis inferred from only a limited sample of noisytraining data. 
Before showing why a hypothesis that minimizes the sum of squared errorsin this setting is also a maximum likelihood hypothesis, let us quickly review twobasic concepts from probability theory: probability densities and Normal distribu- 
tions. First, in order to discuss probabilities over continuous variables such ase, 
we must introduce probability densities. The reason, roughly, is that we wish forthe total probability over all possible values of the random variable to sum to one. 
In the case of continuous variables we cannot achieve this by assigning a finiteprobability to each of the infinite set of possible values for the random variable. 
Instead, we speak ofa probability density for continuous variables such ase andrequire that the integral of this probability density over all possible values be one. 
In general we will use lower case pto refer to the probability density function, 
to distinguish it from a finite probability P (which we will sometimes refer to asa probability mass). The probability density p(x0) is the limit asE goes to zero, 
of times the probability that x will take ona value in the interval [xo, xo + 6). 
Probability density function: 
Second, we stated that the random noise variable eis generated bya Normalprobability distribution. A Normal distribution isa smooth, bell-shaped distribu- 
tion that can be completely characterized by its mean p and its standard deviationa. See Table 5.4 for a precise definition. 
Given this background we now return to the main issue: showing that theleast-squared error hypothesis is, in fact, the maximum likelihood hypothesiswithin our problem setting. We will show this by deriving the maximum like- 
lihood hypothesis starting with our earlier definition Equation (6.3), but usinglower case pto refer to the probability densityAs before, we assume a fixed set of training instances (xl . . . xm) and there- 
fore consider the data Dto be the corresponding sequence of target valuesD = (dl . . .dm). Here di = f (xi) + ei. Assuming the training examples are mu- 
tually independent given h, we can write P(DJh) as the product of the various 
~(dilh) 
Given that the noise ei obeys a Normal distribution with zero mean and unknownvariance a2, each di must also obey a Normal distribution with variance a2 cen- 
tered around the true target value f (xi) rather than zero. Therefore p(dilh) canbe written asa Normal distribution with variance a2 and mean p = f (xi). Let uswrite the formula for this Normal distribution to describe p(diIh), beginning withthe general formula for a Normal distribution from Table 5.4 and substituting theappropriate p and a2. Because we are writing the expression for the probabilityof di given that his the correct description of the target function f, we will alsosubstitute p = f (xi) = h(xi), yieldingWe now apply a transformation that is common in maximum likelihood calcula- 
tions: Rather than maximizing the above complicated expression we shall chooseto maximize its (less complicated) logarithm. This is justified because lnp is amonotonic function ofp. Therefore maximizing Inp also maximizes p. 
... 1 1hML = argmax xln - - -(di - h(~i))~ 
h€ Hi=l dG7 202The first term in this expression isa constant independent ofh, and can thereforebe discarded, yielding1 
hMr = argmax C -s(di - h(xi)12h€ Hi=lMaximizing this negative quantity is equivalent to minimizing the correspondingpositive quantity. 
Finally, we can again discard constants that are independent ofh. 
Thus, Equation (6.6) shows that the maximum likelihood hypothesis ~ML isthe one that minimizes the sum of the squared errors between the observed trainingvalues di and the hypothesis predictions h(xi). This holds under the assumptionthat the observed training values di are generated by adding random noise toCHAPTER 6 BAYESIAN LEARNING 167the true target value, where this random noise is drawn independently for eachexample from a Normal distribution with zero mean. As the above derivationmakes clear, the squared error term (di -h(~~))~ follows directly from the exponentin the definition of the Normal distribution. Similar derivations can be performedstarting with other assumed noise distributions, producing different results. 
Notice the structure of the above derivation involves selecting the hypothesisthat maximizes the logarithm of the likelihood (Inp(D1h)) in order to determinethe most probable hypothesis. As noted earlier, this yields the same result as max- 
imizing the likelihood p(D1h). This approach of working with the log likelihoodis common to many Bayesian analyses, because itis often more mathematicallytractable than working directly with the likelihood. Of course, as noted earlier, 
the maximum likelihood hypothesis might not be the MAP hypothesis, but if oneassumes uniform prior probabilities over the hypotheses then itis. 
Why isit reasonable to choose the Normal distribution to characterize noise? 
One reason, it must be admitted, is that it allows for a mathematically straightfor- 
ward analysis. A second reason is that the smooth, bell-shaped distribution is agood approximation to many types of noise in physical systems. In fact, the Cen- 
i tral Limit Theorem discussed in Chapter 5 shows that the sum ofa sufficientlylarge number of independent, identically distributed random variables itself obeysa Normal distribution, regardless of the distributions of the individual variables. 
This implies that noise generated by the sum of very many independent, butidentically distributed factors will itself be Normally distributed. Of course, inreality, different components that contribute to noise might not follow identicaldistributions, in which case this theorem will not necessarily justify our choice. 
Minimizing the sum of squared errors isa common approach in many neuralnetwork, curve fitting, and other approaches to approximating real-valued func- 
tions. Chapter 4 describes gradient descent methods that seek the least-squarederror hypothesis in neural network learning. 
Before leaving our discussion of the relationship between the maximumlikelihood hypothesis and the least-squared error hypothesis, itis important tonote some limitations of this problem setting. The above analysis considers noiseonly in the target value of the training example and does not consider noise inthe attributes describing the instances themselves. For example, if the problemis to learn to predict the weight of someone based on that person's age andheight, then the above analysis assumes noise in measurements of weight, butperfect measurements of age and height. The analysis becomes significantly morecomplex as these simplifying assumptions are removed. 
6.5 MAXIMUM LIKELIHOOD HYPOTHESES FOR PREDICTINGPROBABILITIESIn the problem setting of the previous section we determined that the maximumlikelihood hypothesis is the one that minimizes the sum of squared errors over thetraining examples. In this section we derive an analogous criterion for a secondsetting that is common in neural network learning: learning to predict probabilities. 
Consider the setting in which we wish to learn a nondeterministic (prob- 
abilistic) function f : X -+ {0, 11, which has two discrete output values. Forexample, the instance space X might represent medical patients in terms of theirsymptoms, and the target function f (x) might be 1 if the patient survives thedisease and 0 if not. Alternatively, X might represent loan applicants in terms oftheir past credit history, and f (x) might be 1 if the applicant successfully repaystheir next loan and 0 if not. In both of these cases we might well expect fto beprobabilistic. For example, among a collection of patients exhibiting the same setof observable symptoms, we might find that 92% survive, and 8% do not. Thisunpredictability could arise from our inability to observe all the important distin- 
guishing features of the patients, or from some genuinely probabilistic mechanismin the evolution of the disease. Whatever the source of the problem, the effect isthat we have a target function f (x) whose output isa probabilistic function of theinput. 
Given this problem setting, we might wish to learn a neural network (or otherreal-valued function approximator) whose output is the probability that f (x) = 1. 
In other words, we seek to learn the target function, f' : X + [O, 11, such thatf '(x) = P( f (x) = 1). In the above medical patient example, ifx is one of thoseindistinguishable patients of which 92% survive, then f'(x) = 0.92 whereas theprobabilistic function f (x) will be equal to 1 in 92% of cases and equal to 0 inthe remaining 8%. 
How can we learn f' using, say, a neural network? One obvious, brute- 
force way would beto first collect the observed frequencies of 1's and 0's foreach possible value ofx and to then train the neural network to output the targetfrequency for each x. Aswe shall see below, we can instead train a neural networkdirectly from the observed training examples off, yet still derive a maximumlikelihood hypothesis for f '. 
What criterion should we optimize in order to find a maximum likelihoodhypothesis for f' in this setting? To answer this question we must first obtainan expression for P(D1h). Let us assume the training data Dis of the formD = {(xl, dl) . . . (x,, dm)}, where diis the observed 0 or 1 value for f (xi). 
Recall that in the maximum likelihood, least-squared error analysis of theprevious section, we made the simplifying assumption that the instances (xl . . . x,) 
were fixed. This enabled usto characterize the data by considering only the targetvalues di. Although we could make a similar simplifying assumption in this case, 
let us avoid it here in order to demonstrate that it has no impact on the finaloutcome. Thus treating both xi and dias random variables, and assuming thateach training example is drawn independently, we can write P(D1h) asm 
P(Dlh) = n ,(xi, 41,) (6.7) 
i=lIt is reasonable to assume, furthermore, that the probability of encounteringany particular instance xiis independent of the hypothesis h. For example, theprobability that our training set contains a particular patient xiis independent ofour hypothesis about survival rates (though of course the survival d, of the patientCHAPTER 6 BAYESIAN LEARNING 169does depend strongly onh). When xis independent ofh we can rewrite the aboveexpression (applying the product rule from Table 6.1) asNow what is the probability P(dilh, xi) of observing di = 1 for a singleinstance xi, given a world in which hypothesis h holds? Recall that his ourhypothesis regarding the target function, which computes this very probability. 
Therefore, P(di = 1 1 h, xi) = h(xi), and in generalIn order to substitute this into the Equation (6.8) for P(Dlh), let us first 
" re-express itin a more mathematically manipulable form, asI' 
Itis easy to verify that the expressions in Equations (6.9) and (6.10) are equivalent. 
Notice that when di = 1, the second term from Equation (6.10), (1 - h(xi))'-", 
becomes equal to 1. Hence P(di = llh,xi) = h(xi), which is equivalent to thefirst case in Equation (6.9). A similar analysis shows that the two equations arealso equivalent when di = 0. 
We can use Equation (6.10) to substitute for P(dilh, xi) in Equation (6.8) toobtainNow we write an expression for the maximum likelihood hypothesisThe last term isa constant independent ofh, soit can be droppedThe expression on the right side of Equation (6.12) can be seen asa gen- 
eralization of the Binomial distribution described in Table 5.3. The expression inEquation (6.12) describes the probability that flipping each ofm distinct coins willproduce the outcome (dl . . .dm), assuming that each coin xi has probability h(xi) 
of producing a heads. Note the Binomial distribution described in Table 5.3 issimilar, but makes the additional assumption that the coins have identical proba- 
bilities of turning up heads (i.e., that h(xi) = h(xj), Vi, j). In both cases we assumethe outcomes of the coin flips are mutually independent-an assumption that fitsour current setting. 
Asin earlier cases, we will find it easier to work with the log of the likeli- 
hood, yieldingEquation (6.13) describes the quantity that must be maximized in order toobtain the maximum likelihood hypothesis in our current problem setting. Thisresult is analogous to our earlier result showing that minimizing the sum of squarederrors produces the maximum likelihood hypothesis in the earlier problem setting. 
Note the similarity between Equation (6.13) and the general form of the entropyfunction, -xipi log pi, discussed in Chapter 3. Because of this similarity, thenegation of the above quantity is sometimes called the cross entropy. 
6.5.1 Gradient Search to Maximize Likelihood ina Neural NetAbove we showed that maximizing the quantity in Equation (6.13) yields themaximum likelihood hypothesis. Let us use G(h, D) to denote this quantity. Inthis section we derive a weight-training rule for neural network learning that seeksto maximize G(h, D) using gradient ascent. 
As discussed in Chapter 4, the gradient ofG(h, D) is given by the vectorof partial derivatives ofG(h, D) with respect to the various network weights thatdefine the hypothesis h represented by the learned network (see Chapter 4 for ageneral discussion of gradient-descent search and for details of the terminologythat we reuse here). In this case, the partial derivative ofG(h, D) with respect toweight wjk from input kto unit j isTo keep our analysis simple, suppose our neural network is constructed froma single layer of sigmoid units. In this case we havewhere xijk is the kth input to unit j for the ith training example, and d(x) isthe derivative of the sigmoid squashing function (again, see Chapter 4). Finally, 
CIUPlER 6 BAYESIAN LEARNING 171substituting this expression into Equation (6.14), we obtain a simple expressionfor the derivatives that constitute the gradientBecause we seek to maximize rather than minimize P(D(h), we performgradient ascent rather than gradient descent search. On each iteration of the searchthe weight vector is adjusted in the direction of the gradient, using the weight- 
update rulewherem 
Awjk = 7 C(di - hbi)) xijk (6.15) 
i=land where 7 isa small positive constant that determines the step size of thei gradient ascent search. 
Itis interesting to compare this weight-update rule to the weight-updaterule used by the BACKPROPAGATION algorithm to minimize the sum of squarederrors between predicted and observed network outputs. The BACKPROPAGATIONupdate rule for output unit weights (see Chapter 4), re-expressed using our currentnotation, iswhereNotice this is similar to the rule given in Equation (6.15) except for the extra termh(x,)(l - h(xi)), which is the derivative of the sigmoid function. 
To summarize, these two weight update rules converge toward maximumlikelihood hypotheses in two different settings. The rule that minimizes sum ofsquared error seeks the maximum likelihood hypothesis under the assumptionthat the training data can be modeled by Normally distributed noise added to thetarget function value. The rule that minimizes cross entropy seeks the maximumlikelihood hypothesis under the assumption that the observed boolean value is aprobabilistic function of the input instance. 
6.6 MINIMUM DESCRIPTION LENGTH PRINCIPLERecall from Chapter 3 the discussion of Occam's razor, a popular inductive biasthat can be summarized as "choose the shortest explanation for the observeddata." In that chapter we discussed several arguments in the long-standing debateregarding Occam's razor. Here we consider a Bayesian perspective on this issueand a closely related principle called the Minimum Description Length (MDL) 
principle. 
The Minimum Description Length principle is motivated by interpreting thedefinition ofhM~pin the light of basic concepts from information theory. Consideragain the now familiar definition of MAP. 
hMAP = argmax P(Dlh)P(h) 
h€ Hwhich can be equivalently expressed in terms of maximizing the log, 
 MAP = argmax log2 P(Dlh) + log, P(h) 
h€ Hor alternatively, minimizing the negative of this quantityhMAp = argmin - log, P(D 1 h) - log, P(h) 
h€ HSomewhat surprisingly, Equation (6.16) can be interpreted asa statementthat short hypotheses are preferred, assuming a particular representation schemefor encoding hypotheses and data. To explain this, let us introduce a basic resultfrom information theory: Consider the problem of designing a code to transmitmessages drawn at random, where the probability of encountering message i ispi. We are interested here in the most compact code; that is, we are interested inthe code that minimizes the expected number of bits we must transmit in order toencode a message drawn at random. Clearly, to minimize the expected code lengthwe should assign shorter codes to messages that are more probable. Shannon andWeaver (1949) showed that the optimal code (i.e., the code that minimizes theexpected message length) assigns - log, pi bitst to encode message i. We willrefer to the number of bits required to encode message i using code Cas thedescription length of message i with respect toC, which we denote byLc(i). 
Let us interpret Equation (6.16) in light of the above result from codingtheory. 
0 - log, P(h) is the description length ofh under the optimal encoding forthe hypothesis space H. In other words, this is the size of the descriptionof hypothesis h using this optimal representation. In our notation, LC, (h) = 
- log, P(h), where CHis the optimal code for hypothesis space H. 
0 -log2 P(D1h) is the description length of the training data D givenhypothesis h, under its optimal encoding. In our notation, Lc,,,(Dlh) = 
- log, P(Dlh), where CD,~ is the optimal code for describing data D assum- 
ing that both the sender and receiver know the hypothesis h. 
t~otice the expected length for transmitting one message is therefore xi -pi logz pi, the formulafor the entropy (see Chapter 3) of the set of possible messages. 
CHAPTER 6 BAYESIAN LEARNING 1730 Therefore we can rewrite Equation (6.16) to show that hMAP is the hypothesish that minimizes the sum given by the description length of the hypothesisplus the description length of the data given the hypothesis. 
where CH and CDlh are the optimal encodings for H and for D given h, 
respectively. 
The Minimum Description Length (MDL) principle recommends choosingthe hypothesis that minimizes the sum of these two description lengths. Of courseto apply this principle in practice we must choose specific encodings or represen- 
tations appropriate for the given learning task. Assuming we use the codes C1 andCZ to represent the hypothesis and the data given the hypothesis, we can state theMDL principle as1' 
I Minimum Description Length principle: Choose hMDL whereThe above analysis shows that ifwe choose C1 tobe the optimal encodingof hypotheses CH, and ifwe choose C2 tobe the optimal encoding CDlh, then 
~MDL = A MAP. 
Intuitively, we can think of the MDL principle as recommending the shortestmethod for re-encoding the training data, where we count both the size of thehypothesis and any additional cost of encoding the data given this hypothesis. 
Let us consider an example. Suppose we wish to apply the MDL prin- 
ciple to the problem of learning decision trees from some training data. Whatshould we choose for the representations C1 and C2 of hypotheses and data? 
For C1 we might naturally choose some obvious encoding of decision trees, inwhich the description length grows with the number of nodes in the tree andwith the number of edges. How shall we choose the encoding C2 of the datagiven a particular decision tree hypothesis? To keep things simple, suppose thatthe sequence of instances (xl . . .x,) is already known to both the transmitterand receiver, so that we need only transmit the classifications (f (XI) . . . f (x,)). 
(Note the cost of transmitting the instances themselves is independent of the cor- 
rect hypothesis, soit does not affect the selection of ~MDL in any case.) Now ifthe training classifications (f (xl) . . . f (xm)) are identical to the predictions of thehypothesis, then there isno need to transmit any information about these exam- 
ples (the receiver can compute these values once it has received the hypothesis). 
The description length of the classifications given the hypothesis in this case is, 
therefore, zero. In the case where some examples are misclassified byh, thenfor each misclassification we need to transmit a message that identifies whichexample is misclassified (which can be done using at most logzm bits) as wellas its correct classification (which can be done using at most log2 k bits, wherek is the number of possible classifications). The hypothesis hMDL under the en- 
coding~ C1 and C2 is just the one that minimizes the sum of these descriptionlengths. 
Thus the MDL principle provides a way of trading off hypothesis complexityfor the number of errors committed by the hypothesis. It might select a shorterhypothesis that makes a few errors over a longer hypothesis that perfectly classifiesthe training data. Viewed in this light, it provides one method for dealing withthe issue of overjitting the data. 
Quinlan and Rivest (1989) describe experiments applying the MDL principleto choose the best size for a decision tree. They report that the MDL-based methodproduced learned trees whose accuracy was comparable to that of the standard tree- 
pruning methods discussed in Chapter 3. Mehta etal. (1995) describe an alternativeMDL-based approach to decision tree pruning, and describe experiments in whichan MDL-based approach produced results comparable to standard tree-pruningmethods. 
What shall we conclude from this analysis of the Minimum DescriptionLength principle? Does this prove once and for all that short hypotheses are best? 
No. What we have shown is only that ifa representation of hypotheses is chosen sothat the size of hypothesis his - log2 P(h), and ifa representation for exceptionsis chosen so that the encoding length ofD given his equal to -log2 P(Dlh), 
then the MDL principle produces MAP hypotheses. However, to show that wehave such a representation we must know all the prior probabilities P(h), as wellas the P(D1h). There isno reason to believe that the MDL hypothesis relative toarbitrary encodings C1 and C2 should be preferred. Asa practical matter it mightsometimes be easier for a human designer to specify a representation that capturesknowledge about the relative probabilities of hypotheses than itis to fully specifythe probability of each hypothesis. Descriptions in the literature on the applicationof MDL to practical learning problems often include arguments providing someform of justification for the encodings chosen for C1 and C2. 
6.7 BAYES OPTIMAL CLASSIFIERSo far we have considered the question "what is the most probable hypothesisgiven the training data?' In fact, the question that is often of most significance isthe closely related question "what is the most probable classiJication of the newinstance given the training data?'Although it may seem that this second questioncan be answered by simply applying the MAP hypothesis to the new instance, infact itis possible todo better. 
To develop some intuitions consider a hypothesis space containing threehypotheses, hl, h2, and h3. Suppose that the posterior probabilities of these hy- 
potheses given the training data are .4, .3, and .3 respectively. Thus, hlis theMAP hypothesis. Suppose a new instance xis encountered, which is classifiedpositive byhl, but negative by h2 and h3. Taking all hypotheses into account, 
the probability that xis positive is .4 (the probability associated with hi), andCHAFER 6 BAYESIAN LEARNING 175the probability that itis negative is therefore .6. The most probable classification 
(negative) in this case is different from the classification generated by the MAPhypothesis. 
In general, the most probable classification of the new instance is obtainedby combining the predictions of all hypotheses, weighted by their posterior prob- 
abilities. If the possible classification of the new example can take on any valuevj from some set V, then the probability P(vjlD) that the correct classificationfor the new instance isv;, is justThe optimal classification of the new instance is the value v,, for whichP (v; 1 D) is maximum. 
Bayes optimal classification: 
To illustrate in terms of the above example, the set of possible classificationsof the new instance isV = (@, 81, andthereforeandAny system that classifies new instances according to Equation (6.18) iscalled a Bayes optimal classzjier, or Bayes optimal learner. No other classificationmethod using the same hypothesis space and same prior knowledge can outperformthis method on average. This method maximizes the probability that the newinstance is classified correctly, given the available data, hypothesis space, andprior probabilities over the hypotheses. 
For example, in learning boolean concepts using version spaces asin theearlier section, the Bayes optimal classification ofa new instance is obtainedby taking a weighted vote among all members of the version space, with eachcandidate hypothesis weighted by its posterior probability. 
Note one curious property of the Bayes optimal classifier is that the pre- 
dictions it makes can correspond toa hypothesis not contained inH! Imagineusing Equation (6.18) to classify every instance inX. The labeling of instancesdefined in this way need not correspond to the instance labeling of any singlehypothesis h from H. One way to view this situation isto think of the Bayesoptimal classifier as effectively considering a hypothesis space H' different fromthe space of hypotheses Hto which Bayes theorem is being applied. In particu- 
lar, H' effectively includes hypotheses that perform comparisons between linearcombinations of predictions from multiple hypotheses inH. 
6.8 GIBBS ALGORITHMAlthough the Bayes optimal classifier obtains the best performance that can beachieved from the given training data, it can be quite costly to apply. The expenseis due to the fact that it computes the posterior probability for every hypothesisin H and then combines the predictions of each hypothesis to classify each newinstance. 
An alternative, less optimal method is the Gibbs algorithm (see Opper andHaussler 1991), defined as follows: 
1. Choose a hypothesis h from Hat random, according to the posterior prob- 
ability distribution over H. 
2. Use hto predict the classification of the next instance x. 
Given a new instance to classify, the Gibbs algorithm simply applies ahypothesis drawn at random according to the current posterior probability distri- 
bution. Surprisingly, it can be shown that under certain conditions the expectedmisclassification error for the Gibbs algorithm isat most twice the expected errorof the Bayes optimal classifier (Haussler etal. 1994). More precisely, the ex- 
pected value is taken over target concepts drawn at random according to the priorprobability distribution assumed by the learner. Under this condition, the expectedvalue of the error of the Gibbs algorithm isat worst twice the expected value ofthe error of the Bayes optimal classifier. 
This result has an interesting implication for the concept learning problemdescribed earlier. In particular, it implies that if the learner assumes a uniformprior over H, and if target concepts are in fact drawn from such a distributionwhen presented to the learner, then classifying the next instance according toa hypothesis drawn at random from the current version space (according to auniform distribution), will have expected error at most twice that of the Bayesoptimal classijier. Again, we have an example where a Bayesian analysis of anon-Bayesian algorithm yields insight into the performance of that algorithm. 
CHAPTJZR 6 BAYESIAN LEARNING 1776.9 NAIVE BAYES CLASSIFIEROne highly practical Bayesian learning method is the naive Bayes learner, oftencalled the naive Bayes classijier. In some domains its performance has been shownto be comparable to that of neural network and decision tree learning. This sectionintroduces the naive Bayes classifier; the next section applies itto the practicalproblem of learning to classify natural language text documents. 
The naive Bayes classifier applies to learning tasks where each instance xis described bya conjunction of attribute values and where the target functionf (x) can take on any value from some finite set V. A set of training examples ofthe target function is provided, and a new instance is presented, described by thetuple of attribute values (al, a2.. .a,). The learner is asked to predict the targetvalue, or classification, for this new instance. 
The Bayesian approach to classifying the new instance isto assign the mostprobable target value, VMAP, given the attribute values (al, a2 . . . a,) that describethe instance. 
VMAP = argmax P(vjlal, a2. . . a,) 
vj€ vWe can use Bayes theorem to rewrite this expression asNow we could attempt to estimate the two terms in Equation (6.19) based onthe training data. Itis easy to estimate each of the P(vj) simply by counting thefrequency with which each target value vj occurs in the training data. However, 
estimating the different P(al, a2.. . a,lvj) terms in this fashion is not feasibleunless we have a very, very large set of training data. The problem is that thenumber of these terms is equal to the number of possible instances times thenumber of possible target values. Therefore, we need to see every instance inthe instance space many times in order to obtain reliable estimates. 
The naive Bayes classifier is based on the simplifying assumption that theattribute values are conditionally independent given the target value. In otherwords, the assumption is that given the target value of the instance, the probabilityof observing the conjunction al, a2.. .a, is just the product of the probabilitiesfor the individual attributes: P(a1, a2 . . . a, 1 vj) = niP(ai lvj). Substituting thisinto Equation (6.19), we have the approach used by the naive Bayes classifier. 
Naive Bayes classifier: 
VNB = argmax P (vj) nP (ai 1vj) (6.20) 
ujcvwhere VNB denotes the target value output by the naive Bayes classifier. Noticethat ina naive Bayes classifier the number of distinct P(ailvj) terms that mustbe estimated from the training data is just the number of distinct attribute valuestimes the number of distinct target values-a much smaller number than if wewere to estimate the P(a1, a2 . . . an lvj) terms as first contemplated. 
To summarize, the naive Bayes learning method involves a learning step inwhich the various P(vj) and P(ai Jvj) terms are estimated, based on their frequen- 
cies over the training data. The set of these estimates corresponds to the learnedhypothesis. This hypothesis is then used to classify each new instance by applyingthe rule in Equation (6.20). Whenever the naive Bayes assumption of conditionalindependence is satisfied, this naive Bayes classification VNB is identical to theMAP classification. 
One interesting difference between the naive Bayes learning method andother learning methods we have considered is that there isno explicit searchthrough the space of possible hypotheses (in this case, the space of possiblehypotheses is the space of possible values that can be assigned to the various P(vj) 
and P(ailvj) terms). Instead, the hypothesis is formed without searching, simply bycounting the frequency of various data combinations within the training examples. 
6.9.1 An Illustrative ExampleLet us apply the naive Bayes classifier toa concept learning problem we consid- 
ered during our discussion of decision tree learning: classifying days accordingto whether someone will play tennis. Table 3.2 from Chapter 3 provides a setof 14 training examples of the target concept PlayTennis, where each day isdescribed by the attributes Outlook, Temperature, Humidity, and Wind. Here weuse the naive Bayes classifier and the training data from this table to classify thefollowing novel instance: 
(Outlook = sunny, Temperature = cool, Humidity = high, Wind = strong) 
Our task isto predict the target value (yes orno) of the target conceptPlayTennis for this new instance. Instantiating Equation (6.20) to fit the currenttask, the target value VNB is given by 
= argrnax P(vj) P(0utlook = sunny)v,)P(Temperature = coolIvj) 
vj~(yes,no] 
Notice in the final expression that ai has been instantiated using the particularattribute values of the new instance. To calculate VNB we now require 10 proba- 
bilities that can be estimated from the training data. First, the probabilities of thedifferent target values can easily be estimated based on their frequencies over the14 training examplesP(P1ayTennis = yes) = 9/14 = .64P(P1ayTennis = no) = 5/14 = .36CHAETER 6 BAYESIAN LEARNING 179Similarly, we can estimate the conditional probabilities. For example, those forWind = strong areP(Wind = stronglPlayTennis = yes) = 319 = .33P(Wind = strongl PlayTennis = no) = 315 = .60Using these probability estimates and similar estimates for the remaining attributevalues, we calculate VNB according to Equation (6.21) as follows (now omittingattribute names for brevity) 
Thus, the naive Bayes classifier assigns the target value PlayTennis = noto thisnew instance, based on the probability estimates learned from the training data. 
Furthermore, by normalizing the above quantities to sum to one we can calculatethe conditional probability that the target value isno, given the observed attributevalues. For the current example, this probability is ,02$ym,, = -795. 
6.9.1.1 ESTIMATING PROBABILITIESUp to this point we have estimated probabilities by the fraction of times the eventis observed to occur over the total number of opportunities. For example, in theabove case we estimated P(Wind = strong] Play Tennis = no) by the fraction % 
where n = 5 is the total number of training examples for which PlayTennis = no, 
and n, = 3 is the number of these for which Wind = strong. 
While this observed fraction provides a good estimate of the probability inmany cases, it provides poor estimates when n, is very small. To see the difficulty, 
imagine that, in fact, the value ofP(Wind = strongl PlayTennis = no) is .08 andthat we have a sample containing only 5 examples for which PlayTennis = no. 
Then the most probable value for n, is 0. This raises two difficulties. First, $ pro- 
duces a biased underestimate of the probability. Second, when this probability es- 
timate is zero, this probability term will dominate the Bayes classifier if the futurequery contains Wind = strong. The reason is that the quantity calculated in Equa- 
tion (6.20) requires multiplying all the other probability terms by this zero value. 
To avoid this difficulty we can adopt a Bayesian approach to estimating theprobability, using the m-estimate defined as follows. 
m-estimate of probability: 
Here, n, and n are defined as before, pis our prior estimate of the probabilitywe wish to determine, and mis a constant called the equivalent sample size, 
which determines how heavily to weight p relative to the observed data. A typicalmethod for choosing pin the absence of other information isto assume uniformpriors; that is, ifan attribute has k possible values we set p = i. For example, inestimating P(Wind = stronglPlayTennis = no) we note the attribute Wind hastwo possible values, so uniform priors would correspond to choosing p = .5. Notethat ifm is zero, the m-estimate is equivalent to the simple fraction 2. If both nand m are nonzero, then the observed fraction 2 and prior p will be combinedaccording to the weight m. The reason mis called the equivalent sample size isthat Equation (6.22) can be interpreted as augmenting the n actual observationsby an additional m virtual samples distributed according top. 
6.10 AN EXAMPLE: LEARNING TO CLASSIFY TEXTTo illustrate the practical importance of Bayesian learning methods, consider learn- 
ing problems in which the instances are text documents. For example, we mightwish to learn the target concept "electronic news articles that I find interesting," 
or "pages on the World Wide Web that discuss machine learning topics." In bothcases, ifa computer could learn the target concept accurately, it could automat- 
ically filter the large volume of online text documents to present only the mostrelevant documents to the user. 
We present here a general algorithm for learning to classify text, basedon the naive Bayes classifier. Interestingly, probabilistic approaches such as theone described here are among the most effective algorithms currently known forlearning to classify text documents. Examples of such systems are described byLewis (1991), Lang (1995), and Joachims (1996). 
The naive Bayes algorithm that we shall present applies in the followinggeneral setting. Consider an instance space X consisting of all possible text docu- 
ments (i.e., all possible strings of words and punctuation of all possible lengths). 
We are given training examples of some unknown target function f (x), whichcan take on any value from some finite set V. The task isto learn from thesetraining examples to predict the target value for subsequent text documents. Forillustration, we will consider the target function classifying documents as interest- 
ing or uninteresting toa particular person, using the target values like and disliketo indicate these two classes. 
The two main design issues involved in applying the naive Bayes classifierto such rext classification problems are first to decide how to represent an arbitrarytext document in terms of attribute values, and second to decide how to estimatethe probabilities required by the naive Bayes classifier. 
Our approach to representing arbitrary text documents is disturbingly simple: 
Given a text document, such as this paragraph, we define an attribute for each wordposition in the document and define the value of that attribute tobe the Englishword found in that position. Thus, the current paragraph would be described by11 1 attribute values, corresponding to the 11 1 word positions. The value of thefirst attribute is the word "our," the value of the second attribute is the word 
"approach," and soon. Notice that long text documents will require a largernumber of attributes than short documents. Aswe shall see, this will not causeus any trouble. 
CHAPTER 6 BAYESIAN LEARNING 181Given this representation for text documents, we can now apply the naiveBayes classifier. For the sake of concreteness, let us assume we are given a set of700 training documents that a friend has classified as dislike and another 300 shehas classified as like. We are now given a new document and asked to classifyit. Again, for concreteness let us assume the new text document is the precedingparagraph. In this case, we instantiate Equation (6.20) to calculate the naive Bayesclassification as 
-a- 
Vns = argmax P(Vj) n ~(ai lvj) 
vj~{like,dislike} i=l 
- - argmax P(vj) P(a1 = "our"lvj)P(a2 = "approach"lvj) 
v, ~{like,dislike} 
To summarize, the naive Bayes classification VNB is the classification that max- 
imizes the probability of observing the words that were actually found in theI document, subject to the usual naive Bayes independence assumption. The inde- 
F pendence assumption P(al, . . . alll lvj) = nfL1 P(ai lvj) states in this setting thatthe word probabilities for one text position are independent of the words that oc- 
cur in other positions, given the document classification vj. Note this assumptionis clearly incorrect. For example, the probability of observing the word "learning" 
in some position may be greater if the preceding word is "machine." Despite theobvious inaccuracy of this independence assumption, we have little choice but tomake it-without it, the number of probability terms that must be computed isprohibitive. Fortunately, in practice the naive Bayes learner performs remarkablywell in many text classification problems despite the incorrectness of this indepen- 
dence assumption. Dorningos and Pazzani (1996) provide an interesting analysisof this fortunate phenomenon. 
To calculate VNB using the above expression, we require estimates for theprobability terms P(vj) and P(ai = wklvj) (here we introduce wkto indicate the kthword in the English vocabulary). The first of these can easily be estimated basedon the fraction of each class in the training data (P(1ike) = .3 and P(dis1ike) = .7in the current example). As usual, estimating the class conditional probabilities 
(e.g., P(al = "our"ldis1ike)) is more problematic because we must estimate onesuch probability term for each combination of text position, English word, andtarget value. Unfortunately, there are approximately 50,000 distinct words in theEnglish vocabulary, 2 possible target values, and 11 1 text positions in the currentexample, sowe must estimate 2. 11 1 -50,000 = 10 million such terms from thetraining data. 
Fortunately, we can make an additional reasonable assumption that reducesthe number of probabilities that must be estimated. In particular, we shall as- 
sume the probability of encountering a specific word wk (e.g., "chocolate") isindependent of the specific word position being considered (e.g., a23 versus agg). 
More formally, this amounts to assuming that the attributes are independent andidentically distributed, given the target classification; that is, P(ai = wk)vj) = 
P(a, = wkJvj) for all i, j, k, m. Therefore, we estimate the entire set of proba- 
bilities P(a1 = wk lvj), P(a2 = wklv,) . . . by the single position-independent prob- 
ability P(wklvj), which we will use regardless of the word position. The neteffect is that we now require only 2.50,000 distinct terms of the form P(wklvj). 
This is still a large number, but manageable. Notice in cases where training datais limited, the primary advantage of making this assumption is that it increasesthe number of examples available to estimate each of the required probabilities, 
thereby increasing the reliability of the estimates. 
To complete the design of our learning algorithm, we must still choose amethod for estimating the probability terms. We adopt the m-estimate-Equa- 
tion (6.22)-with uniform priors and with rn equal to the size of the word vocab- 
ulary. Thus, the estimate for P(wklvj) will bewhere nis the total number of word positions in all training examples whosetarget value isvj, nkis the number of times word wkis found among these nword positions, and I Vocabulary Iis the total number of distinct words (and othertokens) found within the training data. 
To summarize, the final algorithm uses a naive Bayes classifier togetherwith the assumption that the probability of word occurrence is independent ofposition within the text. The final algorithm is shown in Table 6.2. Notice the al- 
gorithm is quite simple. During learning, the procedure LEARN~AIVEBAYES-TEXTexamines all training documents to extract the vocabulary of all words and to- 
kens that appear in the text, then counts their frequencies among the differenttarget classes to obtain the necessary probability estimates. Later, given a newdocument tobe classified, the procedure CLASSINSAIVEJ~AYES-TEXT uses theseprobability estimates to calculate VNB according to Equation (6.20). Note thatany words appearing in the new document that were not observed in the train- 
ing set are simply ignored by CLASSIFYSAIVEBAYES-TEXT. Code for this algo- 
rithm, as well as training data sets, are available on the World Wide Web athttp://www.cs.cmu.edu/-tom/book.htrnl. 
6.10.1 Experimental ResultsHow effective is the learning algorithm of Table 6.2? In one experiment (seeJoachims 1996), a minor variant of this algorithm was applied to the problemof classifying usenet news articles. The target classification for an article in thiscase was the name of the usenet newsgroup in which the article appeared. Onecan think of the task as creating a newsgroup posting service that learns toas- 
sign documents to the appropriate newsgroup. In the experiment described byJoachims (1996), 20 electronic newsgroups were considered (listed in Table 6.3). 
Then 1,000 articles were collected from each newsgroup, forming a data set of20,000 documents. The naive Bayes algorithm was then applied using two-thirdsof these 20,000 documents as training examples, and performance was measuredCHAPTER 6 BAYESIAN LEARNING 183Examples isa set of text documents along with their target values. Vis the set of all possible targetvalues. This function learns the probability terms P(wkIv,), describing the probability that a randomlydrawn word from a document in class vj will be the English word wk. It also learns the class priorprobabilities P(vj). 
1. collect all words, punctwtion, and other tokens that occur in Examplesa Vocabulary c the set of all distinct words and other tokens occurring in any text documentfrom Examples2. calculate the required P(vj) and P(wkJvj) probability termsFor each target value vjin V dodocsj t the subset of documents from Examples for which the target value is vjldocs . IP(uj) + 1ExornLlesla Texti ca single document created by concatenating all members of docsia n +*total number of distinct word positions in ~exc0 for each word wkin Vocabulary0 nkc number of times word wk occurs in TextjP(wk lvj) + n+12LLoryl 
" Return the estimated target value for the document Doc. ai denotes the word found in the ith positionwithin Doc. 
0 positions t all word positions in Doc that contain tokens found in Vocabularya Return VNB, whereVNB = argmax ~(vj) nP(ai 19) 
V, EV ieposirionsTABLE 6.2Naive Bayes algorithms for learning and classifying text. In addition to the usual naive Bayes as- 
sumptions, these algorithms assume the probability ofa word occurring is independent of its positionwithin the text. 
over the remaining third. Given 20 possible newsgroups, we would expect randomguessing to achieve a classification accuracy of approximately 5%. The accuracyachieved by the program was 89%. The algorithm used in these experiments wasexactly the algorithm of Table 6.2, with one exception: Only a subset of the wordsoccurring in the documents were included as the value of the Vocabulary vari- 
able in the algorithm. In particular, the 100 most frequent words were removed 
(these include words such as "the" and "of '), and any word occurring fewer thanthree times was also removed. The resulting vocabulary contained approximately38,500 words. 
Similarly impressive results have been achieved by others applying similarstatistical learning approaches to text classification. For example, Lang (1995) 
describes another variant of the naive Bayes algorithm and its application tolearning the target concept "usenet articles that I find interesting." He describesthe NEWSWEEDER system-a program for reading netnews that allows the user torate articles ashe or she reads them. NEWSWEEDER then uses these rated articles asTABLE 6.3Twenty usenet newsgroups used in the text classification experiment. After training on 667 articlesfrom each newsgroup, a naive Bayes classifier achieved an accuracy of 89% predicting to whichnewsgroup subsequent articles belonged. Random guessing would produce an accuracy of only 5%. 
training examples to learn to predict which subsequent articles will beof interestto the user, so that it can bring these to the user's attention. Lang (1995) reportsexperiments in which NEWSWEEDER used its learned profile of user interests tosuggest the most highly rated new articles each day. By presenting the user withthe top 10% of its automatically rated new articles each day, it created a pool ofarticles containing three to four times as many interesting articles as the generalpool of articles read by the user. For example, for one user the fraction of articlesrated "interesting" was 16% overall, but was 59% among the articles recommendedby NEWSWEEDER. 
Several other, non-Bayesian, statistical text learning algorithms are common, 
many based on similarity metrics initially developed for information retrieval (e.g., 
see Rocchio 197 1; Salton 199 1). Additional text learning algorithms are describedin Hearst and Hirsh (1996). 
6.11 BAYESIAN BELIEF NETWORKSAs discussed in the previous two sections, the naive Bayes classifier makes signif- 
icant use of the assumption that the values of the attributes a1 . . .a, are condition- 
ally independent given the target value v. This assumption dramatically reducesthe complexity of learning the target function. When itis met, the naive Bayesclassifier outputs the optimal Bayes classification. However, in many cases thisconditional independence assumption is clearly overly restrictive. 
A Bayesian belief network describes the probability distribution governing aset of variables by specifying a set of conditional independence assumptions alongwith a set of conditional probabilities. In contrast to the naive Bayes classifier, 
which assumes that all the variables are conditionally independent given the valueof the target variable, Bayesian belief networks allow stating conditional indepen- 
dence assumptions that apply to subsets of the variables. Thus, Bayesian beliefnetworks provide an intermediate approach that is less constraining than the globalassumption of conditional independence made by the naive Bayes classifier, butmore tractable than avoiding conditional independence assumptions altogether. 
Bayesian belief networks are an active focus of current research, and a variety ofalgorithms have been proposed for learning them and for using them for inference. 
CHAPTER 6 BAYESIAN LEARNING 185In this section we introduce the key concepts and the representation of Bayesianbelief networks. More detailed treatments are given by Pearl (1988), Russell andNorvig (1995), Heckerman etal. (1995), and Jensen (1996). 
In general, a Bayesian belief network describes the probability distributionover a set of variables. Consider an arbitrary set of random variables Yl . . . Y,, 
where each variable Yi can take on the set of possible values V(Yi). We definethe joint space of the set of variables Yto be the cross product V(Yl) xV(Y2) x 
. . . V(Y,). In other words, each item in the joint space corresponds to one of thepossible assignments of values to the tuple of variables (Yl . . . Y,). The probabilitydistribution over this joint' space is called the joint probability distribution. Thejoint probability distribution specifies the probability for each of the possiblevariable bindings for the tuple (Yl . . . Y,). A Bayesian belief network describesthe joint probability distribution for a set of variables. 
6.11.1 Conditional Independencei Let us begin our discussion of Bayesian belief networks by defining preciselythe notion of conditional independence. Let X, Y, and Zbe three discrete-valuedrandom variables. We say that Xis conditionally independent ofY given Z ifthe probability distribution governing Xis independent of the value ofY given avalue for 2; that is, ifwhere xiE V(X), yjE V(Y), and zkE V(Z). We commonly write the aboveexpression in abbreviated form asP(XIY, Z) = P(X1Z). This definition of con- 
ditional independence can be extended to sets of variables as well. We say thatthe set of variables X1 . . . Xiis conditionally independent of the set of variablesYl . . . Ym given the set of variables 21 . . . Z, ifP(X1 ... XIJY1 ... Ym, z1 ... Z,) = P(Xl ... X1]Z1 ... Z,) 
Note the correspondence between this definition and our use of conditional , 
independence in the definition of the naive Bayes classifier. The naive Bayesclassifier assumes that the instance attribute A1 is conditionally independent ofinstance attribute A2 given the target value V. This allows the naive Bayes clas- 
sifier to calculate P(Al, A21V) in Equation (6.20) as followsEquation (6.23) is just the general form of the product rule of probability fromTable 6.1. Equation (6.24) follows because if A1 is conditionally independent ofA2 given V, then by our definition of conditional independence P (A1 IA2, V) = 
P(A1IV). 
S,BS,-B 7S.B 1s.-B 
-C 0.6 0.9 0.2CampfireFIGURE 6.3A Bayesian belief network. The network on the left represents a set of conditional independenceassumptions. In particular, each node is asserted tobe conditionally independent of its nondescen- 
dants, given its immediate parents. Associated with each node isa conditional probability table, 
which specifies the conditional distribution for the variable given its immediate parents in the graph. 
The conditional probability table for the Campjire node is shown at the right, where Campjire isabbreviated toC, Storm abbreviated toS, and BusTourGroup abbreviated toB. 
6.11.2 RepresentationA Bayesian belief network (Bayesian network for short) represents the joint prob- 
ability distribution for a set of variables. For example, the Bayesian network inFigure 6.3 represents the joint probability distribution over the boolean variablesStorm, Lightning, Thunder, ForestFire, Campjre, and BusTourGroup. In general, 
a Bayesian network represents the joint probability distribution by specifying aset of conditional independence assumptions (represented bya directed acyclicgraph), together with sets of local conditional probabilities. Each variable in thejoint space is represented bya node in the Bayesian network. For each variable twotypes of information are specified. First, the network arcs represent the assertionthat the variable is conditionally independent of its nondescendants in the networkgiven its immediate predecessors in the network. We say Xjis a descendant of 
, Yif there isa directed path from Yto X. Second, a conditional probability tableis given for each variable, describing the probability distribution for that variablegiven the values of its immediate predecessors. The joint probability for any de- 
sired assignment of values (yl, . . . , y,) to the tuple of network variables (YI . . . Y,) 
can be computed by the formulan 
~(YI,. . . , yd = np(yi~parents(~i)) 
i=lwhere Parents(Yi) denotes the set of immediate predecessors ofYi in the net- 
work. Note the values ofP(yiJ Parents(Yi)) are precisely the values stored in theconditional probability table associated with node Yi. 
To illustrate, the Bayesian network in Figure 6.3 represents the joint prob- 
ability distribution over the boolean variables Storm, Lightning, Thunder, Forest- 
CHmR 6 BAYESIAN LEARNING 187Fire, Campfire, and BusTourGroup. Consider the node Campjire. The networknodes and arcs represent the assertion that CampJire is conditionally indepen- 
dent of its nondescendants Lightning and Thunder, given its immediate parentsStorm and BusTourGroup. This means that once we know the value of the vari- 
ables Storm and BusTourGroup, the variables Lightning and Thunder provide noadditional information about Campfire. The right side of the figure shows theconditional probability table associated with the variable Campfire. The top leftentry in this table, for example, expresses the assertion thatP(Campfire = TruelStorm = True, BusTourGroup = True) = 0.4Note this table provides only the conditional probabilities of Campjire given itsparent variables Storm and BusTourGroup. The set of local conditional probabilitytables for all the variables, together with the set of conditional independence as- 
sumptions described by the network, describe the full joint probability distributionfor the network. 
One attractive feature of Bayesian belief networks is that they allow a con- 
venient way to represent causal knowledge such as the fact that Lightning causesThunder. In the terminology of conditional independence, we express this by stat- 
ing that Thunder is conditionally independent of other variables in the network, 
given the value of Lightning. Note this conditional independence assumption isimplied by the arcs in the Bayesian network of Figure 6.3. 
6.11.3 InferenceWe might wish to use a Bayesian network to infer the value of some targetvariable (e.g., ForestFire) given the observed values of the other variables. Ofcourse, given that we are dealing with random variables it will not generally becorrect to assign the target variable a single determined value. What we reallywish to infer is the probability distribution for the target variable, which specifiesthe probability that it will take on each of its possible values given the observedvalues of the other variables. This inference step can be straightforward if valuesfor all of the other variables in the network are known exactly. In the moregeneral case we may wish to infer the probability distribution for some variable 
(e.g., ForestFire) given observed values for only a subset of the other variables 
(e.g., Thunder and BusTourGroup may be the only observed values available). Ingeneral, a Bayesian network can be used to compute the probability distributionfor any subset of network variables given the values or distributions for any subsetof the remaining variables. 
Exact inference of probabilities in general for an arbitrary Bayesian net- 
work is known tobe NP-hard (Cooper 1990). Numerous methods have beenproposed for probabilistic inference in Bayesian networks, including exact infer- 
ence methods and approximate inference methods that sacrifice precision to gainefficiency. For example, Monte Carlo methods provide approximate solutions byrandomly sampling the distributions of the unobserved variables (Pradham andDagum 1996). In theory, even approximate inference of probabilities in Bayesiannetworks can beNP-hard (Dagum and Luby 1993). Fortunately, in practice ap- 
proximate methods have been shown tobe useful in many cases. Discussions ofinference methods for Bayesian networks are provided by Russell and Norvig 
(1995) and by Jensen (1996). 
6.11.4 Learning Bayesian Belief NetworksCan we devise effective algorithms for learning Bayesian belief networks fromtraining data? This question isa focus of much current research. Several differentsettings for this learning problem can be considered. First, the network structuremight be given in advance, orit might have tobe inferred from the training data. 
Second, all the network variables might be directly observable in each trainingexample, or some might be unobservable. 
In the case where the network structure is given in advance and the variablesare fully observable in the training examples, learning the conditional probabilitytables is straightforward. We simply estimate the conditional probability tableentries just aswe would for a naive Bayes classifier. 
In the case where the network structure is given but only some of the variablevalues are observable in the training data, the learning problem is more difficult. 
This problem is somewhat analogous to learning the weights for the hidden units inan artificial neural network, where the input and output node values are given butthe hidden unit values are left unspecified by the training examples. In fact, Russellet al. (1995) propose a similar gradient ascent procedure that learns the entries inthe conditional probability tables. This gradient ascent procedure searches througha space of hypotheses that corresponds to the set of all possible entries for theconditional probability tables. The objective function that is maximized duringgradient ascent is the probability P(D1h) of the observed training data D giventhe hypothesis h. By definition, this corresponds to searching for the maximumlikelihood hypothesis for the table entries. 
6.11.5 Gradient Ascent Training of Bayesian NetworksThe gradient ascent rule given by Russell etal. (1995) maximizes P(D1h) byfollowing the gradient ofIn P(DIh) with respect to the parameters that define theconditional probability tables of the Bayesian network. Let wi;k denote a singleentry in one of the conditional probability tables. In particular, let wijk denotethe conditional probability that the network variable Yi will take on the value yi, 
given that its immediate parents Ui take on the values given by uik. For example, 
if wijk is the top right entry in the conditional probability table in Figure 6.3, thenYi is the variable Campjire, Uiis the tuple of its parents (Stomz, BusTourGroup), 
yij = True, and uik = (False, False). The gradient ofIn P(D1h) is given bythe derivatives for each of the toijk. Aswe show below, each of thesederivatives can be calculated asCHAPTER 6 BAYESIAN LEARNING 189For example, to calculate the derivative ofIn P(D1h) with respect to the upper- 
rightmost entry in the table of Figure 6.3 we will have to calculate the quan- 
tity P(Campf ire = True, Storm = False, BusTourGroup = Falseld) for eachtraining example din D. When these variables are unobservable for the trainingexample d, this required probability can be calculated from the observed variablesin d using standard Bayesian network inference. In fact, these required quantitiesare easily derived from the calculations performed during most Bayesian networkinference, so learning can be performed at little additional cost whenever theBayesian network is used for inference and new evidence is subsequently obtained. 
Below we derive Equation (6.25) following Russell etal. (1995). The re- 
mainder of this section may be skipped ona first reading without loss of continuity. 
To simplify notation, in this derivation we will write the abbreviation Ph(D) torepresent P(DJh). Thus, our problem isto derive the gradient defined by the setof derivatives for all i, j, and k. Assuming the training examples din thedata set D are drawn independently, we write this derivative asThis last step makes use of the general equality 9 = 1- f(~) ax . W can nowintroduce the values of the variables Yi and Ui = Parents(Yi), by summing overtheir possible values yijl and uiu. 
This last step follows from the product rule of probability, Table 6.1. Now considerthe rightmost sum in the final expression above. Given that Wijk = Ph(yijl~ik), theonly term in this sum for which & is nonzero is the term for which j' = j andi' = i. ThereforeApplying Bayes theorem to rewrite Ph (dlyij, uik), we haveThus, we have derived the gradient given in Equation (6.25). There is one moreitem that must be considered before we can state the gradient ascent trainingprocedure. In particular, we require that as the weights wijk are updated theymust remain valid probabilities in the interval [0,1]. We also require that thesum xj wijk remains 1 for all i, k. These constraints can be satisfied by updatingweights ina two-step process. First we update each wijk by gradient ascentwhere qis a small constant called the learning rate. Second, we renormalizethe weights wijk to assure that the above constraints are satisfied. As discussedby Russell etal., this process will converge toa locally maximum likelihoodhypothesis for the conditional probabilities in the Bayesian network. 
Asin other gradient-based approaches, this algorithm is guaranteed only tofind some local optimum solution. An alternative to gradient ascent is the EMalgorithm discussed in Section 6.12, which also finds locally maximum likelihoodsolutions. 
6.11.6 Learning the Structure of Bayesian NetworksLearning Bayesian networks when the network structure is not known in advanceis also difficult. Cooper and Herskovits (1992) present a Bayesian scoring metricfor choosing among alternative networks. They also present a heuristic searchalgorithm called K2 for learning network structure when the data is fully observ- 
able. Like most algorithms for learning the structure of Bayesian networks, K2performs a greedy search that trades off network complexity for accuracy over thetraining data. In one experiment K2 was given a set of 3,000 training examplesgenerated at random from a manually constructed Bayesian network containing37 nodes and 46 arcs. This particular network described potential anesthesia prob- 
lems ina hospital operating room. In addition to the data, the program was alsogiven an initial ordering over the 37 variables that was consistent with the partialCHAPTER 6 BAYESIAN LEARNING 191ordering of variable dependencies in the actual network. The program succeededin reconstructing the correct Bayesian network structure almost exactly, with theexception of one incorrectly deleted arc and one incorrectly added arc. 
Constraint-based approaches to learning Bayesian network structure havealso been developed (e.g., Spirtes etal. 1993). These approaches infer indepen- 
dence and dependence relationships from the data, and then use these relation- 
ships to construct Bayesian networks. Surveys of current approaches to learningBayesian networks are provided by Heckerman (1995) and Buntine (1994). 
6.12 THE EM ALGORITHMIn many practical learning settings, only a subset of the relevant instance featuresmight be observable. For example, in training or using the Bayesian belief networkof Figure 6.3, we might have data where only a subset of the network variablesStorm, Lightning, Thunder, ForestFire, Campfire, and BusTourGroup have beenobserved. Many approaches have been proposed to handle the problem of learningin the presence of unobserved variables. Aswe saw in Chapter 3, if some variable 
/ is sometimes observed and sometimes not, then we can use the cases for whichit has been observed to learn to predict its values when itis not. In this sectionwe describe the EM algorithm (Dempster etal. 1977), a widely used approachto learning in the presence of unobserved variables. The EM algorithm can beused even for variables whose value is never directly observed, provided thegeneral form of the probability distribution governing these variables is known. 
The EM algorithm has been used to train Bayesian belief networks (see Heckerman1995) as well as radial basis function networks discussed in Section 8.4. The EMalgorithm is also the basis for many unsupervised clustering algorithms (e.g., 
Cheeseman etal. 1988), and itis the basis for the widely used Baum-Welchforward-backward algorithm for learning Partially Observable Markov Models 
(Rabiner 1989). 
6.12.1 Estimating Means ofk GaussiansThe easiest way to introduce the EM algorithm is via an example. Consider aproblem in which the data Dis a set of instances generated bya probabilitydistribution that isa mixture ofk distinct Normal distributions. This problemsetting is illustrated in Figure 6.4 for the case where k = 2 and where the instancesare the points shown along the x axis. Each instance is generated using a two-stepprocess. First, one of the k Normal distributions is selected at random. Second, 
a single random instance xiis generated according to this selected distribution. 
This process is repeated to generate a set of data points as shown in the figure. Tosimplify our discussion, we consider the special case where the selection of thesingle Normal distribution at each step is based on choosing each with uniformprobability, where each of the k Normal distributions has the same variance a2, andwhere a2 is known. The learning task isto output a hypothesis h = (FI, . . . pk) 
that describes the means of each of the k distributions. We would like to findFIGURE 6.4Instances generated bya mixture of two Normal distributions with identical variance a. The instancesare shown by the points along the x axis. If the means of the Normal distributions are unknown, theEM algorithm can be used to search for their maximum likelihood estimates. 
a maximum likelihood hypothesis for these means; that is, a hypothesis h thatmaximizes p(Dlh). 
Note itis easy to calculate the maximum likelihood hypothesis for the meanof a single Normal distribution given the observed data instances XI, x2, . . . , xmdrawn from this single distribution. This problem of finding the mean ofa singledistribution is just a special case of the problem discussed in Section 6.4, Equa- 
tion (6.6), where we showed that the maximum likelihood hypothesis is the onethat minimizes the sum of squared errors over the m training instances. RestatingEquation (6.6) using our current notation, we haveIn this case, the sum of squared errors is minimized by the sample meanOur problem here, however, involves a mixture ofk different Normal dis- 
tributions, and we cannot observe which instances were generated by which dis- 
tribution. Thus, we have a prototypical example ofa problem involving hiddenvariables. In the example of Figure 6.4, we can think of the full description ofeach instance as the triple (xi, zil , ziz), where xiis the observed value of the ithinstance and where zil and zi2 indicate which of the two Normal distributions wasused to generate the value xi. In particular, zij has the value 1 ifxi was created bythe jth Normal distribution and 0 otherwise. Here xiis the observed variable inthe description of the instance, and zil and zi2 are hidden variables. If the valuesof zil and zi2 were observed, we could use Equation (6.27) to solve for the meansp1 and p2. Because they are not, we will instead use the EM algorithm. 
Applied to our k-means problem the EM algorithm searches for a maximumlikelihood hypothesis by repeatedly re-estimating the expected values of the hid- 
den variables zij given its current hypothesis (pI . . . pk), then recalculating theCHAPTER 6 BAYESIAN LEARNING 193maximum likelihood hypothesis using these expected values for the hidden vari- 
ables. We will first describe this instance of the EM algorithm, and later state theEM algorithm in its general form. ' 
Applied to the problem of estimating the two means for Figure 6.4, theEM algorithm first initializes the hypothesis toh = (PI, p2), where p1 and p2 arearbitrary initial values. It then iteratively re-estimates hby repeating the followingtwo steps until the procedure converges toa stationary value for h. 
Step 1: Calculate the expected value E[zij] of each hidden variable zi,, assumingthe current hypothesis h = (p1, p2) holds. 
Step 2: Calculate a new maximum likelihood hypothesis h' = (pi, p;), assumingthe value taken onby each hidden variable zij is its expected value E[zij] 
calculated in Step 1. Then replace the hypothesis h = (pl, p2) by thenew hypothesis h' = (pi, pi) and iterate. 
Let us examine how both of these steps can be implemented in practice. 
/ Step 1 must calculate the expected value of each zi,. This E[4] is just the prob- 
ability that instance xi was generated by the jth Normal distributionThus the first step is implemented by substituting the current values (pl, p2) andthe observed xi into the above expression. 
In the second step we use the E[zij] calculated during Step 1 to derive anew maximum likelihood hypothesis h' = (pi, pi). ASwe will discuss later, themaximum likelihood hypothesis in this case is given byNote this expression is similar to the sample mean from Equation (6.28) that isused to estimate p for a single Normal distribution. Our new expression is justthe weighted sample mean for pj, with each instance weighted by the expectationE[z,j] that it was generated by the jth Normal distribution. 
The above algorithm for estimating the means ofa mixture ofk Normaldistributions illustrates the essence of the EM approach: The current hypothesisis used to estimate the unobserved variables, and the expected values of thesevariables are then used to calculate an improved hypothesis. It can be proved thaton each iteration through this loop, the EM algorithm increases the likelihoodP(Dlh) unless itis ata local maximum. The algorithm thus converges toa localmaximum likelihood hypothesis for (pl, w2). 
6.12.2 General Statement ofEM AlgorithmAbove we described anEM algorithm for the problem of estimating means of amixture of Normal distributions. More generally, the EM algorithm can be appliedin many settings where we wish to estimate some set of parameters 8 that describean underlying probability distribution, given only the observed portion of the fulldata produced by this distribution. In the above two-means example the parametersof interest were 8 = (PI, p2), and the full data were the triples (xi, zil, zi2) ofwhich only the xi were observed. In general let X = {xl, . . . , x,} denote theobserved data ina set ofm independently drawn instances, let Z = {zl, . . . , z,} 
denote the unobserved data in these same instances, and let Y = XU Z denotethe full data. Note the unobserved Z can be treated asa random variable whoseprobability distribution depends on the unknown parameters 8 and on the observeddata X. Similarly, Yis a random variable because itis defined in terms of therandom variable Z. In the remainder of this section we describe the general formof the EM algorithm. We use hto denote the current hypothesized values of theparameters 8, and h' to denote the revised hypothesis that is estimated on eachiteration of the EM algorithm. 
The EM algorithm searches for the maximum likelihood hypothesis h' byseeking the h' that maximizes E[lnP(Y (h')]. This expected value is taken overthe probability distribution governing Y, which is determined by the unknownparameters 8. Let us consider exactly what this expression signifies. First, P(Ylhl) 
is the likelihood of the full data Y given hypothesis h'. Itis reasonable that we wishto find ah' that maximizes some function of this quantity. Second, maximizingthe logarithm of this quantity InP(Ylhl) also maximizes P(Ylhl), aswe havediscussed on several occasions already. Third, we introduce the expected valueE[lnP(Ylhl)] because the full data Yis itself a random variable. Given thatthe full data Yis a combination of the observed data X and unobserved dataZ, we must average over the possible values of the unobserved Z, weightingeach according to its probability. In other words we take the expected valueE[lnP(Ylh')] over the probability distribution governing the random variable Y. 
The distribution governing Yis determined by the completely known values forX, plus the distribution governing Z. 
What is the probability distribution governing Y? In general we will notknow this distribution because itis determined by the parameters 0 that we aretrying to estimate. Therefore, the EM algorithm uses its current hypothesis h inplace of the actual parameters 8 to estimate the distribution governing Y. Let usdefine a function Q(hllh) that gives E[lnP(Ylh')] asa function ofh', under theassumption that 8 = h and given the observed portion Xof the full data Y. 
We write this function Qin the form Q(hllh) to indicate that itis defined in partby the assumption that the current hypothesis his equal to 8. In its general form, 
the EM algorithm repeats the following two steps until convergence: 
CHAPTER 6 BAYESIAN LEARNING 195Step 1: Estimation (E) step: Calculate Q(hllh) using the current hypothesis h andthe observed data Xto estimate the probability distribution over Y. 
Q(hf(h) tE[lnP(Ylhl)lh, XIStep 2: Maximization (M) step: Replace hypothesis hby the hypothesis h' thatmaximizes this Q function. 
ht argmax Q (hf 1 h) 
h' 
When the function Qis continuous, the EM algorithm converges toa sta- 
tionary point of the likelihood function P(Y(hl). When this likelihood functionhas a single maximum, EM will converge to this global maximum likelihood es- 
timate for h'. Otherwise, itis guaranteed only to converge toa local maximum. 
In this respect, EM shares some of the same limitations as other optimizationmethods such as gradient descent, line search, and conjugate gradient discussedin Chapter 4. 
11 6.12.3 Derivation of the k Means AlgorithmTo illustrate the general EM algorithm, let us use itto derive the algorithm given inSection 6.12.1 for estimating the means ofa mixture ofk Normal distributions. Asdiscussed above, the k-means problem isto estimate the parameters 0 = (PI. . . pk) 
that define the means of the k Normal distributions. We are given the observeddata X = {(xi)}. The hidden variables Z = {(zil, . . . , zik)} in this case indicatewhich of the k Normal distributions was used to generate xi. 
To apply EMwe must derive an expression for Q(h(hf) that applies toour k-means problem. First, let us derive an expression for 1np(Y(h1). Note theprobability p(yi (h') ofa single instance yi = (xi, Zil, . . . ~ik) of the full data canbe writtenTo verify this note that only one of the zij can have the value 1, and all others mustbe 0. Therefore, this expression gives the probability distribution for xi generatedby the selected Normal distribution. Given this probability for a single instancep(yi(hl), the logarithm of the probability InP(Y(hl) for all m instances in thedata ism 
lnP(Ylhf) = lnnp(,lhl) 
i=lFinally we must take the expected value of this InP(Ylhl) over the probabilitydistribution governing Yor, equivalently, over the distribution governing the un- 
observed components zij ofY. Note the above expression for InP(Ylhl) isa linearfunction of these zij. In general, for any function f (z) that isa linear function ofz, the following equality holdsE[f (z)l = f (Ek.1) 
This general fact about linear functions allows usto writeTo summarize, the function Q(hllh) for the k means problem iswhere h' = (pi, . . . ,pi) and where E[zij] is calculated based on the currenthypothesis h and observed data X. As discussed earliere-&(x'-~)2E[zij] = 
- --+ --P")~ 
(6.29) 
EL1 e 2Thus, the first (estimation) step of the EM algorithm defines the Q functionbased on the estimated E[zij] terms. The second (maximization) step then findsthe values pi, . . . , pi that maximize this Q function. In the current case1 1 argmax Q(hllh) = argmax - - - 
h' 
CE[zijI(xi - 
h1 i=l &2 2u2 j=lThus, the maximum likelihood hypothesis here minimizes a weighted sum ofsquared errors, where the contribution of each instance xito the error that definespj is weighted byE[zij]. The quantity given by Equation (6.30) is minimized bysetting each pito the weighted sample meanNote that Equations (6.29) and (6.31) define the two steps in the k-meansalgorithm described in Section 6.12.1. 
CHAPTER 6 BAYESIAN LEARNING 1976.13 SUMMARY AND FURTHER READINGThe main points of this chapter include: 
0 Bayesian methods provide the basis for probabilistic learning methods thataccommodate (and require) knowledge about the prior probabilities of alter- 
native hypotheses and about the probability of observing various data giventhe hypothesis. Bayesian methods allow assigning a posterior probability toeach candidate hypothesis, based on these assumed priors and the observeddata. 
0 Bayesian methods can be used to determine the most probable hypothesisgiven the data-the maximum a posteriori (MAP) hypothesis. This is theoptimal hypothesis in the sense that no other hypothesis is more likely. 
0 The Bayes optimal classifier combines the predictions of all alternative hy- 
potheses, weighted by their posterior probabilities, to calculate the mostprobable classification of each new instance. 
i 0 The naive Bayes classifier isa Bayesian learning method that has been foundto be useful in many practical applications. Itis called "naive" because itin- 
corporates the simplifying assumption that attribute values are conditionallyindependent, given the classification of the instance. When this assumptionis met, the naive Bayes classifier outputs the MAP classification. Even whenthis assumption is not met, asin the case of learning to classify text, thenaive Bayes classifier is often quite effective. Bayesian belief networks pro- 
vide a more expressive representation for sets of conditional independenceassumptions among subsets of the attributes. 
0 The framework of Bayesian reasoning can provide a useful basis for ana- 
lyzing certain learning methods that do not directly apply Bayes theorem. 
For example, under certain conditions it can be shown that minimizing thesquared error when learning a real-valued target function corresponds tocomputing the maximum likelihood hypothesis. 
0 The Minimum Description Length principle recommends choosing the hy- 
pothesis that minimizes the description length of the hypothesis plus thedescription length of the data given the hypothesis. Bayes theorem and ba- 
sic results from information theory can be used to provide a rationale forthis principle. 
0 In many practical learning tasks, some of the relevant instance variablesmay be unobservable. The EM algorithm provides a quite general approachto learning in the presence of unobservable variables. This algorithm be- 
gins with an arbitrary initial hypothesis. It then repeatedly calculates theexpected values of the hidden variables (assuming the current hypothesisis correct), and then recalculates the maximum likelihood hypothesis (as- 
suming the hidden variables have the expected values calculated by the firststep). This procedure converges toa local maximum likelihood hypothesis, 
along with estimated values for the hidden variables. 
There are many good introductory texts on probability and statistics, suchas Casella and Berger (1990). Several quick-reference books (e.g., Maisel 1971; 
Speigel 1991) also provide excellent treatments of the basic notions of probabilityand statistics relevant to machine learning. 
Many of the basic notions of Bayesian classifiers and least-squared errorclassifiers are discussed by Duda and Hart (1973). Domingos and Pazzani (1996) 
provide an analysis of conditions under which naive Bayes will output optimalclassifications, even when its independence assumption is violated (the key hereis that there are conditions under which it will output optimal classifications evenwhen the associated posterior probability estimates are incorrect). 
Cestnik (1990) provides a discussion of using the m-estimate to estimateprobabilities. 
Experimental results comparing various Bayesian approaches to decision treelearning and other algorithms can be found in Michie etal. (1994). Chauvin andRumelhart (1995) provide a Bayesian analysis of neural network learning basedon the BACKPROPAGATION algorithm. 
A discussion of the Minimum Description Length principle can be found inRissanen (1983, 1989). Quinlan and Rivest (1989) describe its use in avoidingoverfitting in decision trees. 
EXERCISES6.1. Consider again the example application of Bayes rule in Section 6.2.1. Suppose thedoctor decides to order a second laboratory test for the same patient, and supposethe second test returns a positive result as well. What are the posterior probabilitiesof cancer and -cancer following these two tests? Assume that the two tests areindependent. 
6.2. In the example of Section 6.2.1 we computed the posterior probability of cancer bynormalizing the quantities P (+(cancer) . P (cancer) and P (+I-cancer) . P (-cancer) 
so that they summed to one, Use Bayes theorem and the theorem of total probability 
(see Table 6.1) to prove that this method is valid (i.e., that normalizing in this wayyields the correct value for P(cancerl+)). 
6.3. Consider the concept learning algorithm FindG, which outputs a maximally generalconsistent hypothesis (e.g., some maximally general member of the version space). 
(a) Give a distribution for P(h) and P(D1h) under which FindG is guaranteed tooutput a MAP hypothesis. 
(6) Give a distribution for P(h) and P(D1h) under which FindG is not guaranteedto output a MAP .hypothesis. 
(c) Give a distribution for P(h) and P(D1h) under which FindG is guaranteed tooutput aML hypothesis but not a MAP hypothesis. 
6.4. In the analysis of concept learning in Section 6.3 we assumed that the sequence ofinstances (xl . . . x,) was held fixed. Therefore, in deriving an expression for P(D(h) 
we needed only consider the probability of observing the sequence of target values 
(dl.. .dm) for this fixed instance sequence. Consider the more general setting inwhich the instances are not held fixed, but are drawn independently from someprobability distribution defined over the instance space X. The data D must nowbe described as the set of ordered pairs {(xi, di)}, and P(D1h) must now reflect theCHAPTER 6 BAYESIAN LEARNING 199probability of encountering the specific instance XI, as well as the probability ofthe observed target value di. Show that Equation (6.5) holds even under this moregeneral setting. Hint: Consider the analysis of Section 6.5. 
6.5. Consider the Minimum Description Length principle applied to the hypothesis spaceH consisting of conjunctions ofup ton boolean attributes (e.g., Sunny A Warm). 
Assume each hypothesis is encoded simply by listing the attributes present in thehypothesis, where the number of bits needed to encode any one of the n boolean at- 
tributes is log, n. Suppose the encoding ofan example given the hypothesis uses zerobits if the example is consistent with the hypothesis and uses log, m bits otherwise 
(to indicate which of the m examples was misclassified-the correct classificationcan be inferred tobe the opposite of that predicted by the hypothesis). 
(a) Write down the expression for the quantity tobe minimized according to theMinimum Description Length principle. 
(b) Isit possible to construct a set of training data such that a consistent hypothesisexists, but MDL chooses a less consistent hypothesis? Ifso, give such a trainingset. If not, explain why not. 
(c) Give probability distributions for P(h) and P(D1h) such that the above MDLalgorithm outputs MAP hypotheses. 
6.6. Draw the Bayesian belief network that represents the conditional independence as- 
sumptions of the naive Bayes classifier for the PlayTennis problem of Section 6.9.1. 
Give the conditional probability table associated with the node Wind. 
REFERENCESBuntine W. L. (1994). Operations for learning with graphical models. Journal of Art$cial IntelligenceResearch, 2, 159-225. http://www.cs.washington.edu/research/jair/hom.html. 
Casella, G., & Berger, R. L. (1990). Statistical inference. Pacific Grove, CA: Wadsworth & 
Brooks/Cole. 
Cestnik, B. (1990). Estimating probabilities: A crucial task in machine learning. Proceedings of theNinth European Conference onAm&5al Intelligence (pp. 147-149). London: Pitman. 
Chauvin, Y., & Rumelhart, D. (1995). Backpropagation: Theory, architectures, and applications, 
(edited collection). Hillsdale, NJ: Lawrence Erlbaum Assoc. 
Cheeseman, P., Kelly, J., Self, M., Stutz, J., Taylor, W., & Freeman, D. (1988). AUTOCLASS: Abayesian classification system. Proceedings of AAAI I988 (pp. 607-611). 
Cooper, G. (1990). Computational complexity of probabilistic inference using Bayesian belief net- 
works (research note). Art@cial Intelligence, 42, 393-405. 
Cooper, G., & Herskovits, E. (1992). A Bayesian method for the induction of probabilistic networksfrom data. Machine Learning, 9, 309-347. 
Dagum, P., & Luby, M. (1993). Approximating probabilistic reasoning in Bayesian belief networksis NP-hard. Art$cial Intelligence, 60(1), 141-153. 
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from incomplete datavia the EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1), 1-38. 
Domingos, P., & Pazzani, M. (1996). Beyond independence: Conditions for the optimality of the sim- 
ple Bayesian classifier. Proceedings of the 13th International Conference on Machine Learning 
@p. 105-112). 
Duda, R. O., & Hart, P. E. (1973). Pattern class$cation and scene analysis. New York: John Wiley 
& Sons. 
Hearst, M., & Hirsh, H. (Eds.) (1996). Papers from the AAAI Spring Symposium on MachineLearning in Information Access, Stanford, March 25-27. http://www.parc.xerox.com/ist~ 
projects/mlia/ 
200 MACHINE LEARNINGHeckerman, D., Geiger, D., & Chickering, D. (1995) Learning Bayesian networks: The combinationof knowledge and statistical data. Machine Learning, 20, 197. Kluwer Academic Publishers. 
Jensen, F. V. (1996). An introduction to Bayesian networks. New York: Springer Verlag. 
Joachims, T. (1996). A probabilistic analysis of the Rocchio algorithm with TFIDF for text catego- 
rization, (Computer Science Technical Report CMU-CS-96-118). Carnegie Mellon University. 
Lang, K. (1995). Newsweeder: Learning to filter netnews. In Prieditis and Russell (Eds.), Proceedingsof the 12th International Conference on Machine Learning (pp. 331-339). San Francisco: 
Morgan Kaufmann Publishers. 
Lewis, D. (1991). Representation and learning in information retrieval, (Ph.D. thesis), (COINS Tech- 
nical Report 91-93). Dept. of Computer and Information Science, University of Massachusetts. 
Madigan, D., & Rafferty, A. (1994). ~odel selection and accounting for model uncertainty in graphi- 
cal models using Occam's window. Journal of the American Statistical Association, 89, 1535- 
1546. 
Maisel, L. (1971). Probability, statistics, and random processes. Simon and Schuster Tech Outlines. 
New York: Simon and Schuster. 
Mehta, M., Rissanen, J., & Agrawal, R. (1995). MDL-based decision tree pruning. InU. M. Fayyardand R. Uthurusamy (Eds.), Proceedings of the First International Conference on KnowledgeDiscovery and Data Mining. Menlo Park, CA: AAAI Press. 
Michie, D., Spiegelhalter, D. J., & Taylor, C. C. (1994). Machine learning, neural and statisticalclassification, (edited collection). New York: Ellis Horwood. 
Opper, M., & Haussler, D. (1991). Generalization performance of Bayes optimal prediction algorithmfor learning a perceptron. Physical Review Letters, 66, 2677-2681. 
Pearl, J. (1988). Probabilistic reasoning in intelligent systems: Networks of plausible inference. SanMateo, CA: Morgan-Kaufmann. 
Pradham, M., & Dagum, P. (1996). Optimal Monte Carlo estimation of belief network inference. InProceedings of the Conference on Uncertainty in Artijicial Intelligence (pp. 44-53). 
Quinlan, J. R., & Rivest, R. (1989). Inferring decision trees using the minimum description lengthprinciple. Information and Computation, 80, 227-248. 
Rabiner, L. R. (1989). A tutorial on hidden Markov models and selected applications in speechrecognition. Proceedings of the IEEE, 77(2), 257-286. 
Rissanen, J. (1983). A universal prior for integers and estimation by minimum description length. 
The Annals of Statistics, 11(2), 41-31. 
Rissanen, J., (1989). Stochastic complexity in statistical inquiry. New Jersey: World Scientific Pub. 
Rissanen, J. (1991). Information theory and neural nets. IBM Research Report RJ 8438 (76446), 
IBM Thomas J. Watson Research Center, Yorktown Heights, NY. 
Rocchio, J. (1971). Relevance feedback in information retrieval. In The SMART retrieval system: 
Experiments in automatic document processing, (Chap. 14, pp. 313-323). Englewood Cliffs, 
NJ: Prentice-Hall. 
Russell, S., & Nomig, P. (1995). Artificial intelligence: A modem approach. Englewood Cliffs, NJ: 
Prentice-Hall. 
Russell, S., Binder, J., Koller, D., & Kanazawa, K. (1995). Local learning in probabilistic networkswith hidden variables. Proceedings of the 14th International Joint Conference on ArtificialIntelligence, Montreal. San Francisco: Morgan Kaufmann. 
Salton, G. (1991). Developments in automatic text retrieval. Science, 253, 974-979. 
Shannon, C. E., & Weaver, W. (1949). The mathematical theory of communication. Urbana: Univer- 
sity of Illinois Press. 
Speigel, M. R. (1991). Theory and problems of probability and statistics. Schaum's Outline Series. 
New York: McGraw Hill. 
Spirtes, P., Glymour, C., & Scheines, R. (1993). Causation, prediction, and search. New York: 
Springer Verlag. http://hss.cmu.edu/htmUdepartments/philosophy~~D.BOO~ook.h~ 
CHAPTERCOMPUTATIONALLEARNINGTHEORYThis chapter presents a theoretical characterization of the difficulty of several typesof machine learning problems and the capabilities of several types of machine learn- 
ing algorithms. This theory seeks to answer questions such as "Under what condi- 
tions is successful learning possible and impossible?" and "Under what conditionsis a particular learning algorithm assured of learning successfully?' Two specificframeworks for analyzing learning algorithms are considered. Within the probablyapproximately correct (PAC) framework, we identify classes of hypotheses that canand cannot be learned from a polynomial number of training examples and wede- 
fine a natural measure of complexity for hypothesis spaces that allows boundingthe number of training examples required for inductive learning. Within the mistakebound framework, we examine the number of training errors that will be made bya learner before it determines the correct hypothesis. 
7.1 INTRODUCTIONWhen studying machine learning itis natural to wonder what general laws maygovern machine (and nonmachine) learners. Isit possible to identify classes oflearning problems that are inherently difficult or easy, independent of the learningalgorithm? Can one characterize the number of training examples necessary orsufficient to assure successful learning? How is this number affected if the learneris allowed to pose queries to the trainer, versus observing a random sample oftraining examples? Can one characterize the number of mistakes that a learner202 MACHINE LEARNINGwill make before learning the target function? Can one characterize the inherentcomputational complexity of classes of learning problems? 
Although general answers to all these questions are not yet known, frag- 
ments ofa computational theory of learning have begun to emerge. This chapterpresents key results from this theory, providing answers to these questions withinparticular problem settings. We focus here on the problem of inductively learningan unknown target function, given only training examples of this target func- 
tion and a space of candidate hypotheses. Within this setting, we will be chieflyconcerned with questions such as how many training examples are sufficient tosuccessfully learn the target function, and how many mistakes will the learnermake before succeeding. Aswe shall see, itis possible to set quantitative boundson these measures, depending on attributes of the learning problem such as: 
0 the size or complexity of the hypothesis space considered by the learner0 the accuracy to which the target concept must be approximated0 the probability that the learner will output a successful hypothesis0 the manner in which training examples are presented to the learnerFor the most part, we will focus not on individual learning algorithms, butrather on broad classes of learning algorithms characterized by the hypothesisspaces they consider, the presentation of training examples, etc. Our goal is toanswer questions such as: 
0 Sample complexity. How many training examples are needed for a learnerto converge (with high probability) toa successful hypothesis? 
0 Computational complexity. How much computational effort is needed for alearner to converge (with high probability) toa successful hypothesis? 
0 Mistake bound. How many training examples will the learner misclassifybefore converging toa successful hypothesis? 
Note there are many specific settings in which we could pursue such ques- 
tions. For example, there are various ways to specify what it means for the learnerto be "successful." We might specify that to succeed, the learner must output ahypothesis identical to the target concept. Alternatively, we might simply requirethat it output a hypothesis that agrees with the target concept most of the time, orthat it usually output such a hypothesis. Similarly, we must specify how trainingexamples are tobe obtained by the learner. We might specify that training ex- 
amples are presented bya helpful teacher, or obtained by the learner performingexperiments, or simply generated at random according to some process outsidethe learner's control. Aswe might expect, the answers to the above questionsdepend on the particular setting, or learning model, we have in mind. 
The remainder of this chapter is organized as follows. Section 7.2 introducesthe probably approximately correct (PAC) learning setting. Section 7.3 then an- 
alyzes the sample complexity and computational complexity for several learningCHAPTER 7 COMPUTATIONAL LEARNING THEORY 203problems within this PAC setting. Section 7.4 introduces an important measureof hypothesis space complexity called the VC-dimension and extends our PACanalysis to problems in which the hypothesis space is infinite. Section 7.5 intro- 
duces the mistake-bound model and provides a bound on the number of mistakesmade by several learning algorithms discussed in earlier chapters. Finally, wein- 
troduce the WEIGHTED-MAJORITY algorithm, a practical algorithm for combiningthe predictions of multiple competing learning algorithms, along with a theoreticalmistake bound for this algorithm. 
7.2 PROBABLY LEARNING AN APPROXIMATELY CORRECTHYPOTHESISIn this section we consider a particular setting for the learning problem, called theprobably approximately correct (PAC) learning model. We begin by specifyingthe problem setting that defines the PAC learning model, then consider the ques- 
tions of how many training examples and how much computation are requiredin order to learn various classes of target functions within this PAC model. Fori the sake of simplicity, we restrict the discussion to the case of learning boolean- 
valued concepts from noise-free training data. However, many of the results canbe extended to the more general scenario of learning real-valued target functions 
(see, for example, Natarajan 1991), and some can be extended to learning fromcertain types of noisy data (see, for example, Laird 1988; Kearns and Vazirani1994). 
7.2.1 The Problem SettingAs in earlier chapters, let X refer to the set of all possible instances over whichtarget functions may be defined. For example, X might represent the set of allpeople, each described by the attributes age (e.g., young or old) and height (shortor tall). Let C refer to some set of target concepts that our learner might be calledupon to learn. Each target concept cin C corresponds to some subset ofX, orequivalently to some boolean-valued function c : X + {0, 1). For example, onetarget concept cin C might be the concept "people who are skiers." Ifx is apositive example ofc, then we will write c(x) = 1; ifx isa negative example, 
c(x) = 0. 
We assume instances are generated at random from X according to someprobability distribution D. For example, 2) might be the distribution of instancesgenerated by observing people who walk out of the largest sports store in Switzer- 
land. In general, D may be any distribution, and it will not generally be knownto the learner. All that we require ofD is that itbe stationary; that is, that thedistribution not change over time. Training examples are generated by drawingan instance xat random according toD, then presenting x along with its targetvalue, c(x), to the learner. 
The learner L considers some set Hof possible hypotheses when attemptingto learn the target concept. For example, H might be the set of all hypothesesdescribable by conjunctions of the attributes age and height. After observinga sequence of training examples of the target concept c, L must output somehypothesis h from H, which is its estimate ofc. Tobe fair, we evaluate thesuccess ofL by the performance ofh over new instances drawn randomly fromX according toD, the same probability distribution used to generate the trainingdata. 
Within this setting, we are interested in characterizing the performance ofvarious learners L using various hypothesis spaces H, when learning individualtarget concepts drawn from various classes C. Because we demand that L begeneral enough to learn any target concept from C regardless of the distributionof training examples, we will often be interested in worst-case analyses over allpossible target concepts from C and all possible instance distributions D. 
7.2.2 Error ofa HypothesisBecause we are interested in how closely the learner's output hypothesis hap- 
proximates the actual target concept c, let us begin by defining the true errorof a hypothesis h with respect to target concept c and instance distribution D. 
Informally, the true error ofh is just the error rate we expect when applying hto future instances drawn according to the probability distribution 27. In fact, wealready defined the true error ofh in Chapter 5. For convenience, we restate thedefinition here using cto represent the boolean target function. 
Definition: The true error (denoted errorv(h)) of hypothesis h with respect to targetconcept c and distribution Dis the probability that h will misclassify an instancedrawn at random according toD. 
Here the notation Pr indicates that the probability is taken over the instancex€ Ddistribution V. 
Figure 7.1 shows this definition of error in graphical form. The concepts cand h are depicted by the sets of instances within X that they label as positive. Theerror ofh with respect toc is the probability that a randomly drawn instance willfall into the region where h and c disagree (i.e., their set difference). Note we havechosen to define error over the entire distribution of instances-not simply overthe training examples-because this is the true error we expect to encounter whenactually using the learned hypothesis hon subsequent instances drawn from D. 
Note that error depends strongly on the unknown probability distribution2). For example, ifD isa uniform probability distribution that assigns the sameprobability to every instance inX, then the error for the hypothesis in Figure 7.1will be the fraction of the total instance space that falls into the region where hand c disagree. However, the same h and c will have a much higher error if Dhappens to assign very high probability to instances for which h and c disagree. 
In the extreme, ifV happens to assign zero probability to the instances for whichInstance space XC 
Where cand h disagreeFIGURE 7.1The error of hypothesis h with respect to target concept c. The error ofh with respect toc is theprobability that a randomly drawn instance will fall into the region where h and c disagree on itsclassification. The + and - points indicate positive and negative training examples. Note h has anonzero error with respect toc despite the fact that h and c agree on all five training examplesobserved thus far. 
h(x) = ~(x), then the error for the hin Figure 7.1 will be 1, despite the fact theh and c agree ona very large number of (zero probability) instances. 
Finally, note that the error ofh with respect toc is not directly observable tothe learner. L can only observe the performance ofh over the training examples, 
and it must choose its output hypothesis on this basis only. We will use the termtraining error to refer to the fraction of training examples misclassified byh, incontrast to the true error defined above. Much of our analysis of the complexity oflearning centers around the question "how probable isit that the observed trainingerror for h gives a misleading estimate of the true errorv(h)?" 
Notice the close relationship between this question and the questions con- 
sidered in Chapter 5. Recall that in Chapter 5 we defined the sample error of hwith respect toa set Sof examples tobe the fraction ofS rnisclassified byh. Thetraining error defined above is just the sample error when Sis the set of trainingexamples. In Chapter 5 we determined the probability that the sample error willprovide a misleading estimate of the true error, under the assumption that the datasample Sis drawn independent ofh. However, when Sis the set of training data, 
the learned hypothesis h depends very much onS! Therefore, in this chapter weprovide an analysis that addresses this important special case. 
7.2.3 PAC LearnabilityOur aim isto characterize classes of target concepts that can be reliably learnedfrom a reasonable number of randomly drawn training examples and a reasonableamount of computation. 
What kinds of statements about learnability should we guess hold true? 
We might try to characterize the number of training examples needed to learna hypothesis h for which errorD(h) = 0. Unfortunately, it turns out this isfu- 
tile in the setting we are considering, for two reasons. First, unless we providetraining examples corresponding to every possible instance inX (an unrealisticassumption), there may be multiple hypotheses consistent with the provided train- 
ing examples, and the learner cannot be certain to pick the one correspondingto the target concept. Second, given that the training examples are drawn ran- 
domly, there will always be some nonzero probability that the training examplesencountered by the learner will be misleading. (For example, although we mightfrequently see skiers of different heights, on any given day there is some smallchance that all observed training examples will happen tobe 2 meters tall.) 
To accommodate these two difficulties, we weaken our demands on thelearner in two ways. First, we will not require that the learner output a zero errorhypothesis-we will require only that its error be bounded by some constant, c, 
that can be made arbitrarily small. Second, we will not require that the learnersucceed for every sequence of randomly drawn training examples-we will requireonly that its probability of failure be bounded by some constant, 6, that can bemade arbitrarily small. In short, we require only that the learner probably learn ahypothesis that is approximately correct-hence the term probably approximatelycorrect learning, or PAC learning for short. 
Consider some class Cof possible target concepts and a learner L usinghypothesis space H. Loosely speaking, we will say that the concept class Cis PAC-learnable byL using Hif, for any target concept cin C, L will withprobability (1 - 6) output a hypothesis h with errorv(h) < c, after observing areasonable number of training examples and performing a reasonable amount ofcomputation. More precisely, 
Definition: Consider a concept class C defined over a set of instances Xof lengthn and a learner L using hypothesis space H. Cis PAC-learnable byL using Hif for all cE C, distributions D over X, E such that 0 < 6 < 112, and 6 such that0 < 6 < 112, learner L will with probability at least (1 - 6) output a hypothesish EH such that errorv(h) 5 E, in time that is polynomial in 116, 116, n, andsize(c). 
Our definition requires two things from L. First, L must, with arbitrarily highprobability (1 - 6), output a hypothesis having arbitrarily low error (6). Second, itmust doso efficiently-in time that grows at most polynomially with 1/c and 116, 
which define the strength of our demands on the output hypothesis, and with n andsize(c) that define the inherent complexity of the underlying instance space X andconcept class C. Here, nis the size of instances inX. For example, if instances inX are conjunctions ofk boolean features, then n = k. The second space parameter, 
size(c), is the encoding length ofc inC, assuming some representation for C. 
For example, if concepts inC are conjunctions ofup tok boolean features, eachdescribed by listing the indices of the features in the conjunction, then size(c) isthe number of boolean features actually used to describe c. 
Our definition of PAC learning may at first appear tobe concerned onlywith the computational resources required for learning, whereas in practice we areusually more concerned with the number of training examples required. However, 
the two are very closely related: IfL requires some minimum processing timeper training example, then for Cto be PAC-learnable byL, L must learn from apolynomial number of training examples. In fact, a typical approach to showingthat some class Cof target concepts is PAC-learnable, isto first show that eachtarget concept inC can be learned from a polynomial number of training examplesand then show that the processing time per example is also polynomially bounded. 
Before moving on, we should point out a restrictive assumption implicitin our definition of PAC-learnable. This definition implicitly assumes that thelearner's hypothesis space H contains a hypothesis with arbitrarily small error forevery target concept inC. This follows from the requirement in the above defini- 
tion that the learner succeed when the error bound 6 is arbitrarily close to zero. Ofcourse this is difficult to assure if one does not know Cin advance (what isC fora program that must learn to recognize faces from images?), unless His taken tobe the power set ofX. As pointed out in Chapter 2, such an unbiased H will notsupport accurate generalization from a reasonable number of training examples. 
Nevertheless, the results based on the PAC learning model provide useful insights 
/ regarding the relative complexity of different learning problems and regarding therate at which generalization accuracy improves with additional training examples. 
Furthermore, in Section 7.3.1 we will lift this restrictive assumption, to considerthe case in which the learner makes no prior assumption about the form of thetarget concept. 
7.3 SAMPLE COMPLEXITY FOR FINITE HYPOTHESIS SPACESAs noted above, PAC-learnability is largely determined by the number of trainingexamples required by the learner. The growth in the number of required trainingexamples with problem size, called the sample complexity of the learning problem, 
is the characteristic that is usually of greatest interest. The reason is that in mostpractical settings the factor that most limits success of the learner is the limitedavailability of training data. 
Here we present a general bound on the sample complexity for a very broadclass of learners, called consistent learners. A learner is consistent ifit outputshypotheses that perfectly fit the training data, whenever possible. Itis quite rea- 
sonable to ask that a learning algorithm be consistent, given that we typicallyprefer a hypothesis that fits the training data over one that does not. Note thatmany of the learning algorithms discussed in earlier chapters, including all thelearning algorithms described in Chapter 2, are consistent learners. 
Can we derive a bound on the number of training examples required byany consistent learner, independent of the specific algorithm it uses to derive aconsistent hypothesis? The answer is yes. To accomplish this, itis useful to recallthe definition of version space from Chapter 2. There we defined the version space, 
VSH,D, tobe the set of all hypotheses hE H that correctly classify the trainingexamples D. 
vs,~ = {hE HI(V(x, 4~)) ED) (h(x) = ~(x))} 
The significance of the version space here is that every consistent learner outputsa hypothesis belonging to the version space, regardless of the instance space X, 
hypothesis space H, or training data D. The reason is simply that by definitionthe version space VSH,D contains every consistent hypothesis inH. Therefore, 
to bound the number of examples needed by any consistent learner, we need onlybound the number of examples needed to assure that the version space contains nounacceptable hypotheses. The following definition, after Haussler (1988), statesthis condition precisely. 
Definition: Consider a hypothesis space H, target concept c, instance distributionV, and set of training examples Dof c. The version space VS,, is said tobe 
€- ex ha us te d with respect toc and V, if every hypothesis hin VSH,* has error lessthan 6 with respect toc and V. 
This definition is illustrated in Figure 7.2. The version space is € -exhaustedjust in the case that all the hypotheses consistent with the observed training ex- 
amples (i.e., those with zero training error) happen to have true error less thanE. Of course from the learner's viewpoint all that can be known is that thesehypotheses fit the training data equally well-they all have zero training error. 
Only an observer who knew the identity of the target concept could determinewith certainty whether the version space is +exhausted. Surprisingly, a proba- 
bilistic argument allows usto bound the probability that the version space willbe € -exhausted after a given number of training examples, even without knowingthe identity of the target concept or the distribution from which training examplesHypothesis space Hm error =.3r =.4FIGURE 7.2Exhausting the version space. The version space VSH,Dis the subset of hypotheses hE H, whichhave zero training error (denoted byr = 0 in the figure). Of course the true errorv(h) (denoted byerror in the figure) may be nonzero, even for hypotheses that commit zero errors over the trainingdata. The version space is said tobe €- ex ha us te d when all hypotheses h remaining in VSH,~ haveerrorw(h) < E. 
are drawn. Haussler (1988) provides such a bound, in the form of the followingtheorem. 
Theorem 7.1. € -exhausting the version space. If the hypothesis space His finite, 
and Dis a sequence ofrn 1 independent randomly drawn examples of some targetconcept c, then for any 0 5 E 5 1, the probability that the version space VSH,~ isnot €- ex ha us te d (with respect toc) is less than or equal toProof. Let hl, h2, . . . hkbe all the hypotheses inH that have true error greater than Ewith respect toc. We fail to €- ex ha us t the version space if and only ifat least one ofthese k hypotheses happens tobe consistent with all rn independent random trainingexamples. The probability that any single hypothesis having true error greater than Ewould be consistent with one randomly drawn example isat most (1 - E). Thereforethe probability that this hypothesis will be consistent with rn independently drawnexamples isat most (1 - E)~. Given that we have k hypotheses with error greaterthan E, the probability that at least one of these will be consistent with all rn trainingexamples isat mostAnd since k 5 IH 1, this isat most 1 HI(1- 6)". Finally, we use a general inequalitystating that if 0 5 E 5 1 then (1 - E) 5 e-'. Thus, 
which proves the theorem. .OWe have just proved an upper bound on the probability that the version spaceis not € -exhausted, based on the number of training examples m, the allowed errorE, and the size ofH. Put another way, this bounds the probability that m trainingexamples will fail to eliminate all "bad" hypotheses (i.e., hypotheses with trueerror greater than E), for any consistent learner using hypothesis space H. 
Let us use this result to determine the number of training examples requiredto reduce this probability of failure below some desired level 6. 
Rearranging terms to solve for m, we find1 
m 2 - (ln 1 HI + ln(l/6)) 
E 
(7.2) 
To summarize, the inequality shown in Equation (7.2) provides a generalbound on the number of training examples sufficient for any consistent learnerto successfully learn any target concept inH, for any desired values of 6 andE. This number rnof training examples is sufficient to assure that any consistenthypothesis will be probably (with probability (1 - 6)) approximately (within errorE) correct. Notice m grows linearly in 1/~ and logarithmically in 116. It also growslogarithmically in the size of the hypothesis space H. 
210 MACHINE LEARNINGNote that the above bound can bea substantial overestimate. For example, 
although the probability of failing to exhaust the version space must lie in theinterval [O, 11, the bound given by the theorem grows linearly with IHI. Forsufficiently large hypothesis spaces, this bound can easily be greater than one. 
Asa result, the bound given by the inequality in Equation (7.2) can substantiallyoverestimate the number of training examples required. The weakness of thisbound is mainly due to the IHI term, which arises in the proof when summingthe probability that a single hypothesis could be unacceptable, over all possiblehypotheses. In fact, a much tighter bound is possible in many cases, as well as abound that covers infinitely large hypothesis spaces. This will be the subject ofSection 7.4. 
7.3.1 Agnostic Learning and Inconsistent HypothesesEquation (7.2) is important because it tells us how many training examples sufficeto ensure (with probability (1 - 6)) that every hypothesis inH having zero trainingerror will have a true error ofat most E. Unfortunately, ifH does not containthe target concept c, then a zero-error hypothesis cannot always be found. In thiscase, the most we might ask of our learner isto output the hypothesis from Hthat has the minimum error over the training examples. A learner that makes noassumption that the target concept is representable byH and that simply findsthe hypothesis with minimum training error, is often called an agnostic learner, 
because it makes no prior commitment about whether or not Cg H. 
Although Equation (7.2) is based on the assumption that the learner outputsa zero-error hypothesis, a similar bound can be found for this more general casein which the learner entertains hypotheses with nonzero training error. To statethis precisely, let D denote the particular set of training examples available tothe learner, in contrast toD, which denotes the probability distribution over theentire set of instances. Let errorD(h) denote the training error of hypothesis h. 
In particular, error~(h) is defined as the fraction of the training examples inD that are misclassified byh. Note the errorD(h) over the particular sample oftraining data D may differ from the true error errorv(h) over the entire probabilitydistribution 2). Now let hb,,, denote the hypothesis from H having lowest trainingerror over the training examples. How many training examples suffice to ensure 
(with high probability) that its true error errorD(hb,,,) will beno more thanE + errorg (hbest)? Notice the question considered in the previous section is just aspecial case of this question, when errorD(hb,,) happens tobe zero. 
This question can be answered (see Exercise 7.3) using an argument analo- 
gous to the proof of Theorem 7.1. Itis useful here to invoke the general Hoeffdingbounds (sometimes called the additive Chernoff bounds). The Hoeffding boundscharacterize the deviation between the true probability of some event and its ob- 
served frequency over m independent trials. More precisely, these bounds applyto experiments involving m distinct Bernoulli trials (e.g., m independent flips of acoin with some probability of turning up heads). This is exactly analogous to thesetting we consider when estimating the error ofa hypothesis in Chapter 5: TheCHAPTER 7 COMPUTATIONAL LEARNING THEORY 211probability of the coin being heads corresponds to the probability that the hypothe- 
sis will misclassify a randomly drawn instance. The m independent coin flips corre- 
spond to the m independently drawn instances. The frequency of heads over the mexamples corresponds to the frequency of misclassifications over the m instances. 
The Hoeffding bounds state that if the training error errOrD(h) is measuredover the set D containing m randomly drawn examples, thenThis gives usa bound on the probability that an arbitrarily chosen single hypothesishas a very misleading training error. To assure that the best hypothesis found byL has an error bounded in this way, we must consider the probability that anyone of the 1 H 1 hypotheses could have a large errorPr[(3h EH)(errorv(h) > error~(h) + E)] 5 1 H ~e-~~'~ 
Ifwe call this probability 6, and ask how many examples m suffice to hold S tosome desired value, we now obtainThis is the generalization of Equation (7.2) to the case in which the learner stillpicks the best hypothesis hE H, but where the best hypothesis may have nonzerotraining error. Notice that m depends logarithmically onH and on 116, asit didin the more restrictive case of Equation (7.2). However, in this less restrictivesituation m now grows as the square of 116, rather than linearly with 116. 
7.3.2 Conjunctions of Boolean Literals Are PAC-LearnableNow that we have a bound indicating the number of training examples sufficientto probably approximately learn the target concept, we can use itto determine thesample complexity and PAC-learnability of some specific concept classes. 
Consider the class Cof target concepts described by conjunctions of booleanliterals. A boolean literal is any boolean variable (e.g., Old), or its negation (e.g., 
-Old). Thus, conjunctions of boolean literals include target concepts such as 
"Old A -Tallv. IsC PAC-learnable? We can show that the answer is yes byfirst showing that any consistent learner will require only a polynomial numberof training examples to learn any cin C, and then suggesting a specific algorithmthat uses polynomial time per training example. 
Consider any consistent learner L using a hypothesis space H identical toC. 
We can use Equation (7.2) to compute the number mof random training examplessufficient to ensure that L will, with probability (1 - S), output a hypothesis withmaximum error E. To accomplish this, we need only determine the size IHI ofthe hypothesis space. 
Now consider the hypothesis space H defined by conjunctions of literalsbased onn boolean variables. The size 1HI of this hypothesis space is 3". To seethis, consider the fact that there are only three possibilities for each variable in212 MACHINE LEARNINGany given hypothesis: Include the variable asa literal in the hypothesis, includeits negation asa literal, or ignore it. Given n such variables, there are 3" distincthypotheses. 
Substituting IHI = 3" into Equation (7.2) gives the following bound for thesample complexity of learning conjunctions ofup ton boolean literals. 
For example, ifa consistent learner attempts to learn a target concept describedby conjunctions ofup to 10 boolean literals, and we desire a 95% probabilitythat it will learn a hypothesis with error less than .l, then it suffices to present mrandomly drawn training examples, where rn = -$ (10 1n 3 + ln(11.05)) = 140. 
Notice that m grows linearly in the number of literals n, linearly in 116, andlogarithmically in 116. What about the overall computational effort? That willdepend, of course, on the specific learning algorithm. However, as long as ourlearning algorithm requires no more than polynomial computation per trainingexample, and no more than a polynomial number of training examples, then thetotal computation required will be polynomial as well. 
In the case of learning conjunctions of boolean literals, one algorithm thatmeets this requirement has already been presented in Chapter 2. Itis the FIND-Salgorithm, which incrementally computes the most specific hypothesis consistentwith the training examples. For each new positive training example, this algorithmcomputes the intersection of the literals shared by the current hypothesis and thenew training example, using time linear inn. Therefore, the FIND-S algorithmPAC-learns the concept class of conjunctions ofn boolean literals with negations. 
Theorem 7.2. PAC-learnability of boolean conjunctions. The class Cof con- 
junctions of boolean literals is PAC-learnable by the FIND-S algorithm using H = C. 
Proof. Equation (7.4) shows that the sample complexity for this concept class ispolynomial inn, 116, and 116, and independent of size (c). To incrementally processeach training example, the FIND-S algorithm requires effort linear inn and indepen- 
dent of 116, 116, and size(c). Therefore, this concept class is PAC-learnable by theFIND-S algorithm. 07.3.3 PAC-Learnability of Other Concept ClassesAs we just saw, Equation (7.2) provides a general basis for bounding the samplecomplexity for learning target concepts in some given class C. Above we appliedit to the class of conjunctions of boolean literals. It can also be used to showthat many other concept classes have polynomial sample complexity (e.g., seeExercise 7.2). 
7.3.3.1 UNBIASED LEARNERSNot all concept classes have polynomially bounded sample complexity accordingto the bound of Equation (7.2). For example, consider the unbiased concept classC that contains every teachable concept relative toX. The set Cof all definabletarget concepts corresponds to the power set ofX-the set of all subsets ofX- 
which contains ICI = 2IXI concepts. Suppose that instances inX are defined byn boolean features. In this case, there will be 1x1 = 2" distinct instances, andtherefore ICI = 21'1 = 2' distinct concepts. Of course to learn such an unbiasedconcept class, the learner must itself use an unbiased hypothesis space H = C. 
Substituting IH I = 22n into Equation (7.2) gives the sample complexity for learningthe unbiased concept class relative toX. 
Thus, this unbiased class of target concepts has exponential sample complexityunder the PAC model, according to Equation (7.2). Although Equations (7.2) 
and (7.5) are not tight upper bounds, it can in fact be proven that the samplecomplexity for the unbiased concept class is exponential inn. 
I1I 7.3.3.2 K-TERM DNF AND K-CNF CONCEPTSI1 Itis also possible to find concept classes that have polynomial sample complexity, 
but nevertheless cannot be learned in polynomial time. One interesting example isthe concept class Cof k-term disjunctive normal form (k-term DNF) expressions. 
k-term DNF expressions are of the form TIv T2 v . . - vTk, where each term 1;: 
isa conjunction ofn boolean attributes and their negations. Assuming H = C, itis easy to show that IHI isat most 3"k (because there are k terms, each of whichmay take on 3" possible values). Note 3"kis an overestimate ofH, because it isdouble counting the cases where = I;. and where 1;: is more_general-than I;.. 
Still, we can use this upper bound onI HIto obtain an upper bound on the samplecomplexity, substituting this into Equation (7.2). 
which indicates that the sample complexity ofk-term DNF is polynomial in1/~, 116, n, and k. Despite having polynomial sample complexity, the computa- 
tional complexity is not polynomial, because this learning problem can be shownto be equivalent to other problems that are known tobe unsolvable in polynomialtime (unless RP = NP). Thus, although k-term DNF has polynomial samplecomplexity, it does not have polynomial computational complexity for a learnerusing H = C. 
The surprising fact about k-term DNF is that although itis not PAC- 
learnable, there isa strictly larger concept class that is! This is possible becausethe larger concept class has polynomial computation complexity per example andstill has polynomial sample complexity. This larger class is the class ofk-CNFexpressions: conjunctions of arbitrary length of the form TIA T2 A. . . AI;., whereeach isa disjunction ofup tok boolean attributes. Itis straightforward to showthat k-CNF subsumes k-DNF, because any k-term DNF expression can easily berewritten asa k-CNF expression (but not vice versa). Although k-CNF is moreexpressive than k-term DNF, it has both polynomial sample complexity and poly- 
nomial time complexity. Hence, the concept class k-term DNF is PAC learnableby an efficient algorithm using H = k-CNF. See Kearns and Vazirani (1994) fora more detailed discussion. 
7.4 SAMPLE COMPLEXITY FOR INFINITE HYPOTHESIS SPACESIn the above section we showed that sample complexity for PAC learning growsas the logarithm of the size of the hypothesis space. While Equation (7.2) is quiteuseful, there are two drawbacks to characterizing sample complexity in terms ofIHI. First, it can lead to quite weak bounds (recall that the bound on 6 can besignificantly greater than 1 for large IH I). Second, in the case of infinite hypothesisspaces we cannot apply Equation (7.2) at all! 
Here we consider a second measure of the complexity ofH, called theVapnik-Chervonenkis dimension ofH (VC dimension, orVC(H), for short). Aswe shall see, we can state bounds on sample complexity that use VC(H) ratherthan IHI. In many cases, the sample complexity bounds based onVC(H) willbe tighter than those from Equation (7.2). In addition, these bounds allow us tocharacterize the sample complexity of many infinite hypothesis spaces, and canbe shown tobe fairly tight. 
7.4.1 Shattering a Set of InstancesThe VC dimension measures the complexity of the hypothesis space H, not by thenumber of distinct hypotheses 1 H 1, but instead by the number of distinct instancesfrom X that can be completely discriminated using H. 
To make this notion more precise, we first define the notion of shattering aset of instances. Consider some subset of instances SE X. For example, Figure 7.3shows a subset of three instances from X. Each hypothesis h from H imposes somedichotomy onS; that is, h partitions S into the two subsets {xE Slh(x) = 1) and 
{xE Slh(x) = 0). Given some instance set S, there are 2ISI possible dichotomies, 
though H may be unable to represent some of these. We say that H shatters S ifevery possible dichotomy ofS can be represented by some hypothesis from H. 
Definition: A set of instances Sis shattered by hypothesis space Hif and only iffor every dichotomy.ofS there exists some hypothesis inH consistent with thisdichotomy. 
Figure 7.3 illustrates a set Sof three instances that is shattered by thehypothesis space. Notice that each of the 23 dichotomies of these three instancesis covered by some hypothesis. 
Note that ifa set of instances is not shattered bya hypothesis space, thenthere must be some concept (dichotomy) that can be defined over the instances, 
but that cannot be represented by the hypothesis space. The ability ofH to shatterInstance space XFIGURE 73A set of three instances shattered by eight hypotheses. For every possible dichotomy of the instances, 
there exists a corresponding hypothesis. 
a set .of instances is thus a measure of its capacity to represent target conceptsdefined over these instances. 
7.4.2 The Vapnik-Chervonenkis DimensionThe ability to shatter a set of instances is closely related to the inductive bias ofa hypothesis space. Recall from Chapter 2 that an unbiased hypothesis space isone capable of representing every possible concept (dichotomy) definable over theinstance space X. Put briefly, an unbiased hypothesis space His one that shattersthe instance space X. What ifH cannot shatter X, but can shatter some largesubset Sof X? Intuitively, it seems reasonable to say that the larger the subsetof X that can be shattered, the more expressive H. The VC dimension ofH isprecisely this measure. 
Definition: The Vapnik-Chervonenkis dimension, VC(H), of hypothesis space Hdefined over instance space Xis the size of the largest finite subset ofX shatteredby H. If arbitrarily large finite sets ofX can be shattered byH, then VC(H) = oo. 
Note that for any finite H, VC(H) 5 log2 IHI. To see this, suppose thatVC(H) = d. Then H will require 2d distinct hypotheses to shatter d instances. 
Hence, 2d 5 IHI, andd = VC(H) slog2(H(. 
7.4.2.1 ILLUSTRATIW EXAMPLESIn order to develop an intuitive feeling for VC(H), consider a few example hy- 
pothesis spaces. To get started, suppose the instance space Xis the set of realnumbers X = 8 (e.g., describing the height of people), and H the set of inter- 
vals on the real number line. In other words, His the set of hypotheses of theform a < x < b, where a and b may be any real constants. What isVC(H)? 
To answer this question, we must find the largest subset ofX that can be shat- 
tered byH. Consider a particular subset containing two distinct instances, sayS = {3.1,5.7}. Can Sbe shattered byH? Yes. For example, the four hypotheses 
(1 < x < 2), (1 < x < 4), (4 < x < 7), and (1 < x < 7) will do. Together, theyrepresent each of the four dichotomies over S, covering neither instance, eitherone of the instances, and both of the instances, respectively. Since we have founda set of size two that can be shattered byH, we know the VC dimension of His at least two. Is there a set of size three that can be shattered? Consider a setS = (xo, xl, x2} containing three arbitrary instances. Without loss of generality, 
assume xo < xl < x2. Clearly this set cannot be shattered, because the dichotomythat includes xo and x2, but not XI, cannot be represented bya single closed inter- 
val. Therefore, no subset Sof size three can be shattered, and VC(H) = 2. Notehere that His infinite, but VC(H) finite. 
Next consider the set Xof instances corresponding to points on the x, y plane 
(see Figure 7.4). Let Hbe the set of all linear decision surfaces in the plane. Inother words, His the hypothesis space corresponding toa single perceptron unitwith two inputs (see Chapter 4 for a general discussion of perceptrons). Whatis the VC dimension of this H? Itis easy to see that any two distinct points inthe plane can be shattered byH, because we can find four linear surfaces thatinclude neither, either, or both points. What about sets of three points? As long asthe points are not colinear, we will be able to find 23 linear surfaces that shatterthem. Of course three colinear points cannot be shattered (for the same reason thatthe three points on the real line could not be shattered in the previous example). 
What isVC(H) in this case-two or three? Itis at least three. The definition ofVC dimension indicates that ifwe find any set of instances of size d that canbe shattered, then VC(H) 2 d. To show that VC(H) < d, we must show thatno set of size d can be shattered. In this example, no sets of size four can beshattered, soVC(H) = 3. More generally, it can be shown that the VC dimensionof linear decision surfaces inan r dimensional space (i.e., the VC dimension of aperceptron with r inputs) isr + 1. 
As one final example, suppose each instance inX is described by the con- 
junction of exactly three boolean literals, and suppose that each hypothesis inH isdescribed by the conjunction ofup to three boolean literals. What isVC(H)? WeFIGURE 7.4The VC dimension for linear decision surfaces in the x, y plane is 3. (a) A set of three points thatcan be shattered using linear decision surfaces. (b) A set of three that cannot be shattered. 
can show that itis at least 3, as follows. Represent each instance bya 3-bit stringcorresponding to the values of each of its three literals 11, 12, and 13. Consider thefollowing set of three instances: 
This set of three instances can be shattered byH, because a hypothesiscan be constructed for any desired dichotomy as follows: If the dichotomy is toexclude instancei, add the literal -lito the hypothesis. For example, suppose wewish to include instance2, but exclude instance1 and instance3. Then we use thehypothesis -IlA -I3. This argument easily extends from three features ton. Thus, 
the VC dimension for conjunctions ofn boolean literals isat least n. In fact, it isexactly n, though showing this is more difficult, because it requires demonstratingthat no set ofn + 1 instances can be shattered. 
i 7.4.3 Sample Complexity and the VC DimensionEarlier we considered the question "How many randomly drawn training examplessuffice to probably approximately learn any target concept inC?' (i.e., how manyexamples suffice to € -exhaust the version space with probability (1 - a)?). UsingVC(H) asa measure for the complexity ofH, itis possible to derive an alternativeanswer to this question, analogous to the earlier bound of Equation (7.2). Thisnew bound (see Blumer etal. 1989) isNote that just asin the bound from Equation (7.2), the number of required trainingexamples m grows logarithmically in 118. It now grows log times linear in 116, 
rather than linearly. Significantly, the InI HI term in the earlier bound has nowbeen replaced by the alternative measure of hypothesis space complexity, VC(H) 
(recall VC(H) I log2 IH I). 
Equation (7.7) provides an upper bound on the number of training examplessufficient to probably approximately learn any target concept inC, for any desiredt and a. Itis also possible to obtain a lower bound, as summarized in the followingtheorem (see Ehrenfeucht etal. 1989). 
Theorem 7.3. Lower bound on sample complexity. Consider any concept classC such that VC(C) 2 2, any learner L, and any 0 < E < $, and 0 < S < &. Thenthere exists a distribution 23 and target concept inC such that ifL observes fewerexamples thanthen with probability at least 6, L outputs a hypothesis h having errorD(h) > E. 
This theorem states that if the number of training examples is too few, thenno learner can PAC-learn every target concept in any nontrivial C. Thus, thistheorem provides a lower bound on the number of training examples necessary forsuccessful learning, complementing the earlier upper bound that gives a suficientnumber. Notice this lower bound is determined by the complexity of the conceptclass C, whereas our earlier upper bounds were determined byH. (why?)+ 
This lower bound shows that the upper bound of the inequality in Equa- 
tion (7.7) is fairly tight. Both bounds are logarithmic in 116 and linear inVC(H). 
The only difference in the order of these two bounds is the extra log(l/c) depen- 
dence in the upper bound. 
7.4.4 VC Dimension for Neural NetworksGiven the discussion of artificial neural network learning in Chapter 4, itis in- 
teresting to consider how we might calculate the VC dimension ofa network ofinterconnected units such as the feedforward networks trained by the BACKPROPA- 
GATION procedure. This section presents a general result that allows computing theVC dimension of layered acyclic networks, based on the structure of the networkand the VC dimension of its individual units. This VC dimension can then be usedto bound the number of training examples sufficient to probably approximatelycorrectly learn a feedforward network to desired values ofc and 6. This sectionmay be skipped ona first reading without loss of continuity. 
Consider a network, G, of units, which forms a layered directed acyclicgraph. A directed acyclic graph is one for which the edges have a direction (e.g., 
the units have inputs and outputs), and in which there are no directed cycles. 
A layered graph is one whose nodes can be partitioned into layers such thatall directed edges from nodes at layer 1 goto nodes at layer 1 + 1. The layeredfeedforward neural networks discussed throughout Chapter 4 are examples of suchlayered directed acyclic graphs. 
It turns out that we can bound the VC dimension of such networks based ontheir graph structure and the VC dimension of the primitive units from which theyare constructed. To formalize this, we must first define a few more terms. Let nbe the number of inputs to the network G, and let us assume that there is just oneoutput node. Let each internal unit Niof G (i.e., each node that is not an input) 
have at most r inputs and implement a boolean-valued function ci : 8'' + (0, 1) 
from some function class C. For example, if the internal nodes are perceptrons, 
then C will be the class of linear threshold functions defined over 8'. 
We can now define the G-composition ofC tobe the class of all functionsthat can be implemented by the network G assuming individual units inG takeon functions from the class C. In brief, the G-composition ofC is the hypothesisspace representable by the network G. 
t~int: Ifwe were to substitute H for Cin the lower bound, this would result ina tighter bound onm in the case H > C. 
The following theorem bounds the VC dimension of the G-composition ofC, based on the VC dimension ofC and the structure ofG. 
Theorem 7.4. VC-dimension of directed acyclic layered networks. (See Kearnsand Vazirani 1994.) Let Gbe a layered directed acyclic graph with n input nodesand s 2 2 internal nodes, each having at most r inputs. Let Cbe a concept class over8Y ofVC dimension d, corresponding to the set of functions that can be describedby each of the s internal nodes. Let CGbe the G-composition ofC, correspondingto the set of functions that can be represented byG. Then VC(CG) 5 2dslog(es), 
where eis the base of the natural logarithm. 
Note this bound on the VC dimension of the network G grows linearly withthe VC dimension dof its individual units and log times linear ins, the numberof threshold units in the network. 
Suppose we consider acyclic layered networks whose individual nodes areperceptrons. Recall from Chapter 4 that anr input perceptron uses linear decisionsurfaces to represent boolean functions over %'. As noted in Section 7.4.2.1, theVC dimension of linear decision surfaces over isr + 1. Therefore, a singleperceptron with r inputs has VC dimension r + 1. We can use this fact, togetherwith the above theorem, to bound the VC dimension of acyclic layered networkscontaining s perceptrons, each with r inputs, asWe can now bound the number mof training examples sufficient to learn perceptrons 
(with probability at least (1 - 6)) any target concept from C, to withinerror E. Substituting the above expression for the network VC dimension intoEquation (7.7), we haveAs illustrated by this perceptron network example, the above theorem isinteresting because it provides a general method for bounding the VC dimensionof layered, acyclic networks of units, based on the network structure and the VCdimension of the individual units. Unfortunately the above result does not directlyapply to networks trained using BACKPROPAGATION, for two reasons. First, thisresult applies to networks of perceptrons rather than networks of sigmoid unitsto which the BACKPROPAGATION algorithm applies. Nevertheless, notice that theVC dimension of sigmoid units will beat least as great as that of perceptrons, 
because a sigmoid unit can approximate a perceptron to arbitrary accuracy byusing sufficiently large weights. Therefore, the above bound onm will beat leastas large for acyclic layered networks of sigmoid units. The second shortcomingof the above result is that it fails to account for the fact that BACKPROPAGATION220 MACHINE LEARNINGtrains a network by beginning with near-zero weights, then iteratively modifyingthese weights until an acceptable hypothesis is found. Thus, BACKPROPAGATIONwith a cross-validation stopping criterion exhibits an inductive bias in favor ofnetworks with small weights. This inductive bias, which reduces the effective VCdimension, is not captured by the above analysis. 
7.5 THE MISTAKE BOUND MODEL OF LEARNINGWhile we have focused thus far on the PAC learning model, computational learn- 
ing theory considers a variety of different settings and questions. Different learningsettings that have been studied vary by how the training examples are generated 
(e.g., passive observation of random examples, active querying by the learner), 
noise in the data (e.g., noisy or error-free), the definition of success (e.g., thetarget concept must be learned exactly, or only probably and approximately), as- 
sumptions made by the learner (e.g., regarding the distribution of instances andwhether CG H), and the measure according to which the learner is evaluated 
(e.g., number of training examples, number of mistakes, total time). 
In this section we consider the mistake bound model of learning, in whichthe learner is evaluated by the total number of mistakes it makes before it con- 
verges to the correct hypothesis. Asin the PAC setting, we assume the learnerreceives a sequence of training examples. However, here we demand that uponreceiving each example x, the learner must predict the target value c(x), beforeit is shown the correct target value by the trainer. The question considered is 
"How many mistakes will the learner make in its predictions before it learns thetarget concept?' This question is significant in practical settings where learningmust be done while the system isin actual use, rather than during some off-linetraining stage. For example, if the system isto learn to predict which credit cardpurchases should be approved and which are fraudulent, based on data collectedduring use, then we are interested in minimizing the total number of mistakes itwill make before converging to the correct target function. Here the total num- 
ber of mistakes can be even more important than the total number of trainingexamples. 
This mistake bound learning problem may be studied in various specificsettings. For example, we might count the number of mistakes made before PAClearning the target concept. In the examples below, we consider instead the numberof mistakes made before learning the target concept exactly. Learning the targetconcept exactly means converging toa hypothesis such that (Vx)h(x) = c(x). 
7.5.1 Mistake Bound for the FIND-S AlgorithmTo illustrate, consider again the hypothesis space H consisting of conjunctions ofup ton boolean literals 11 . . .1, and their negations (e.g., Rich A -Handsome). 
Recall the FIND-S algorithm from Chapter 2, which incrementally computes themaximally specific hypothesis consistent with the training examples. A straight- 
forward implementation of FIND-S for the hypothesis space His as follows: 
CHAPTER 7 COMPUTATIONAL LEARNING THEORY 221FIND-S: 
0 Initialize hto the most specific hypothesis l1 A -IIA 12 A -12.. .1, A -1, 
0 For each positive training instance x0 Remove from h any literal that is not satisfied by x0 Output hypothesis h. 
FIND-S converges in the limit toa hypothesis that makes no errors, providedC H and provided the training data is noise-free. FIND-S begins with the mostspecific hypothesis (which classifies every instance a negative example), thenincrementally generalizes this hypothesis as needed to cover observed positivetraining examples. For the hypothesis representation used here, this generalizationstep consists of deleting unsatisfied literals. 
Can we prove a bound on the total number of mistakes that FIND-S will makebefore exactly learning the target concept c? The answer is yes. To see this, notefirst that ifc EH, then FIND-S can never mistakenly classify a negative example aspositive. The reason is that its current hypothesis his always at least as specific as1 the target concept e. Therefore, to calculate the number of mistakes it will make, 
we need only count the number of mistakes it will make misclassifying trulypositive examples as negative. How many such mistakes can occur before FIND-Slearns c exactly? Consider the first positive example encountered by FIND-S. Thelearner will certainly make a mistake classifying this example, because its initialhypothesis labels every instance negative. However, the result will be that halfof the 2n terms in its initial hypothesis will be eliminated, leaving only n terms. 
For each subsequent positive example that is mistakenly classified by the currenthypothesis, at least one more of the remaining n terms must be eliminated fromthe hypothesis. Therefore, the total number of mistakes can beat most n + 1. Thisnumber of mistakes will be required in the worst case, corresponding to learningthe most general possible target concept (Vx)c(x) = 1 and corresponding to aworst case sequence of instances that removes only one literal per mistake. 
7.5.2 Mistake Bound for the HALVING AlgorithmAs a second example, consider an algorithm that learns by maintaining a descrip- 
tion of the version space, incrementally refining the version space as each newtraining example is encountered. The CANDIDATE-ELIMINATION algorithm and theLIST-THEN-ELIMINATE algorithm from Chapter 2 are examples of such algorithms. 
In this section we derive a worst-case bound on the number of mistakes that willbe made by such a learner, for any finite hypothesis space H, assuming again thatthe target concept must be learned exactly. 
To analyze the number of mistakes made while learning we must first specifyprecisely how the learner will make predictions given a new instance x. Let usassume this prediction is made by taking a majority vote among the hypotheses inthe current version space. If the majority of version space hypotheses classify thenew instance as positive, then this prediction is output by the learner. Otherwisea negative prediction is output. 
222 MACHINE LEARNINGThis combination of learning the version space, together with using ama- 
jority vote to make subsequent predictions, is often called the HALVING algorithm. 
What is the maximum number of mistakes that can be made by the HALVINGalgorithm, for an arbitrary finite H, before it exactly learns the target concept? 
Notice that learning the target concept "exactly" corresponds to reaching a statewhere the version space contains only a single hypothesis (as usual, we assumethe target concept cis inH). 
To derive the mistake bound, note that the only time the HALVING algorithmcan make a mistake is when the majority of hypotheses in its current version spaceincorrectly classify the new example. In this case, once the correct classification isrevealed to the learner, the version space will be reduced toat most half its currentsize (i.e., only those hypotheses that voted with the minority will be retained). 
Given that each mistake reduces the size of the version space byat least half, 
and given that the initial version space contains only IH I members, the maximumnumber of mistakes possible before the version space contains just one memberis log2 IH I. In fact one can show the bound is Llog, IH (1. Consider, for example, 
the case in which IHI = 7. The first mistake must reduce IHI toat most 3, andthe second mistake will then reduce itto 1. 
Note that [log2 IH(1 isa worst-case bound, and that itis possible for theHALVING algorithm to learn the target concept exactly without making any mis- 
takes at all! This can occur because even when the majority vote is correct, thealgorithm will remove the incorrect, minority hypotheses. If this occurs over theentire training sequence, then the version space may be reduced toa single memberwhile making no mistakes along the way. 
One interesting extension to the HALVING algorithm isto allow the hy- 
potheses to vote with different weights. Chapter 6 describes the Bayes optimalclassifier, which takes such a weighted vote among hypotheses. In the Bayes op- 
timal classifier, the weight assigned to each hypothesis is the estimated posteriorprobability that it describes the target concept, given the training data. Later inthis section we describe a different algorithm based on weighted voting, calledthe WEIGHTED-MAJORITY algorithm. 
7.5.3 Optimal Mistake BoundsThe above analyses give worst-case mistake bounds for two specific algorithms: 
FIND-S and CANDIDATE-ELIMINATION. Itis interesting to ask what is the optimalmistake bound for an arbitrary concept class C, assuming H = C. By optimalmistake bound we mean the lowest worst-case mistake bound over all possiblelearning algorithms. Tobe more precise, for any learning algorithm A and anytarget concept c, let MA(c) denote the maximum over all possible sequences oftraining examples of the number of mistakes made byA to exactly learn c. Nowfor any nonempty concept class C, let MA(C) - max,,~ MA(c). Note that abovewe showed MFindPS(C) = n + 1 when Cis the concept class described by upto n boolean literals. We also showed MHalving(C) 5 log2((CI) for any conceptclass C. 
We define the optimal mistake bound for a concept class C below. 
Definition: Let Cbe an arbitrary nonempty concept class. The optimal mistakebound for C, denoted Opt (C), is the minimum over all possible learning algorithmsA ofMA(C). 
Opt (C) = min Adearning algorithmsMA (aSpeaking informally, this definition states that Opt(C) is the number ofmistakes made for the hardest target concept inC, using the hardest trainingsequence, by the best algorithm. Littlestone (1987) shows that for any conceptclass C, there isan interesting relationship among the optimal mistake bound forC, the bound of the HALVING algorithm, and the VC dimension ofC, namelyFurthermore, there exist concept classes for which the four quantities aboveare exactly equal. One such concept class is the powerset Cpof any finite setof instances X. In this case, VC(Cp) = 1x1 = log2(1CpJ), so all four quantitiesmust be equal. Littlestone (1987) provides examples of other concept classes forwhich VC(C) is strictly less than Opt (C) and for which Opt (C) is strictly lessthan M~aIvin~(C) 
7.5.4 WEIGHTED-MAJORITY AlgorithmIn this section we consider a generalization of the HALVING algorithm calledthe WEIGHTED-MAJORITY algorithm. The WEIGHTED-MAJORITY algorithm makespredictions by taking a weighted vote among a pool of prediction algorithms andlearns by altering the weight associated with each prediction algorithm. Theseprediction algorithms can be taken tobe the alternative hypotheses inH, or theycan be taken tobe alternative learning algorithms that themselves vary over time. 
All that we require ofa prediction algorithm is that it predict the value of the targetconcept, given an instance. One interesting property of the WEIGHTED-MAJORITYalgorithm is that itis able to accommodate inconsistent training data. This isbecause it does not eliminate a hypothesis that is found tobe inconsistent withsome training example, but rather reduces its weight. A second interesting propertyis that we can bound the number of mistakes made by WEIGHTED-MAJORITY interms of the number of mistakes committed by the best of the pool of predictionalgorithms. 
The WEIGHTED-MAJORITY algorithm begins by assigning a weight of 1 toeach prediction algorithm, then considers the training examples. Whenever a pre- 
diction algorithm misclassifies a new training example its weight is decreased bymultiplying itby some number B, where 0 5 B < 1. The exact definition of theWEIGHTED-MAJORITY algorithm is given in Table 7.1. 
Notice iff? = 0 then WEIGHTED-MAJORITY is identical to the HALVING al- 
gorithm. On the other hand, ifwe choose some other value for p, no predictionai denotes the if* prediction algorithm in the pool Aof algorithms. wi denotes the weight associatedwith ai. 
For all i initialize wic 1For each training example (x, c(x)) 
c Initialize qo and qlto 0am For each prediction algorithm aic Ifai(x) =O then qot q0 +wiIf ai(x) = 1 then qlc ql + wiIf ql > qo then predict c(x) = 1If qo > q1 then predict c(x) = 0If ql = qo then predict 0 or 1 at random for c(x) 
For each prediction algorithm aiin A doIf ai(x) # c(x) then wi +- BuriTABLE 7.1WEIGHTED-MAJORITY algorithm. 
algorithm will ever be eliminated completely. Ifan algorithm misclassifies a train- 
ing example, it will simply receive a smaller vote in the future. 
We now show that the number of mistakes committed by the WEIGHTED- 
MAJORITY algorithm can be bounded in terms of the number of mistakes made bythe best prediction algorithm in the voting pool. 
Theorem 7.5. Relative mistake bound for WEIGHTED-MAJORITY. Let Dbe anysequence of training examples, let Abe any set ofn prediction algorithms, and letk be the minimum number of mistakes made by any algorithm inA for the trainingsequence D. Then the number of mistakes over D made by the WEIGHTED-MAJORITYalgorithm using /3 = 4 isat most2.4(k + log, n) 
Proof. We prove the theorem by comparing the final weight of the best predictionalgorithm to the sum of weights over all algorithms. Let aj denote an algorithm fromA that commits the optimal number kof mistakes. The final weight wj associatedwith aj will be because its initial weight is 1 and itis multiplied by 3 for eachmistake. Now consider the sum W = x:=, wiof the weights associated with all nalgorithms inA. Wis initially n. For each mistake made by WEIGHTED-MAJORITY, 
Wis reduced toat most :w. This is the case because the algorithms voting in theweighted majority must hold at least half of the total weight W, and this portionof W will be reduced bya factor of 4. Let M denote the total number of mistakescommitted by WEIGHTED-MAJORITY for the training sequence D. Then the final totalweight Wis at most n(:lM. Because the final weight wj cannot be greater than thefinal total weight, we haveRearranging terms yieldsM 5 (k + log' n, 2.4(k + log, n) 
-1% (a) - 
which proves the theorem. 
To summarize, the above theorem states that the number of mistakes madeby the WEIGHTED-MAJORITY algorithm will never be greater than a constant factortimes the number of mistakes made by the best member of the pool, plus a termthat grows only logarithmically in the size of the pool. 
This theorem is generalized by Littlestone and Warmuth (1991), who showthat for an arbitrary 0 5 j3 < 1 the above bound is 
/ 7.6 SUMMARY AND FURTHER READINGThe main points of this chapter include: 
0 The probably approximately correct (PAC) model considers algorithms thatlearn target concepts from some concept class C, using training examplesdrawn at random according toan unknown, but fixed, probability distribu- 
tion. It requires that the learner probably (with probability at least [l - 61) 
learn a hypothesis that is approximately (within error E) correct, given com- 
putational effort and training examples that grow only polynornially withI/€ , 1/6, the size of the instances, and the size of the target concept. 
0 Within the setting of the PAC learning model, any consistent learner usinga finite hypothesis space H where CH will, with probability (1 - S), 
output a hypothesis within error Eof the target concept, after observing mrandomly drawn training examples, as long asThis gives a bound on the number of training examples sufficient for suc- 
cessful learning under the PAC model. 
One constraining assumption of the PAC learning model is that the learnerknows in advance some restricted concept class C that contains the targetconcept tobe learned. In contrast, the agnostic learning model considers themore general setting in which the learner makes no assumption about theclass from which the target concept is drawn. Instead, the learner outputsthe hypothesis from H that has the least error (possibly nonzero) over thetraining data. Under this less restrictive agnostic learning model, the learneris assured with probability (1 -6) to output a hypothesis within error Eof thebest possible hypothesis inH, after observing rn randomly drawn trainingexamples, provideda The number of training examples required for successful learning is stronglyinfluenced by the complexity of the hypothesis space considered by thelearner. One useful measure of the complexity ofa hypothesis space His its Vapnik-Chervonenkis dimension, VC(H). VC(H) is the size of thelargest subset of instances that can be shattered (split in all possible ways) 
byH. 
aAn alternative upper bound on the number of training examples sufficientfor successful learning under the PAC model, stated in terms ofVC(H) isA lower bound isa An alternative learning model, called the mistake bound model, is used toanalyze the number of training examples a learner will misclassify beforeit exactly learns the target concept. For example, the HALVING algorithmwill make at most Llog, 1 H 1 J mistakes before exactly learning any targetconcept drawn from H. For an arbitrary concept class C, the best worst- 
case algorithm will make Opt (C) mistakes, whereVC(C> 5 Opt(C) I log,(lCI) 
a The WEIGHTED-MAJORITY algorithm combines the weighted votes of multipleprediction algorithms to classify new instances. It learns weights for each ofthese prediction algorithms based on errors made over a sequence of exam- 
ples. Interestingly, the number of mistakes made by WEIGHTED-MAJORITY canbe bounded in terms of the number of mistakes made by the best predictionalgorithm in the pool. 
Much early work on computational learning theory dealt with the questionof whether the learner could identify the target concept in the limit, given anindefinitely long sequence of training examples. The identification in the limitmodel was introduced by Gold (1967). A good overview of results in this area is 
(Angluin 1992). Vapnik (1982) examines in detail the problem of uniform con- 
vergence, and the closely related PAC-learning model was introduced by Valiant 
(1984). The discussion in this chapter of € -exhausting the version space is basedon Haussler's (1988) exposition. A useful collection of results under the PACmodel can be found in Blumer etal. (1989). Kearns and Vazirani (1994) pro- 
vide an excellent exposition of many results from computational learning theory. 
Earlier texts in this area include Anthony and Biggs (1992) and Natarajan (1991). 
Current research on computational learning theory covers a broad range oflearning models and learning algorithms. Much of this research can be foundin the proceedings of the annual conference on Computational Learning Theory 
(COLT). Several special issues of the journal Machine Learning have also beendevoted to this topic. 
EXERCISES7.1. Consider training a two-input perceptron. Give an upper bound on the number oftraining examples sufficient to assure with 90% confidence that the learned percep- 
tron will have true error ofat most 5%. Does this bound seem realistic? 
7.2. Consider the class Cof concepts of the form (a 4 x 5 b)~(c 5 y 5 d), where a, b, c, 
and d are integers in the interval (0,99). Note each concept in this class correspondsto a rectangle with integer-valued boundaries ona portion of the x, y plane. Hint: 
Given a region in the plane bounded by the points (0,O) and (n - 1, n - I), thenumber of distinct rectangles with integer-valued boundaries within this region is 
("M)2. 
2i (a) Give an upper bound on the number of randomly drawn training examplessufficient to assure that for any target concept cin C, any consistent learnerusing H = C will, with probability 95%, output a hypothesis with error atmost .15. 
(b) Now suppose the rectangle boundaries a, b, c, and d take on real values insteadof integer values. Update your answer to the first part of this question. 
7.3. In this chapter we derived an expression for the number of training examples suf- 
ficient to ensure that every hypothesis will have true error no worse than 6 plusits observed training error errorD(h). In particular, we used Hoeffding bounds toderive Equation (7.3). Derive an alternative expression for the number of trainingexamples sufficient to ensure that every hypothesis will have true error no worsethan (1 + y)errorD(h). You can use the general Chernoff bounds to derive such aresult. 
Chernoff bounds: Suppose XI,. . . , Xm are the outcomes ofrn independentcoin flips (Bernoulli trials), where the probability of heads on any single trial isPr[Xi = 11 = p and the probability of tails isPr[Xi = 01 = 1 - p. Define S = 
XI + X2 + -. - + Xmto be the sum of the outcomes of these m trials. The expectedvalue ofS/mis E[S/m] = p. The Chernoff bounds govern the probability that S/mwill differ from pby some factor 0 5 y 5 1. 
7.4. Consider a learning problem in which X = % is the set of real numbers, and C = His the set of intervals over the reals, H = {(a < x < b) Ia, bE E}. What is theprobability that a hypothesis consistent with m examples of this target concept willhave error at least E? Solve this using the VC dimension. Can you find a secondway to solve this, based on first principles and ignoring the VC dimension? 
7.5. Consider the space of instances X corresponding to all points in the x, y plane. Givethe VC dimension of the following hypothesis spaces: 
(a) H, = the set of all rectangles in the x, y plane. That is, H = {((a < x < b)~(c < 
Y -= d))la, b, c, dE W. 
(b) H, = circles in the x, y plane. Points inside the circle are classified as positiveexamples 
(c) H, =triangles in the x, y plane. Points inside the triangle are classified as positiveexamples7.6. Write a consistent learner for Hr from Exercise 7.5. Generate a variety of targetconcept rectangles at random, corresponding to different rectangles in the plane. 
Generate random examples of each of these target concepts, based ona uniformdistribution of instances within the rectangle from (0,O) to (100, 100). Plot thegeneralization error asa function of the number of training examples, m. On thesame graph, plot the theoretical relationship between 6 and m, for 6 = .95. Doestheory fit experiment? 
7.7. Consider the hypothesis class Hrd2 of "regular, depth-2 decision trees" over nBoolean variables. A "regular, depth-2 decision tree" isa depth-2 decision tree (atree with four leaves, all distance 2 from the root) in which the left and right childof the root are required to contain the same variable. For instance, the followingtree isin HrdZ. 
x3 
/ \ 
xlxl 
/\ /\ 
+ - - + 
(a) Asa function ofn, how many syntactically distinct trees are there in HrdZ? 
(b) Give an upper bound for the number of examples needed in the PAC model tolearn Hrd2 with error 6 and confidence 6. 
(c) Consider the following WEIGHTED-MAJORITY algorithm, for the class Hrd2. YOUbegin with all hypotheses in Hrd2 assigned an initial weight equal to 1. Everytime you see a new example, you predict based ona weighted majority vote overall hypotheses in Hrd2. Then, instead of eliminating the inconsistent trees, you cutdown their weight bya factor of 2. How many mistakes will this procedure makeat most, asa function ofn and the number of mistakes of the best tree in Hrd2? 
7.8. This question considers the relationship between the PAC analysis considered in thischapter and the evaluation of hypotheses discussed in Chapter 5. Consider a learningtask in which instances are described byn boolean variables (e.g., xlA& AX^ . . . f,) 
and are drawn according toa fixed but unknown probability distribution V. Thetarget concept is known tobe describable bya conjunction of boolean attributes andtheir negations (e.g., xzA&), and the learning algorithm uses this concept class as itshypothesis space H. A consistent learner is provided a set of 100 training examplesdrawn according toV. It outputs a hypothesis h from H that is consistent with all100 examples (i.e., the error ofh over these training examples is zero). 
(a) We are interested in the true error ofh, that is, the probability that it willmisclassify future instances drawn randomly according toV. Based on the aboveinformation, can you give an interval into which this true error will fall withat least 95% probability? Ifso, state it and justify it briefly. If not, explain thedifficulty. 
(b) You now draw a new set of 100 instances, drawn independently according to thesame distribution D. You find that h misclassifies 30 of these 100 new examples. 
Can you give an interval into which this true error will fall with approximately95% probability? (Ignore the performance over the earlier training data for thispart.) Ifso, state it and justify it briefly. If not, explain the difficulty. 
(c) It may seem a bit odd that h misclassifies 30% of the new examples even thoughit perfectly classified the training examples. Is this event more likely for largen or small n? Justify your answer ina sentence. 
REFERENCESAngluin, D. (1992). Computational learning theory: Survey and selected bibliography. Proceedingsof the Twenty-Fourth Annual ACM Symposium on Theory of Computing (pp. 351-369). ACMPress. 
Angluin, D., Frazier, M., & Pitt, L. (1992). Learning conjunctions of horn clauses. Machine Learning, 
9, 147-164. 
Anthony, M., & Biggs, N. (1992). Computational learning theory: An introduction. Cambridge, 
England: Cambridge University Press. 
Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. (1989). Learnability and the Vapnik- 
Chemonenkis dimension. Journal of the ACM, 36(4) (October), 929-965. 
Ehrenfeucht, A., Haussler, D., Kearns, M., & Valiant, L. (1989). A general lower bound on thenumber of examples needed for learning. Informution and computation, 82, 247-261. 
Gold, E. M. (1967). Language identification in the limit. Information and Control, 10, 447-474. 
Goldman, S. (Ed.). (1995). Special issue on computational learning theory. Machine Learning, 
18(2/3), February. 
Haussler, D. (1988). Quantifying inductive bias: A1 learning algorithms and Valiant's learning frame- 
work. ArtGcial Intelligence, 36, 177-221. 
Kearns, M. J., & Vazirani, U. V. (1994). An introduction to computational learning theory. Cambridge, 
MA: MIT Press. 
Laird, P. (1988). Learning from good and bad data. Dordrecht: Kluwer Academic Publishers. 
Li, M., & Valiant, L. G. (Eds.). (1994). Special issue on computational learning theory. MachineLearning, 14(1). 
Littlestone, N. (1987). Learning quickly when irrelevant attributes abound: A new linear-thresholdalgorithm. Machine Learning, 2, 285-318. 
Littlestone, N., & Warmuth, M. (1991). The weighted majority algorithm (Technical report UCSC- 
CRL-91-28). Univ. of California Santa Cruz, Computer Engineering and Information SciencesDept., Santa Cruz, CA. 
Littlestone, N., & Warmuth, M. (1994). The weighted majority algorithm. Information and Compu- 
tation (log), 212-261. 
Pltt, L. (Ed.). (1990). Special issue on computational learning theory. Machine Learning, 5(2). 
Natarajan, B. K. (1991). Machine learning: A theoretical approach. San Mateo, CA: Morgan Kauf- 
mann. 
Valiant, L. (1984). A theory of the learnable. Communications of the ACM, 27(1 I), 1134-1 142. 
Vapnik, V. N. (1982). Estimation of dependences based on empirical data. New York: Springer- 
Verlag. 
Vapnik, V. N., & Chervonenkis, A. (1971). On the uniform convergence of relative frequencies ofevents to their probabilities. Theory of Probability and Its Applications, 16, 264-280. 
CHAPTERINSTANCE-BASEDLEARNINGIn contrast to learning methods that construct a general, explicit description ofthe target function when training examples are provided, instance-based learningmethods simply store the training examples. Generalizing beyond these examplesis postponed until a new instance must be classified. Each time a new queryinstance is encountered, its relationship to the previously stored examples isex- 
amined in order to assign a target function value for the new instance. Instance- 
based learning includes nearest neighbor and locally weighted regression meth- 
ods that assume instances can be represented as points ina Euclidean space. Italso includes case-based reasoning methods that use more complex, symbolic rep- 
resentations for instances. Instance-based methods are sometimes referred toas 
"lazy" learning methods because they delay processing until a new instance mustbe classified. A key advantage of this kind of delayed, or lazy, learning isthat instead of estimating the target function once for the entire instance space, 
these methods can estimate it locally and differently for each new instance to beclassified. 
8.1 INTRODUCTIONInstance-based learning methods such as nearest neighbor and locally weighted re- 
gression are conceptually straightforward approaches to approximating real-valuedor discrete-valued target functions. Learning in these algorithms consists of simplystoring the presented training data. When a new query instance is encountered, aset of similar related instances is retrieved from memory and used to classify theCHAPTER 8 INSTANCE-BASED LEARNING 231new query instance. One key difference between these approaches and the meth- 
ods discussed in other chapters is that instance-based approaches can constructa different approximation to the target function for each distinct query instancethat must be classified. In fact, many techniques construct only a local approxi- 
mation to the target function that applies in the neighborhood of the new queryinstance, and never construct an approximation designed to perform well over theentire instance space. This has significant advantages when the target function isvery complex, but can still be described bya collection of less complex localapproximations. 
Instance-based methods can also use more complex, symbolic representa- 
tions for instances. In case-based learning, instances are represented in this fashionand the process for identifying "neighboring" instances is elaborated accordingly. 
Case-based reasoning has been applied to tasks such as storing and reusing pastexperience ata help desk, reasoning about legal cases by referring to previouscases, and solving complex scheduling problems by reusing relevant portions ofpreviously solved problems. 
One disadvantage of instance-based approaches is that the cost of classifyingnew instances can be high. This is due to the fact that nearly all computationtakes place at classification time rather than when the training examples are firstencountered. Therefore, techniques for efficiently indexing training examples area significant practical issue in reducing the computation required at query time. 
A second disadvantage to many instance-based approaches, especially nearest- 
neighbor approaches, is that they typically consider all attributes of the instanceswhen attempting to retrieve similar training examples from memory. If the targetconcept depends on only a few of the many available attributes, then the instancesthat are truly most "similar" may well bea large distance apart. 
In the next section we introduce the k-NEAREST NEIGHBOR learning algo- 
rithm, including several variants of this widely-used approach. The subsequentsection discusses locally weighted regression, a learning method that constructslocal approximations to the target function and that can be viewed asa general- 
ization ofk-NEAREST NEIGHBOR algorithms. We then describe radial basis functionnetworks, which provide an interesting bridge between instance-based and neuralnetwork learning algorithms. The next section discusses case-based reasoning, aninstance-based approach that employs symbolic representations and knowledge- 
based inference. This section includes an example application of case-based rea- 
soning toa problem in engineering design. Finally, we discuss the fundarnen- 
tal differences in capabilities that distinguish lazy learning methods discussed inthis chapter from eager learning methods discussed in the other chapters of thisbook. 
8.2 k-NEAREST NEIGHBOR LEARNINGThe most basic instance-based method is the k-NEAREST NEIGHBOR algorithm. Thisalgorithm assumes all instances correspond to points in the n-dimensional space8". The nearest neighbors ofan instance are defined in terms of the standardI 
Euclidean distance. More precisely, let an arbitrary instance xbe described by thefeature vectorwhere ar (x) denotes the value of the rth attribute of instance x. Then the distancebetween two instances xi and xjis defined tobe d(xi, xj), whereIn nearest-neighbor learning the target function may be either discrete-valuedor real-valued. Let us first consider learning discrete-valued target functions of theform f : W -+ V, where Vis the finite set {vl, . . . v,}. The k-NEAREST NEIGHBORalgorithm for approximatin5 a discrete-valued target function is given in Table 8.1. 
As shown there, the value f (x,) returned by this algorithm as its estimate off (x,) 
is just the most common value off among the k training examples nearest tox,. Ifwe choose k = 1, then the 1-NEAREST NEIGHBOR algorithm assigns tof(x,) 
the value f (xi) where xiis the training instance nearest tox,. For larger valuesof k, the algorithm assigns the most common value among the k nearest trainingexamples. 
Figure 8.1 illustrates the operation of the k-NEAREST NEIGHBOR algorithm forthe case where the instances are points ina two-dimensional space and where thetarget function is boolean valued. The positive and negative training examples areshown by "+" and "-" respectively. A query point x, is shown as well. Note the1-NEAREST NEIGHBOR algorithm classifies x, asa positive example in this figure, 
whereas the 5-NEAREST NEIGHBOR algorithm classifies itas a negative example. 
What is the nature of the hypothesis space H implicitly considered by thek-NEAREST NEIGHBOR algorithm? Note the k-NEAREST NEIGHBOR algorithm neverforms an explicit general hypothesis f regarding the target function f. It simplycomputes the classification of each new query instance as needed. Nevertheless, 
Training algorithm: 
For each training example (x, f (x)), add the example to the list trainingaxamplesClassification algorithm: 
Given a query instance xqto be classified, 
Let xl . . .xk denote the k instances from trainingaxamples that are nearest to xqReturn Gwhere S(a, b) = 1 ifa = b and where 6(a, b) = 0 otherwise. 
TABLE 8.1The k-NEAREST NEIGHBOR algorithm for approximating a discrete-valued function f : 8" -+ V. 
CHAPTER 8 INSTANCE-BASED LEARNING 233FIGURE 8.1k-NEAREST NEIGHBOR. A set of positive and negative training examples is shown on the left, alongwith a query instance x, tobe classified. The I-NEAREST NEIGHBOR algorithm classifies x, positive, 
whereas 5-NEAREST NEIGHBOR classifies itas negative. On the right is the decision surface inducedby the 1-NEAREST NEIGHBOR algorithm for a typical set of training examples. The convex polygonsurrounding each training example indicates the region of instance space closest to that point (i.e., 
the instances for which the 1-NEAREST NEIGHBOR algorithm will assign the classification belongingto that training example). 
we can still ask what the implicit general function is, or what classificationswould be assigned ifwe were to hold the training examples constant and querythe algorithm with every possible instance inX. The diagram on the right sideof Figure 8.1 shows the shape of this decision surface induced by 1-NEARESTNEIGHBOR over the entire instance space. The decision surface isa combination ofconvex polyhedra surrounding each of the training examples. For every trainingexample, the polyhedron indicates the set of query points whose classificationwill be completely determined by that training example. Query points outside thepolyhedron are closer to some other training example. This kind of diagram isoften called the Voronoi diagram of the set of training examples. 
The k-NEAREST NEIGHBOR algorithm is easily adapted to approximatingcontinuous-valued target functions. To accomplish this, we have the algorithmcalculate the mean value of the k nearest training examples rather than calculatetheir most common value. More precisely, to approximate a real-valued targetfunction f : !)In + !)Iwe replace the final line of the above algorithm by the line8.2.1 Distance-Weighted NEAREST NEIGHBOR AlgorithmOne obvious refinement to the k-NEAREST NEIGHBOR algorithm isto weight the con- 
tribution of each of the k neighbors according to their distance to the query pointx,, giving greater weight to closer neighbors. For example, in the algorithm ofTable 8.1, which approximates discrete-valued target functions, we might weightthe vote of each neighbor according to the inverse square of its distance from x,. 
This can be accomplished by replacing the final line of the algorithm bywhereTo accommodate the case where the query point x, exactly matches one of thetraining instances xi and the denominator d(x,, xi12 is therefore zero, we assignf(x,) tobe f (xi) in this case. If there are several such training examples, weassign the majority classification among them. 
We can distance-weight the instances for real-valued target functions in asimilar fashion, replacing the final line of the algorithm in this case bywhere wiis as defined in Equation (8.3). Note the denominator in Equation (8.4) isa constant that normalizes the contributions of the various weights (e.g., it assuresthat iff (xi) = c for all training examples, then f(x,) tc as well). 
Note all of the above variants of the k-NEAREST NEIGHBOR algorithm consideronly the k nearest neighbors to classify the query point. Once we add distanceweighting, there is really no harm in allowing all training examples to have aninfluence on the classification of the x,, because very distant examples will havevery little effect onf(x,). The only disadvantage of considering all examples isthat our classifier will run more slowly. If all training examples are consideredwhen classifying a new query instance, we call the algorithm a global method. 
If only the nearest training examples are considered, we call ita local method. 
When the rule in Equation (8.4) is applied asa global method, using all trainingexamples, itis known as Shepard's method (Shepard 1968). 
8.2.2 Remarks onk-NEAREST NEIGHBOR AlgorithmThe distance-weighted k-NEAREST NEIGHBOR algorithm isa highly effective induc- 
tive inference method for many practical problems. Itis robust to noisy trainingdata and quite effective when itis provided a sufficiently large set of trainingdata. Note that by taking the weighted average of the k neighbors nearest to thequery point, it can smooth out the impact of isolated noisy training examples. 
What is the inductive bias ofk-NEAREST NEIGHBOR? The basis for classifyingnew query points is easily understood based on the diagrams in Figure 8.1. Theinductive bias corresponds toan assumption that the classification ofan instancex, will be most similar to the classification of other instances that are nearby inEuclidean distance. 
One practical issue in applying k-NEAREST NEIGHBOR algorithms is that thedistance between instances is calculated based on all attributes of the instance 
(i.e., on all axes in the Euclidean space containing the instances). This lies incontrast to methods such as rule and decision tree learning systems that selectonly a subset of the instance attributes when forming the hypothesis. To see theeffect of this policy, consider applying k-NEAREST NEIGHBOR toa problem in whicheach instance is described by 20 attributes, but where only 2 of these attributesare relevant to determining the classification for the particular target function. Inthis case, instances that have identical values for the 2 relevant attributes maynevertheless be distant from one another in the 20-dimensional instance space. 
Asa result, the similarity metric used byk-NEAREST NEIGHBOR--depending onall 20 attributes-will be misleading. The distance between neighbors will bedominated by the large number of irrelevant attributes. This difficulty, whicharises when many irrelevant attributes are present, is sometimes referred toas thecurse of dimensionality. Nearest-neighbor approaches are especially sensitive tothis problem. 
One interesting approach to overcoming this problem isto weight eachattribute differently when calculating the distance between two instances. Thiscorresponds to stretching the axes in the Euclidean space, shortening the axes thatcorrespond to less relevant attributes, and lengthening the axes that correspondto more relevant attributes. The amount by which each axis should be stretchedcan be determined automatically using a cross-validation approach. To see how, 
first note that we wish to stretch (multiply) the jth axis by some factor zj, wherethe values zl . . . z, are chosen to minimize the true classification error of thelearning algorithm. Second, note that this true error can be estimated using cross- 
validation. Hence, one algorithm isto select a random subset of the availabledata to use as training examples, then determine the values ofzl . . . z, that leadto the minimum error in classifying the remaining examples. By repeating thisprocess multiple times the estimate for these weighting factors can be made moreaccurate. This process of stretching the axes in order to optimize the performanceof k-NEAREST NEIGHBOR provides a mechanism for suppressing the impact ofirrelevant attributes. 
An even more drastic alternative isto completely eliminate the least relevantattributes from the instance space. This is equivalent to setting some of the ziscaling factors to zero. Moore and Lee (1994) discuss efficient cross-validationmethods for selecting relevant subsets of the attributes for k-NEAREST NEIGHBORalgorithms. In particular, they explore methods based on leave-one-out cross- 
validation, in which the set ofm training instances is repeatedly divided into atraining set of size m - 1 and test set of size 1, in all possible ways. This leave-one- 
out approach is easily implemented ink-NEAREST NEIGHBOR algorithms becauseno additional training effort is required each time the training set is redefined. 
Note both of the above approaches can be seen as stretching each axis by someconstant factor. Alternatively, we could stretch each axis bya value that varies overthe instance space. However, aswe increase the number of degrees of freedomavailable to the algorithm for redefining its distance metric in such a fashion, wealso increase the risk of overfitting. Therefore, the approach of locally stretchingthe axes is much less common. 
One additional practical issue in applying k-NEAREST NEIGHBOR is efficientmemory indexing. Because this algorithm delays all processing until a new queryis received, significant computation can be required to process each new query. 
Various methods have been developed for indexing the stored training examples sothat the nearest neighbors can be identified more efficiently at some additional costin memory. One such indexing method is the kd-tree (Bentley 1975; Friedmanet al. 1977), in which instances are stored at the leaves ofa tree, with nearbyinstances stored at the same or nearby nodes. The internal nodes of the tree sortthe new query x, to the relevant leaf by testing selected attributes ofx,. 
8.2.3 A Note on TerminologyMuch of the literature on nearest-neighbor methods and weighted local regressionuses a terminology that has arisen from the field of statistical pattern recognition. 
In reading that literature, itis useful to know the following terms: 
0 Regression means approximating a real-valued target function. 
Residual is the error {(x) - f (x) in approximating the target function. 
Kernel function is the function of distance that is used to determine theweight of each training example. In other words, the kernel function is thefunction K such that wi = K(d(xi, x,)). 
83 LOCALLY WEIGHTED REGRESSIONThe nearest-neighbor approaches described in the previous section can be thoughtof as approximating the target function f (x) at the single query point x = x,. 
Locally weighted regression isa generalization of this approach. It constructs anexplicit approximation tof over a local region surrounding x,. Locally weightedregression uses nearby or distance-weighted training examples to form this localapproximation tof. For example, we might approximate the target function inthe neighborhood surrounding x, using a linear function, a quadratic function, 
a multilayer neural network, or some other functional form. The phrase "locallyweighted regression" is called local because the function is approximated based aonly on data near the query point, weighted because the contribution of eachtraining example is weighted by its distance from the query point, and regressionbecause this is the term used widely in the statistical learning community for theproblem of approximating real-valued functions. 
Given a new query instance x,, the general approach in locally weightedregression isto construct an approximation f^ that fits the training examples in theneighborhood surrounding x,. This approximation is then used to calculate thevalue f"(x,), which is output as the estimated target value for the query instance. 
The description off^ may then be deleted, because a different local approximationwill be calculated for each distinct query instance. 
CHAPTER 8 INSTANCE-BASED LEARNING 2378.3.1 Locally Weighted Linear RegressionLet us consider the case of locally weighted regression in which the target functionf is approximated near x, using a linear function of the formAs before, ai(x) denotes the value of the ith attribute of the instance x. 
Recall that in Chapter 4 we discussed methods such as gradient descent tofind the coefficients wo . . . w, to minimize the error in fitting such linear func- 
tions toa given set of training examples. In that chapter we were interested ina global approximation to the target function. Therefore, we derived methods tochoose weights that minimize the squared error summed over the set Dof trainingexampleswhich led usto the gradient descent training rulewhere qis a constant learning rate, and where the training rule has been re- 
expressed from the notation of Chapter 4 to fit our current notation (i.e., t + f (x), 
o -+ f(x), and xj -+ aj(x)). 
How shall we modify this procedure to derive a local approximation ratherthan a global one? The simple way isto redefine the error criterion Eto emphasizefitting the local training examples. Three possible criteria are given below. Notewe write the error E(x,) to emphasize the fact that now the error is being definedas a function of the query point x,. 
1. Minimize the squared error over just the k nearest neighbors: 
1El(xq) = - C (f (x) - f^(xN2xc k nearest nbrs of xq2. Minimize the squared error over the entire set Dof training examples, whileweighting the error of each training example by some decreasing functionK of its distance from x, : 
3. Combine 1 and 2: 
Criterion two is perhaps the most esthetically pleasing because it allowsevery training example to have an impact on the classification ofx,. However, 
this approach requires computation that grows linearly with the number of trainingexamples. Criterion three isa good approximation to criterion two and has theadvantage that computational cost is independent of the total number of trainingexamples; its cost depends only on the number kof neighbors considered. 
Ifwe choose criterion three above and rederive the gradient descent ruleusing the same style of argument asin Chapter 4, we obtain the following trainingrule (see Exercise 8.1): 
Notice the only differences between this new rule and the rule given by Equa- 
tion (8.6) are that the contribution of instance xto the weight update is nowmultiplied by the distance penalty K(d(x,, x)), and that the error is summed overonly the k nearest training examples. In fact, ifwe are fitting a linear functionto a fixed set of training examples, then methods much more efficient than gra- 
dient descent are available to directly solve for the desired coefficients wo . . . urn. 
Atkeson etal. (1997a) and Bishop (1995) survey several such methods. 
8.3.2 Remarks on Locally Weighted RegressionAbove we considered using a linear function to approximate fin the neigh- 
borhood of the query instance x,. The literature on locally weighted regressioncontains a broad range of alternative methods for distance weighting the trainingexamples, and a range of methods for locally approximating the target function. Inmost cases, the target function is approximated bya constant, linear, or quadraticfunction. More complex functional forms are not often found because (1) the costof fitting more complex functions for each query instance is prohibitively high, 
and (2) these simple approximations model the target function quite well over asufficiently small subregion of the instance space. 
8.4 RADIAL BASIS FUNCTIONSOne approach to function approximation that is closely related to distance-weightedregression and also to artificial neural networks is learning with radial basis func- 
tions (Powell 1987; Broomhead and Lowe 1988; Moody and Darken 1989). Inthis approach, the learned hypothesis isa function of the formwhere each xuis an instance from X and where the kernel function K,(d(x,, x)) 
is defined so that it decreases as the distance d(x,, x) increases. Here kis a user- 
provided constant that specifies the number of kernel functions tobe included. 
Even though f(x) isa global approximation tof (x), the contribution from eachof the Ku(d (xu, x)) terms is localized toa region nearby the point xu. Itis commonCHmR 8 INSTANCE-BASED LEARNING 239to choose each function K, (d (xu, x)) tobe a Gaussian function (see Table 5.4) 
centered at the point xu with some variance a;. 
+d2(xu,x) 
K,(d(x,, x)) = e2". 
We will restrict our discussion here to this common Gaussian kernel function. 
As shown by Hartman etal. (1990), the functional form of Equation (8.8) canapproximate any function with arbitrarily small error, provided a sufficiently largenumber kof such Gaussian kernels and provided the width a2 of each kernel canbe separately specified. 
The function given by Equation (8.8) can be viewed as describing a two- 
layer network where the first layer of units computes the values of the variousK,(d(x,, x)) and where the second layer computes a linear combination of thesefirst-layer unit values. An example radial basis function (RBF) network is illus- 
trated in Figure 8.2. 
Given a set of training examples of the target function, RBF networks aretypically trained ina two-stage process. First, the number kof hidden units isdetermined and each hidden unit uis defined by choosing the values ofxu and a: 
that define its kernel function K,(d(x,, x)). Second, the weights w, are trained tomaximize the fit of the network to the training data, using the global error criteriongiven by Equation (8.5). Because the kernel functions are held fixed during thissecond stage, the linear weight values w, can be trained very efficiently. 
Several alternative methods have been proposed for choosing an appropriatenumber of hidden units or, equivalently, kernel functions. One approach is toallocate a Gaussian kernel function for each training example (xi, f (xi)), centeringthis Gaussian at the point xi. Each of these kernels may be assigned the same widtha2. Given this approach, the RBF network learns a global approximation to thetarget function in which each training example (xi, f (xi)) can influence the valueof f only in the neighborhood ofxi. One advantage of this choice of kernelfunctions is that it allows the RBF network to fit the training data exactly. Thatis, for any set ofm training examples the weights wo . . . w, for combining them Gaussian kernel functions can be set so that f(xi) = f (xi) for each trainingFIGURE 8.2A radial basis function network. Each hidden unit producesan activation determined bya Gaussian function centered atsome instance xu. Therefore, its activation will be close to zerounless the input xis near xu. The output unit produces a linearcombination of the hidden unit activations. Although the networkshown here has just one output, multiple output units can alsobe included. 
A second approach isto choose a set of kernel functions that is smallerthan the number of training examples. This approach can be much more effi- 
cient than the first approach, especially when the number of training examplesis large. The set of kernel functions may be distributed with centers spaced uni- 
formly throughout the instance space X. Alternatively, we may wish to distributethe centers nonuniformly, especially if the instances themselves are found to bedistributed nonuniformly over X. In this later case, we can pick kernel functioncenters by randomly selecting a subset of the training instances, thereby samplingthe underlying distribution of instances. Alternatively, we may identify prototyp- 
ical clusters of instances, then add a kernel function centered at each cluster. Theplacement of the kernel functions in this fashion can be accomplished using un- 
supervised clustering algorithms that fit the training instances (but not their targetvalues) toa mixture of Gaussians. The EM algorithm discussed in Section 6.12.1provides one algorithm for choosing the means ofa mixture ofk Gaussians tobest fit the observed instances. In the case of the EM algorithm, the means arechosen to maximize the probability of observing the instances xi, given the kestimated means. Note the target function value f (xi) of the instance does notenter into the calculation of kernel centers by unsupervised clustering methods. 
The only role of the target values f (xi) in this case isto determine the outputlayer weights w,. 
To summarize, radial basis function networks provide a global approxima- 
tion to the target function, represented bya linear combination of many localkernel functions. The value for any given kernel function is non-negligible onlywhen the input x falls into the region defined by its particular center and width. 
Thus, the network can be viewed asa smooth linear combination of many localapproximations to the target function. One key advantage to RBF networks is thatthey can be trained much more efficiently than feedforward networks trained withBACKPROPAGATION. This follows from the fact that the input layer and the outputlayer ofan RBF are trained separately. 
8.5 CASE-BASED REASONINGInstance-based methods such ask-NEAREST NEIGHBOR and locally weighted re- 
gression share three key properties. First, they are lazy learning methods in thatthey defer the decision of how to generalize beyond the training data until a newquery instance is observed. Second, they classify new query instances by ana- 
lyzing similar instances while ignoring instances that are very different from thequery. Third, they represent instances as real-valued points inan n-dimensionalEuclidean space. Case-based reasoning (CBR) isa learning paradigm based onthe first two of these principles, but not the third. In CBR, instances are typi- 
ca:'y represented using more rich symbolic descriptions, and the methods usedto retrieve similar instances are correspondingly more elaborate. CBR has beenapplied to problems such as conceptual design of mechanical devices based ona stored library of previous designs (Sycara etal. 1992), reasoning about newlegal cases based on previous rulings (Ashley 1990), and solving planning andCHAPTER 8 INSTANCEBASED LEARNING 241scheduling problems by reusing and combining portions of previous solutions tosimilar problems (Veloso 1992). 
Let us consider a prototypical example ofa case-based reasoning system toground our discussion. The CADET system (Sycara etal. 1992) employs case- 
based reasoning to assist in the conceptual design of simple mechanical devicessuch as water faucets. It uses a library containing approximately 75 previousdesigns and design fragments to suggest conceptual designs to meet the specifi- 
cations of new design problems. Each instance stored in memory (e.g., a waterpipe) is represented by describing both its structure and its qualitative function. 
New design problems are then presented by specifying the desired function andrequesting the corresponding structure. This problem setting is illustrated in Fig- 
ure 8.3. The top half of the figure shows the description ofa typical stored casecalled aT-junction pipe. Its function is represented in terms of the qualitative re- 
lationships among the waterflow levels and temperatures at its inputs and outputs. 
In the functional description at its right, an arrow with a "+" label indicates thatthe variable at the arrowhead increases with the variable at its tail. For example, 
the output waterflow Q3 increases with increasing input waterflow Ql. Similarly, 
A stored case: T-junction pipeStructure: 
QIJT T = temperature 
'LQ = watertlowr Q3J5 Qz4A problem specification: Water faucetStructure: 
Function: 
Function: 
FIGURE 8.3A stored case and a new problem. The top half of the figure describes a typical design fragmentin the case library of CADET. The function is represented by the graph of qualitative dependenciesamong the T-junction variables (described in the text). The bottom half of the figure shows a typicaldesign problem. 
a "-" label indicates that the variable at the head decreases with the variable atthe tail. The bottom half of this figure depicts a new design problem describedby its desired function. This particular function describes the required behavior ofone type of water faucet. Here Q, refers to the flow of cold water into the faucet, 
Qhto the input flow of hot water, and Q, to the single mixed flow out of thefaucet. Similarly, T,, Th, and T, refer to the temperatures of the cold water, hotwater, and mixed water respectively. The variable C, denotes the control signalfor temperature that is input to the faucet, and Cf denotes the control signal forwaterflow. Note the description of the desired function specifies that these con- 
trols C, and Cf are to influence the water flows Q, and Qh, thereby indirectlyinfluencing the faucet output flow Q, and temperature T,. 
Given this functional specification for the new design problem, CADETsearches its library for stored cases whose functional descriptions match the designproblem. Ifan exact match is found, indicating that some stored case implementsexactly the desired function, then this case can be returned asa suggested solutionto the design problem. Ifno exact match occurs, CADET may find cases thatmatch various subgraphs of the desired functional specification. In Figure 8.3, forexample, the T-junction function matches a subgraph of the water faucet functiongraph. More generally, CADET searches for subgraph isomorphisms between thetwo function graphs, so that parts ofa case can be found to match parts of thedesign specification. Furthermore, the system may elaborate the original functionspecification graph in order to create functionally equivalent graphs that maymatch still more cases. It uses general knowledge about physical influences tocreate these elaborated function graphs. For example, it uses a rewrite rule thatallows itto rewrite the influenceThis rewrite rule can be interpreted as stating that ifB must increase with A, 
then itis sufficient to find some other quantity x such that B increases with x, 
and x increases with A. Here xis a universally quantified variable whose valueis bound when matching the function graph against the case library. In fact, thefunction graph for the faucet shown in Figure 8.3 isan elaboration of the original - 
functional specification produced by applying such rewrite rules. 
By retrieving multiple cases that match different subgraphs, the entire de- 
sign can sometimes be pieced together. In general, the process of producing afinal solution from multiple retrieved cases can be very complex. It may requiredesigning portions of the system from first principles, in addition to merging re- 
trieved portions from stored cases. It may also require backtracking on earlierchoices of design subgoals and, therefore, rejecting cases that were previouslyretrieved. CADET has very limited capabilities for combining and adapting multi- 
ple retrieved cases to form the final design and relies heavily on the user for thisadaptation stage of the process. As described by Sycara etal. (1992), CADET isCHAPTER 8 INSTANCE-BASED LEARMNG 243a research prototype system intended to explore the potential role of case-basedreasoning in conceptual design. It does not have the range of analysis algorithmsneeded to refine these abstract conceptual designs into final designs. 
Itis instructive to examine the correspondence between the problem settingof CADET and the general setting for instance-based methods such ask-NEARESTNEIGHBOR. In CADET each stored training example describes a function graphalong with the structure that implements it. New queries correspond to new func- 
tion graphs. Thus, we can map the CADET problem into our standard notation bydefining the space of instances Xto be the space of all function graphs. The tar- 
get function f maps function graphs to the structures that implement them. Eachstored training example (x, f (x)) isa pair that describes some function graph xand the structure f (x) that implements x. The system must learn from the trainingexample cases to output the structure f (x,) that successfully implements the inputfunction graph query x,. 
The above sketch of the CADET system illustrates several generic propertiesof case-based reasoning systems that distinguish them from approaches such ask-NEAREST NEIGHBOR. 
0 Instances or cases may be represented by rich symbolic descriptions, suchas the function graphs used in CADET. This may require a similarity metricdifferent from Euclidean distance, such as the size of the largest sharedsubgraph between two function graphs. 
0 Multiple retrieved cases may be combined to form the solution to the newproblem. This is similar to the k-NEAREST NEIGHBOR approach, in that mul- 
tiple similar cases are used to construct a response for the new query. 
However, the process for combining these multiple retrieved cases can bevery different, relying on knowledge-based reasoning rather than statisticalmethods. 
0 There may bea tight coupling between case retrieval, knowledge-basedreasoning, and problem solving. One simple example of this is found inCADET, which uses generic knowledge about influences to rewrite functiongraphs during its attempt to find matching cases. Other systems have beendeveloped that more fully integrate case-based reasoning into general search- 
based problem-solving systems. Two examples are ANAPRON (Golding andRosenbloom 199 1) and PRODIGY/ANALOGY (Veloso 1992). 
To summarize, case-based reasoning isan instance-based learning methodin which instances (cases) may be rich relational descriptions and in which the re- 
trieval and combination of cases to solve the current query may rely on knowledge- 
based reasoning and search-intensive problem-solving methods. One current re- 
search issue in case-based reasoning isto develop improved methods for indexingcases. The central issue here is that syntactic similarity measures (e.g., subgraphisomorphism between function graphs) provide only an approximate indication ofthe relevance ofa particular case toa particular problem. When the CBR systemattempts to reuse the retrieved cases it may uncover difficulties that were not244 MACHINE LEARNINGcaptured by this syntactic similarity measure. For example, in CADET the multi- 
ple retrieved design fragments may turn out tobe incompatible with one another, 
making it impossible to combine them into a consistent final design. When thisoccurs in general, the CBR system may backtrack and search for additional cases, 
adapt the existing cases, or resort to other problem-solving methods. Importantly, 
when such difficulties are detected they also provide training data for improvingthe similarity metric or, equivalently, the indexing structure for the case library. 
In particular, ifa case is retrieved based on the similarity metric, but found to beirrelevant based on further analysis, then the similarity metric should be refinedto reject this case for similar subsequent queries. 
8.6 REMARKS ON LAZY AND EAGER LEARNINGIn this chapter we considered three lazy learning methods: the k-NEAREST NEIGH- 
BOR algorithm, locally weighted regression, and case-based reasoning. We callthese methods lazy because they defer the decision of how to generalize beyondthe training data until each new query instance is encountered. We also discussedone eager learning method: the method for learning radial basis function networks. 
We call this method eager because it generalizes beyond the training data beforeobserving the new query, committing at training time to the network structure andweights that define its approximation to the target function. In this same sense, 
every other algorithm discussed elsewhere in this book (e.g., BACKPROPAGATION, 
C4.5) isan eager learning algorithm. 
Are there important differences in what can be achieved by lazy versus eagerlearning? Let us distinguish between two kinds of differences: differences in com- 
putation time and differences in the classifications produced for new queries. Thereare obviously differences in computation time between eager and lazy methods. 
For example, lazy methods will generally require less computation during training, 
but more computation when they must predict the target value for a new query. 
The more fundamental question is whether there are essential differences inthe inductive bias that can be achieved by lazy versus eager methods. The keydifference between lazy and eager methods in this regard is0 Lazy methods may consider the query instance x, when deciding how togeneralize beyond the training data D. 
0 Eager methods cannot. By the time they observe the query instance x, theyhave already chosen their (global) approximation to the target function. 
Does this distinction affect the generalization accuracy of the learner? It does if werequire that the lazy and eager learner employ the same hypothesis space H. Toillustrate, consider the hypothesis space consisting of linear functions. The locallyweighted linear regression algorithm discussed earlier isa lazy learning methodbased on this hypothesis space. For each new query x, it generalizes from thetraining data by choosing a new hypothesis based on the training examples near x,. 
In contrast, an eager learner that uses the same hypothesis space of linear functionsCHAPTER 8 INSTANCE-BASED LEARNING 245must choose its approximation before the queries are observed. The eager learnermust therefore commit toa single linear function hypothesis that covers the entireinstance space and all future queries. The lazy method effectively uses a richerhypothesis space because it uses many different local linear functions to form itsimplicit global approximation to the target function. Note this same situation holdsfor other learners and hypothesis spaces as well. A lazy version of BACKPROPAGA- 
TION, for example, could learn a different neural network for each distinct querypoint, compared to the eager version of BACKPROPAGATION discussed in Chapter 4. 
The key point in the above paragraph is that a lazy learner has the optionof (implicitly) representing the target function bya combination of many localapproximations, whereas an eager learner must commit at training time toa singleglobal approximation. The distinction between eager and lazy learning is thusrelated to the distinction between global and local approximations to the targetfunction. 
Can we create eager methods that use multiple local approximations toachieve the same effects as lazy local methods? Radial basis function networks canbe seen as one attempt to achieve this. The RBF learning methods we discussedare eager methods that commit toa global approximation to the target functionat training time. However, an RBF network represents this global function as alinear combination of multiple local kernel functions. Nevertheless, because RBFlearning methods must commit to the hypothesis before the query point is known, 
the local approximations they create are not specifically targeted to the querypoint to the same degree asin a lazy learning method. Instead, RBF networks arebuilt eagerly from local approximations centered around the training examples, oraround clusters of training examples, but not around the unknown future querypoints. 
To summarize, lazy methods have the option of selecting a different hypoth- 
esis or local approximation to the target function for each query instance. Eagermethods using the same hypothesis space are more restricted because they mustcommit toa single hypothesis that covers the entire instance space. Eager methodscan, of course, employ hypothesis spaces that combine multiple local approxima- 
tions, asin RBF networks. However, even these combined local approximations donot give eager methods the full ability of lazy methods to customize to unknownfuture query instances. 
8.7 SUMMARY AND FURTHER READINGThe main points of this chapter include: 
Instance-based learning methods differ from other approaches to function ap- 
proximation because they delay processing of training examples until theymust label a new query instance. Asa result, they need not form an explicithypothesis of the entire target function over the entire instance space, in- 
dependent of the query instance. Instead, they may form a different localapproximation to the target function for each query instance. 
246 MACHINE LEARNING1 
0 Advantages of instance-based methods include the ability to model complextarget functions bya collection of less complex local approximations and thefact that information present in the training examples is never lost (becausethe examples themselves are stored explicitly). The main practical difficul- 
ties include efficiency of labeling new instances (all processing is done atquery time rather than in advance), difficulties in determining an appropriatedistance metric for retrieving "related" instances (especially when examplesare represented by complex symbolic descriptions), and the negative impactof irrelevant features on the distance metric. 
0 k-NEAREST NEIGHBOR isan instance-based algorithm for approximating real- 
valued or discrete-valued target functions, assuming instances correspond topoints inan n-dimensional Euclidean space. The target function value fora new query is estimated from the known values of the k nearest trainingexamples. 
0 Locally weighted regression methods are a generalization ofk-NEARESTNEIGHBOR in which an explicit local approximation to the target functionis constructed for each query instance. The local approximation to the targetfunction may be based ona variety of functional forms such as constant, 
linear, or quadratic functions oron spatially localized kernel functions. 
0 Radial basis function (RBF) networks are a type of artificial neural networkconstructed from spatially localized kernel functions. These can be seen as ablend of instance-based approaches (spatially localized influence of each ker- 
nel function) and neural network approaches (a global approximation to thetarget function is formed at training time rather than a local approximationat query time). Radial basis function networks have been used successfullyin applications such as interpreting visual scenes, in which the assumptionof spatially local influences is well-justified. 
0 Case-based reasoning isan instance-based approach in which instances arerepresented by complex logical descriptions rather than points ina Euclideanspace. Given these complex symbolic descriptions of instances, a rich varietyof methods have been proposed for mapping from the training examples totarget function values for new instances. Case-based reasoning methods havebeen used in applications such as modeling legal reasoning and for guidingsearches in complex manufacturing and transportation planning problems. 
The k-NEAREST NEIGHBOR algorithm is one of the most thoroughly analyzedalgorithms in machine learning, due in part to its age and in part to its simplicity. 
Cover and Hart (1967) present early theoretical results, and Duda and Hart (1973) 
provide a good overview. Bishop (1995) provides a discussion ofk-NEARESTNEIGHBOR and its relation to estimating probability densities. An excellent currentsurvey of methods for locally weighted regression is given by Atkeson etal. 
(1997). The application of these methods to robot control is surveyed by Atkesonet al. (1997b). 
A thorough discussion of radial basis functions is provided by Bishop (1995). 
Other treatments are given by Powell (1987) and Poggio and Girosi (1990). SeeSection 6.12 of this book for a discussion of the EM algorithm and its applicationto selecting the means ofa mixture of Gaussians. 
Kolodner (1993) provides a general introduction to case-based reasoning. 
Other general surveys and collections describing recent research are given byAamodt etal. (1994), Aha etal. (1991), Haton etal. (1995), Riesbeck and Schank 
(1989), Schank etal. (1994), Veloso and Aamodt (1995), Watson (1995), andWess etal. (1994). 
EXERCISES8.1. Derive the gradient descent rule for a distance-weighted local linear approximationto the target function, given by Equation (8.1). 
8.2. Consider the following alternative method for accounting for distance in weightedlocal regression. Create a virtual set of training examples D' as follows: For eachtraining example (x, f (x)) in the original data set D, create some (possibly fractional) 
number of copies of (x, f (x)) inD', where the number of copies isK (d(x,, x)). Nowtrain a linear approximation to minimize the error criterionThe idea here isto make more copies of training examples that are near the queryinstance, and fewer of those that are distant. Derive the gradient descent rule forthis criterion. Express the rule in the form ofa sum over members ofD rather thanD', and compare it with the rules given by Equations (8.6) and (8.7). 
8.3. Suggest a lazy version of the eager decision tree learning algorithm ID3 (see Chap- 
ter 3). What are the advantages and disadvantages of your lazy algorithm comparedto the original eager algorithm? 
REFERENCESAamodt, A., & Plazas, E. (1994). Case-based reasoning: Foundational issues, methodological varia- 
tions, and system approaches. A1 Communications, 7(1), 39-52. 
Aha, D., & Kibler, D. (1989). Noise-tolerant instance-based learning algorithms. Proceedings of theIJCAI-89 (794-799). 
Aha, D., Kibler, D., & Albert, M. (1991). Instance-based learning algorithms. Machine Learning, 6, 
37-66. 
Ashley, K. D. (1990). Modeling legal argument: Reasoning with cases and hypotheticals. Cambridge, 
MA: MIT Press. 
Atkeson, C. G., Schaal, S. A., & Moore, A. W. (1997a). Locally weighted learning. AIReview, (toappear). 
Atkeson, C. G., Moore, A. W., & Schaal, S. A. (1997b). Locally weighted learning for control. A1Review, (to appear). 
Bareiss, E. R., Porter, B., & Weir, C. C. (1988). PROTOS: An exemplar-based learning apprentice. 
International Journal of Man-Machine Studies, 29, 549-561. 
Bentley, J. L. (1975). Multidimensional binary search trees used for associative searching. Cornmu- 
nications of the ACM, 18(9), 509-517. 
248 MACHINE LEARNING1 
Bishop, C. M. (1995). Neural networks for pattern recognition. Oxford, England: Oxford UniversityPress. 
Bisio, R., & Malabocchia, F. (1995). Cost estimation of software projects through case-based reason- 
ing. InM. Veloso and A. Aamodt (Eds.), Lecture Notes in Artificial Intelligence (pp. 11-22). 
Berlin: Springer-Verlag. 
Broomhead, D. S., & Lowe, D. (1988). Multivariable functional interpolation and adaptive networks. 
Complex Systems, 2, 321-355. 
Cover, T., & Hart, P. (1967). Nearest neighbor pattern classification. IEEE Transactions on Infonna- 
tion Theory, 13,21-27. 
Duda, R., & Hart, P. (1973). Pattern classification and scene analysis. New York: John Wiley & 
Sons. 
Franke, R. (1982). Scattered data interpolation: Tests of some methods. Mathematics of Computation, 
38, 181-200. 
Friedman, J., Bentley, J., & Finkel, R. (1977). An algorithm for finding best matches in logarithmicexpected time. ACM Transactions on Mathematical Software, 3(3), 209-226. 
Golding, A., & Rosenbloom, P. (1991). Improving rule-based systems through case-based reasoning. 
Proceedings of the Ninth National Conference on Artificial Intelligence (pp. 22-27). Cam- 
bridge: AAAI Pressme MIT Press. 
Hartman, E. J., Keller, J. D., & Kowalski, J. M. (1990). Layered neural networks with Gaussianhidden units as universal approximations. Neural Computation, 2(2), 210-215. 
Haton, J.-P., Keane, M., & Manago, M. (Eds.). (1995). Advances in case-based reasoning: SecondEuropean workshop. Berlin: Springer-Verlag. 
Kolodner, J. L. (1993). Case-Based Reasoning. San Francisco: Morgan Kaufmann. 
Moody, J. E., & Darken, C. J. (1989). Fast learning in networks of locally-tuned processing units. 
Neural Computation, 1(2), 281-294. 
Moore, A. W., & Lee, M. S. (1994). Efficient algorithms for minimizing cross validation error. Pro- 
ceedings of the 11th International Conference on Machine Learning. San Francisco: MorganKaufmann. 
Poggio, T., & Girosi, F. (1990). Networks for approximation and learning. Proceedings of the IEEE, 
78(9), 1481-1497. 
Powell, M. J. D. (1987). Radial basis functions for multivariable interpolation: A review. In Mason, 
J., & Cox, M. (Eds.). Algorithms for approximation (pp. 143-167). Oxford: Clarendon Press. 
Riesbeck, C., & Schank, R. (1989). Inside case-based reasoning. Hillsdale, NJ: Lawrence Erlbaum. 
Schank, R. (1982). Dynamic Memory. Cambridge, England: Cambridge University Press. 
Schank, R., Riesbeck, C., & Kass, A. (1994). Inside case-based explanation. Hillsdale, NJ: LawrenceErlbaum. 
Shepard, D. (1968). A two-dimensional interpolation function for irregularly spaced data. Proceedingsof the 23rd National Conference of the ACM (pp. 517-523). 
Stanfill, C., & Waltz, D. (1986). Toward memory-based reasoning. Communications of the ACM, 
29(12), 1213-1228. 
Sycara, K., Guttal, R., Koning, J., Narasimhan, S., & Navinchandra, D. (1992). CADET: A case- 
based synthesis tool for engineering design. International Journal of Expert Systems, 4(2), 
157-188. 
Veloso, M. M. (1992). Planning and learning by analogical reasoning. Berlin: Springer-Verlag. 
Veloso, M. M., & Aamodt, A. (Eds.). (1995). Case-based reasoning research and development. 
Lectwe Notes in Artificial Intelligence. Berlin: Springer-Verlag. 
Watson, I. (Ed.). (1995). Progress in case-based reasoning: First United Kingdom workshop. Berlin: 
Springer-Verlag. 
Wess, S., Althoff, K., & Richter, M. (Eds.). (1994). Topics in case-based reasoning. Berlin: Springer- 
Verlag. 
CHAPTERGENETICALGORITHMSGenetic algorithms provide an approach to learning that is based loosely on simulatedevolution. Hypotheses are often described by bit strings whose interpretation dependson the application, though hypotheses may also be described by symbolic expressionsor even computer programs. The search for an appropriate hypothesis begins with apopulation, or collection, of initial hypotheses. Members of the current populationgive rise to the next generation population by means of operations such as randommutation and crossover, which are patterned after processes in biological evolution. 
At each step, the hypotheses in the current population are evaluated relative toa given measure of fitness, with the most fit hypotheses selected probabilisticallyas seeds for producing the next generation. Genetic algorithms have been appliedsuccessfully toa variety of learning tasks and to other optimization problems. Forexample, they have been used to learn collections of rules for robot control and tooptimize the topology and learning parameters for artificial neural networks. Thischapter covers both genetic algorithms, in which hypotheses are typically describedby bit strings, and genetic programming, in which hypotheses are described bycomputer programs. 
9.1 MOTIVATIONGenetic algorithms (GAS) provide a learning method motivated byan analogy tobiological evolution. Rather than search from general-to-specific hypotheses, orfrom simple-to-complex, GAS generate successor hypotheses by repeatedly mutat- 
ing and recombining parts of the best currently known hypotheses. At each step, 
a collection of hypotheses called the current population is updated by replacingsome fraction of the population by offspring of the most fit current hypotheses. 
The process forms a generate-and-test beam-search of hypotheses, in which vari- 
ants of the best current hypotheses are most likely tobe considered next. Thepopularity of GAS is motivated bya number of factors including: 
Evolution is known tobe a successful, robust method for adaptation withinbiological systems. 
GAS can search spaces of hypotheses containing complex interacting parts, 
where the impact of each part on overall hypothesis fitness may be difficultto model. 
0 Genetic algorithms are easily parallelized and can take advantage of thedecreasing costs of powerful computer hardware. 
This chapter describes the genetic algorithm approach, illustrates its use, andexamines the nature of its hypothesis space search. We also describe a variantcalled genetic programming, in which entire computer programs are evolved tocertain fitness criteria. Genetic algorithms and genetic programming are two ofthe more popular approaches ina field that is sometimes called evolutionarycomputation. In the final section we touch on selected topics in the study ofbiological evolution, including the Baldwin effect, which describes an interestinginterplay between the learning capabilities of single individuals and the rate ofevolution of the entire population. 
9.2 GENETIC ALGORITHMS - 
The problem addressed by GAS isto search a space of candidate hypotheses toidentify the best hypothesis. In GAS the "best hypothesis" is defined as the onethat optimizes a predefined numerical measure for the problem at hand, called thehypothesis Jitness. For example, if the learning task is the problem of approxi- 
mating an unknown function given training examples of its input and output, thenfitness could be defined as the accuracy of the hypothesis over this training data. 
If the task isto learn a strategy for playing chess, fitness could be defined as thenumber of games won by the individual when playing against other individualsin the current population. 
Although different implementations of genetic algorithms vary in their de- 
tails, they typically share the following structure: The algorithm operates by itera- 
tively updating a pool of hypotheses, called the population. On each iteration, allmembers of the population are evaluated according to the fitness function. A newpopulation is then generated by probabilistically selecting the most fit individualsfrom the current population. Some of these selected individuals are carried forwardinto the next generation population intact. Others are used as the basis for creatingnew offspring individuals by applying genetic operations such as crossover andmutation. 
Fitness: A function that assigns an evaluation score, given a hypothesis. 
Fitnessdhreshold: A threshold specifying the termination criterion. 
p: The number of hypotheses tobe included in the population. 
r: The fraction of the population tobe replaced by Crossover at each step. 
m: The mutation rate. 
Initialize population: Pc Generate p hypotheses at randomEvaluate: For each hin P, compute Fitness(h)' 
While [max Fitness(h)] < Fitnessdhreshold doh 
Create a new generation, Ps: 
1. Select: F'robabilistically select (1 - r)p members ofP to add toPs. The probability Pr(hi) ofselecting hypothesis hi from Pis given by2. Crossover: Probabilistically select pairs of hypotheses from P, according to &(hi) givenabove. For each pair, (hl, h2), produce two offspring by applying the Crossover operator. 
Add all offspring toP,. 
3. Mutate: Choose m percent of the members ofP, with uniform probability. For each, invertone randomly selected bit in its representation. 
4. Update: Pt P,. 
5. Evaluate: for each hin P, compute Fitness(h) 
Return the hypothesis from P that has the highest fitness. 
TABLE 9.1A prototypical genetic algorithm. A population containing p hypotheses is maintained. On each itera- 
tion, the successor population Psis formed by probabilistically selecting current hypotheses accordingto their fitness and by adding new hypotheses. New hypotheses are created by applying a crossoveroperator to pairs of most fit hypotheses and by creating single point mutations in the resulting gener- 
ation of hypotheses. This process is iterated until sufficiently fit hypotheses are discovered. Typicalcrossover and mutation operators are defined ina subsequent table. 
A prototypical genetic algorithm is described in Table 9.1. The inputs tothis algorithm include the fitness function for ranking candidate hypotheses, athreshold defining an acceptable level of fitness for terminating the algorithm, 
the size of the population tobe maintained, and parameters that determine howsuccessor populations are tobe generated: the fraction of the population to bereplaced at each generation and the mutation rate. 
Notice in this algorithm each iteration through the main loop produces a newgeneration of hypotheses based on the current population. First, a certain numberof hypotheses from the current population are selected for inclusion in the nextgeneration. These are selected probabilistically, where the probability of selectinghypothesis hiis given byThus, the probability that a hypothesis will be selected is proportional to itsown fitness and is inversely proportional to the fitness of the other competinghypotheses in the current population. 
Once these members of the current generation have been selected for inclu- 
sion in the next generation population, additional members are generated using acrossover operation. Crossover, defined in detail in the next section, takes two par- 
ent hypotheses from the current generation and creates two offspring hypothesesby recombining portions of both parents. The parent hypotheses are chosen proba- 
bilistically from the current population, again using the probability function givenby Equation (9.1). After new members have been created by this crossover opera- 
tion, the new generation population now contains the desired number of members. 
At this point, a certain fraction mof these members are chosen at random, andrandom mutations all performed to alter these members. 
This GA algorithm thus performs a randomized, parallel beam search forhypotheses that perform well according to the fitness function. In the follow- 
ing subsections, we describe in more detail the representation of hypotheses andgenetic operators used in this algorithm. 
9.2.1 Representing HypothesesHypotheses in GAS are often represented by bit strings, so that they can be easilymanipulated by genetic operators such as mutation and crossover. The hypothesesrepresented by these bit strings can be quite complex. For example, sets ofif-thenrules can easily be represented in this way, by choosing an encoding of rulesthat allocates specific substrings for each rule precondition and postcondition. 
Examples of such rule representations inGA systems are described by Holland 
(1986); Grefenstette (1988); and DeJong etal. (1993). 
To see how if-then rules can be encoded by bit strings, .first consider how wemight use a bit string to describe a constraint on the value ofa single attribute. Topick an example, consider the attribute Outlook, which can take on any of the threevalues Sunny, Overcast, or Rain. One obvious way to represent a constraint onOutlook isto use a bit string of length three, in which each bit position correspondsto one of its three possible values. Placing a 1 in some position indicates that theattribute is allowed to take on the corresponding value. For example, the string 010represents the constraint that Outlook must take on the second of these values, , 
or Outlook = Overcast. Similarly, the string 011 represents the more generalconstraint that allows two possible values, or (Outlook = Overcast v Rain). 
Note 11 1 represents the most general possible constraint, indicating that we don'tcare which of its possible values the attribute takes on. 
Given this method for representing constraints ona single attribute, con- 
junctions of constraints on multiple attributes can easily be represented by con- 
catenating the corresponding bit strings. For example, consider a second attribute, 
Wind, that can take on the value Strong or Weak. A rule precondition such as 
(Outlook = Overcast V Rain) A (Wind = Strong) 
can then be represented by the following bit string of length five: 
Outlook Wind01 1 10Rule postconditions (such as PlayTennis = yes) can be represented in asimilar fashion. Thus, an entire rule can be described by concatenating the bitstrings describing the rule preconditions, together with the bit string describingthe rule postcondition. For example, the ruleIF Wind = Strong THEN PlayTennis = yeswould be represented by the stringOutlook Wind PlayTennis111 10 10where the first three bits describe the "don't care" constraint on Outlook, the nexttwo bits describe the constraint on Wind, and the final two bits describe the rulepostcondition (here we assume PlayTennis can take on the values Yes orNo). 
Note the bit string representing the rule contains a substring for each attributein the hypothesis space, even if that attribute is not constrained by the rule pre- 
conditions. This yields a fixed length bit-string representation for rules, in whichsubstrings at specific locations describe constraints on specific attributes. Giventhis representation for single rules, we can represent sets of rules by similarlyconcatenating the bit string representations of the individual rules. 
In designing a bit string encoding for some hypothesis space, itis useful toarrange for every syntactically legal bit string to represent a well-defined hypoth- 
esis. To illustrate, note in the rule encoding in the above paragraph the bit string11 1 10 11 represents a rule whose postcondition does not constrain the targetattribute PlayTennis. Ifwe wish to avoid considering this hypothesis, we mayemploy a different encoding (e.g., allocate just one bit to the PlayTennis post- 
condition to indicate whether the value is Yes orNo), alter the genetic operatorsso that they explicitly avoid constructing such bit strings, or simply assign a verylow fitness to such bit strings. 
In some GAS, hypotheses are represented by symbolic descriptions ratherthan bit strings. For example, in Section 9.5 we discuss a genetic algorithm thatencodes hypotheses as computer programs. 
9.2.2 Genetic OperatorsThe generation of successors ina GAis determined bya set of operators thatrecombine and mutate selected members of the current population. Typical GAoperators for manipulating bit string hypotheses are illustrated in Table 9.1. Theseoperators correspond to idealized versions of the genetic operations found inbi- 
ological evolution. The two most common operators are crossover and mutation. 
The crossover operator produces two new offspring from two parent strings, 
by copying selected bits from each parent. The bit at position iin each offspringis copied from the bit at position iin one of the two parents. The choice of whichparent contributes the bit for position iis determined byan additional string calledthe crossover mask. To illustrate, consider the single-point crossover operator atthe top of Table 9.2. Consider the topmost of the two offspring in this case. Thisoffspring takes its first five bits from the first parent and its remaining six bitsfrom the second parent, because the crossover mask 11 11 1000000 specifies thesechoices for each of the bit positions. The second offspring uses the same crossovermask, but switches the roles of the two parents. Therefore, it contains the bits thatwere not used by the first offspring. In single-point crossover, the crossover maskis always constructed so that it begins with a string containing n contiguous Is, 
followed by the necessary number of 0s to complete the string. This results inoffspring in which the first n bits are contributed by one parent and the remainingbits by the second parent. Each time the single-point crossover operator is applied, 
Initial strings Crossover Mask OffspringSingle-point crossover: 
Two-point crossover: 
Uniform crossover: 
Point mutation: lllOloo_1000 111010~1000TABLE 9.2Common operators for genetic algorithms. These operators form offspring of hypotheses representedby bit strings. The crossover operators create two descendants from two parents, using the crossovermask to determine which parent contributes which bits. Mutation creates a single descendant from asingle parent by changing the value ofa randomly chosen bit. 
the crossover point nis chosen at random, and the crossover mask is then createdand applied. 
In two-point crossover, offspring are created by substituting intermediatesegments of one parent into the middle of the second parent string. Put anotherway, the crossover mask isa string beginning with no zeros, followed bya con- 
tiguous string ofnl ones, followed by the necessary number of zeros to completethe string. Each time the two-point crossover operator is applied, a mask is gen- 
erated by randomly choosing the integers no and nl. For instance, in the exampleshown in Table 9.2 the offspring are created using a mask for which no = 2 andn 1 = 5. Again, the two offspring are created by switching the roles played by thetwo parents. 
Uniform crossover combines bits sampled uniformly from the two parents, 
as illustrated in Table 9.2. In this case the crossover mask is generated asa randombit string with each bit chosen at random and independent of the others. 
In addition to recombination operators that produce offspring by combiningparts of two parents, a second type of operator produces offspring from a singleparent. In particular, the mutation operator produces small random changes to thebit string by choosing a single bit at random, then changing its value. Mutation isoften performed after crossover has been applied asin our prototypical algorithmfrom Table 9.1. 
Some GA systems employ additional operators, especially operators that arespecialized to the particular hypothesis representation used by the system. Forexample, Grefenstette etal. (1991) describe a system that learns sets of rulesfor robot control. It uses mutation and crossover, together with an operator forspecializing rules. Janikow (1993) describes a system that learns sets of rulesusing operators that generalize and specialize rules ina variety of directed ways 
(e.g., by explicitly replacing the condition onan attribute by "don't care"). 
9.2.3 Fitness Function and SelectionThe fitness function defines the criterion for ranking potential hypotheses and forprobabilistically selecting them for inclusion in the next generation population. Ifthe task isto learn classification rules, then the fitness function typically has acomponent that scores the classification accuracy of the rule over a set of providedtraining examples. Often other criteria may be included as well, such as the com- 
plexity or generality of the rule. More generally, when the bit-string hypothesis isinterpreted asa complex procedure (e.g., when the bit string represents a collec- 
tion ofif-then rules that will be chained together to control a robotic device), thefitness function may measure the overall performance of the resulting procedurerather than performance of individual rules. 
In our prototypical GA shown in Table 9.1, the probability that a hypothesiswill be selected is given by the ratio of its fitness to the fitness of other membersof the current population as seen in Equation (9.1). This method is sometimescalled jitness proportionate selection, or roulette wheel selection. Other methodsfor using fitness to select hypotheses have also been proposed. For example, in256 MACHINE LEARNING1 
tournament selection, two hypotheses are first chosen at random from the currentpopulation. With some predefined probability p the more fit of these two is thenselected, and with probability (1 - p) the less fit hypothesis is selected. Tourna- 
ment selection often yields a more diverse population than fitness proportionateselection (Goldberg and Deb 1991). In another method called rank selection, thehypotheses in the current population are first sorted by fitness. The probabilitythat a hypothesis will be selected is then proportional to its rank in this sortedlist, rather than its fitness. 
9.3 AN ILLUSTRATIVE EXAMPLEA genetic algorithm can be viewed asa general optimization method that searchesa large space of candidate objects seeking one that performs best according to thefitness function. Although not guaranteed to find an optimal object, GAS oftensucceed in finding an object with high fitness. GAS have been applied toa numberof optimization problems outside machine learning, including problems such ascircuit layout and job-shop scheduling. Within machine learning, they have beenapplied both to function-approximation problems and to tasks such as choosingthe network topology for artificial neural network learning systems. 
To illustrate the use of GAS for concept learning, we briefly summarizethe GABIL system described by DeJong etal. (1993). GABIL uses aGA tolearn boolean concepts represented bya disjunctive set of propositional rules. 
In experiments over several concept learning problems, GABIL was found to beroughly comparable in generalization accuracy to other learning algorithms suchas the decision tree learning algorithm C4.5 and the rule learning system AQ14. 
The learning tasks in this study included both artificial learning tasks designed toexplore the systems' generalization accuracy and the real world problem of breastcancer diagnosis. 
The algorithm used by GABIL is exactly the algorithm described inTa- 
ble 9.1. In experiments reported by DeJong etal. (1993), the parameter r, whichdetermines the fraction of the parent population replaced by crossover, was setto 0.6. The parameter m, which determines the mutation rate, was set to 0.001. 
These are typical settings for these parameters. The population size p was variedfrom 100 to 1000, depending on the specific learning task. 
The specific instantiation of the GA algorithm in GABIL can be summarizedas follows: 
0 Representation. Each hypothesis in GABIL corresponds toa disjunctive setof propositional rules, encoded as described in Section 9.2.1. In particular, 
the hypothesis space of rule preconditions consists ofa conjunction of con- 
straints ona fixed set of attributes, as described in that earlier section. Torepresent a set of rules, the bit-string representations of individual rules areconcatenated. To illustrate, consider a hypothesis space in which rule precon- 
ditions are conjunctions of constraints over two boolean attributes, a1 and a2. 
The rule postcondition is described bya single bit that indicates the predictedvalue of the target attribute c. Thus, the hypothesis consisting of the two rulesIFal=Tr\az=F THEN c=T; IF a2=T THEN c=Fwould be represented by the stringNote the length of the bit string grows with the number of rules in the hy- 
pothesis. This variable bit-string length requires a slight modification to thecrossover operator, as described below. 
a Genetic operators. GABIL uses the standard mutation operator of Table 9.2, 
in which a single bit is chosen at random and replaced by its complement. 
The crossover operator that it uses isa fairly standard extension to thetwo-point crossover operator described in Table 9.2. In particular, to accom- 
modate the variable-length bit strings that encode rule sets, and to constrainthe system so that crossover occurs only between like sections of the bitstrings that encode rules, the following approach is taken. To perform acrossover operation on two parents, two crossover points are first chosenat random in the first parent string. Let dl (dz) denote the distance fromthe leftmost (rightmost) of these two crossover points to the rule boundaryimmediately to its left. The crossover points in the second parent are nowrandomly chosen, subject to the constraint that they must have the same dland d2 value. For example, if the two parent strings areandand the crossover points chosen for the first parent are the points followingbit positions 1 and 8, 
where "[" and "1" indicate crossover points, then dl = 1 and dz = 3. Hencethe allowed pairs of crossover points for the second parent include the pairsof bit positions (1,3), (1,8), and (6,8). If the pair (1,3) happens to bechosen, 
then the two resulting offspring will beandAs this example illustrates, this crossover operation enables offspring tocontain a different number of rules than their parents, while assuring that allbit strings generated in this fashion represent well-defined rule sets. 
Fitness function. The fitness of each hypothesized rule set is based on itsclassification accuracy over the training data. In particular, the function usedto measure fitness iswhere correct (h) is the percent of all training examples correctly classifiedby hypothesis h. 
In experiments comparing the behavior of GABIL to decision tree learningalgorithms such as C4.5 and ID5R, and to the rule learning algorithm AQ14, 
DeJong etal. (1993) report roughly comparable performance among these systems, 
tested ona variety of learning problems. For example, over a set of 12 syntheticproblems, GABIL achieved an average generalization accuracy of 92.1 %, whereasthe performance of the other systems ranged from 91.2 % to 96.6 %. 
9.3.1 ExtensionsDeJong etal. (1993) also explore two interesting extensions to the basic designof GABIL. In one set of experiments they explored the addition of two new ge- 
netic operators that were motivated by the generalization operators common inmany symbolic learning methods. The first of these operators, AddAlternative, 
generalizes the constraint ona specific attribute by changing a 0 toa 1 in thesubstring corresponding to the attribute. For example, if the constraint onan at- 
tribute is represented by the string 10010, this operator might change itto 101 10. 
This operator was applied with probability .O1 to selected members of the popu- 
lation on each generation. The second operator, Dropcondition performs a moredrastic generalization step, by replacing all bits for a particular attribute bya 1. 
This operator corresponds to generalizing the rule by completely dropping theconstraint on the attribute, and was applied on each generation with probability 
.60. The authors report this revised system achieved an average performance of95.2% over the above set of synthetic learning tasks, compared to 92.1% for thebasic GA algorithm. 
In the above experiment, the two new operators were applied with the sameprobability to each hypothesis in the population on each generation. Ina secondexperiment, the bit-string representation for hypotheses was extended to includetwo bits that determine which of these operators may be applied to the hypothesis. 
In this extended representation, the bit string for a typical rule set hypothesiswould bewhere the final two bits indicate in this case that the AddAlternative operator maybe applied to this bit string, but that the Dropcondition operator may not. Thesetwo new bits define part of the search strategy used by the GA and are themselvesaltered and evolved using the same crossover and mutation operators that operateon other bits in the string. While the authors report mixed results with this approach 
(i.e., improved performance on some problems, decreased performance on others), 
it provides an interesting illustration of how GAS might in principle be used toevolve their own hypothesis search methods. 
9.4 HYPOTHESIS SPACE SEARCHAs illustrated above, GAS employ a randomized beam search method to seek amaximally fit hypothesis. This search is quite different from that of other learningmethods we have considered in this book. To contrast the hypothesis space searchof GAS with that of neural network BACKPROPAGATION, for example, the gradientdescent search in BACKPROPAGATION moves smoothly from one hypothesis to anew hypothesis that is very similar. In contrast, the GA search can move muchmore abruptly, replacing a parent hypothesis byan offspring that may be radicallydifferent from the parent. Note the GA search is therefore less likely to fall intothe same kind of local minima that can plague gradient descent methods. 
One practical difficulty in some GA applications is the problem of crowding. 
Crowding isa phenomenon in which some individual that is more highly fit thanothers in the population quickly reproduces, so that copies of this individual and1 very similar individuals take over a large fraction of the population. The negativeimpact of crowding is that it reduces the diversity of the population, thereby slow- 
ing further progress by the GA. Several strategies have been explored for reducingcrowding. One approach isto alter the selection function, using criteria such astournament selection or rank selection in place of fitness proportionate roulettewheel selection. A related strategy is "fitness sharing," in which the measuredfitness ofan individual is reduced by the presence of other, similar individualsin the population. A third approach isto restrict the kinds of individuals allowedto recombine to form offspring. For example, by allowing only the most similarindividuals to recombine, we can encourage the formation of clusters of similarindividuals, or multiple "subspecies" within the population. A related approach isto spatially distribute individuals and allow only nearby individuals to recombine. 
Many of these techniques are inspired by the analogy to biological evolution. 
9.4.1 Population Evolution and the Schema TheoremIt is interesting to ask whether one can mathematically characterize the evolutionover time of the population within aGA. The schema theorem of Holland (1975) 
provides one such characterization. Itis based on the concept of schemas, or pat- 
terns that describe sets of bit strings. Tobe precise, a schema is any string com- 
posed ofOs, Is, and *'s. Each schema represents the set of bit strings containing theindicated 0s and Is, with each "*" interpreted asa "don't care." For example, theschema 0*10 represents the set of bit strings that includes exactly 0010 and 01 10. 
An individual bit string can be viewed asa representative of each of thedifferent schemas that it matches. For example, the bit string 0010 can be thoughtof asa representative of 24 distinct schemas including 00**, O* 10, ****, etc. Sim- 
ilarly, a population of bit strings can be viewed in terms of the set of schemas thatit represents and the number of individuals associated with each of these schema. 
The schema theorem characterizes the evolution of the population within aGA in terms of the number of instances representing each schema. Let m(s, t) 
denote the number of instances of schema sin the population at time t (i.e., 
during the tth generation). The schema theorem describes the expected value ofm(s, t + 1) in terms ofm(s, t) and other properties of the schema, population, andGA algorithm parameters. 
The evolution of the population in the GA depends on the selection step, 
the recombination step, and the mutation step. Let us start by considering just theeffect of the selection step. Let f (h) denote the fitness of the individual bit stringh and f(t) denote the average fitness of all individuals in the population at time t. 
Let nbe the total number of individuals in the population. Let hE sn p, indicatethat the individual his both a representative of schema s and a member of thepopulation at time t. Finally, let 2(s, t) denote the average fitness of instances ofschema sin the population at time t. 
We are interested in calculating the expected value ofm(s, t + l), whichwe denote E[m(s, t + I)]. We can calculate E[m (s, t + I)] using the probabilitydistribution for selection given in Equation (9. I), which can be restated using ourcurrent terminology as follows: 
Now ifwe select one member for the new population according to this probabilitydistribution, then the probability that we will select a representative of schema s isThe second step above follows from the fact that by definition, 
Equation (9.2) gives the probability that a single hypothesis selected by the GAwill bean instance of schema s. Therefore, the expected number of instancesof s resulting from the n independent selection steps that create the entire newgeneration is just n times this probability. 
Equation (9.3) states that the expected number of instances of schema sat gener- 
ation t + 1 is proportional to the average fitness i(s, t) of instances of this schemaat time t, and inversely proportional to the average fitness f(t) of all membersof the population at time t. Thus, we can expect schemas with above average fit- 
ness tobe represented with increasing frequency on successive generations. If weview the GAas performing a virtual parallel search through the space of possibleschemas at the same time it performs its explicit parallel search through the spaceof individuals, then Equation (9.3) indicates that more fit schemas will grow ininfluence over time. 
While the above analysis considered only the selection step of the GA, thecrossover and mutation steps must be considered as well. The schema theorem con- 
siders only the possible negative influence of these genetic operators (e.g., randommutation may decrease the number of representatives ofs, independent ofO(s, t)), 
and considers only the case of single-point crossover. The full schema theoremthus provides a lower bound on the expected frequency of schema s, as follows: 
Here, p, is the probability that the single-point crossover operator will be appliedto an arbitrary individual, and p, is the probability that an arbitrary bit of anarbitrary individual will be mutated by the mutation operator. o(s) is the numberI of defined bits in schema s, where 0 and 1 are defined bits, but * is not. d(s) isthe distance between the leftmost and rightmost defined bits ins. Finally, 1 is thelength of the individual bit strings in the population. Notice the leftmost term inEquation (9.4) is identical to the term from Equation (9.3) and describes the ef- 
fect of the selection step. The middle term describes the effect of the single-pointcrossover operator-in particular, it describes the probability that an arbitrary in- 
dividual representing s will still represent s following application of this crossoveroperator. The rightmost term describes the probability that an arbitrary individualrepresenting schema s will still represent schema s following application of themutation operator. Note that the effects of single-point crossover and mutationincrease with the number of defined bits o(s) in the schema and with the distanced(s) between the defined bits. Thus, the schema theorem can be roughly interpretedas stating that more fit schemas will tend to grow in influence, especially schemascontaining a small number of defined bits (i.e., containing a large number of *'s), 
and especially when these defined bits are near one another within the bit string. 
The schema theorem is perhaps the most widely cited characterization ofpopulation evolution within aGA. One way in which itis incomplete is that it failsto consider the (presumably) positive effects of crossover and mutation. Numerousmore recent theoretical analyses have been proposed, including analyses based onMarkov chain models and on statistical mechanics models. See, for example, 
Whitley and Vose (1995) and Mitchell (1996). 
9.5 GENETIC PROGRAMMINGGenetic programming (GP) isa form of evolutionary computation in which the in- 
dividuals in the evolving population are computer programs rather than bit strings. 
Koza (1992) describes the basic genetic programming approach and presents abroad range of simple programs that can be successfully learned byGP. 
9.5.1 Representing ProgramsPrograms manipulated bya GP are typically represented by trees correspond- 
ing to the parse tree of the program. Each function call is represented by anode in the tree, and the arguments to the function are given by its descendantnodes. For example, Figure 9.1 illustrates this tree representation for the functionsin(x) + J-. To apply genetic programming toa particular domain, the usermust define the primitive functions tobe considered (e.g., sin, cos, J, +, -, ex- 
ponential~), as well as the terminals (e.g., x, y, constants such as 2). The geneticprogramming algorithm then uses an evolutionary search to explore the vast spaceof programs that can be described using these primitives. 
Asin a genetic algorithm, the prototypical genetic programming algorithmmaintains a population of individuals (in this case, program trees). On each it- 
eration, it produces a new generation of individuals using selection, crossover, 
and mutation. The fitness ofa given individual program in the population is typ- 
ically determined by executing the program ona set of training data. Crossoveroperations are performed by replacing a randomly chosen subtree of one parentFIGURE 9.1Program tree representation in genetic programming. 
Arbitrary programs are represented by their parse trees. 
FIGURE 9.2Crossover operation applied to two parent program trees (top). Crossover points (nodes shown inbold at top) are chosen at random. The subtrees rooted at these crossover points are then exchangedto create children trees (bottom). 
program bya subtree from the other parent program. Figure 9.2 illustrates a typicalcrossover operation. 
Koza (1992) describes a set of experiments applying aGP toa number ofapplications. In his experiments, 10% of the current population, selected prob- 
abilistically according to fitness, is retained unchanged in the next generation. 
The remainder of the new generation is created by applying crossover to pairsof programs from the current generation, again selected probabilistically accord- 
ing to their fitness. The mutation operator was not used in this particular set ofexperiments. 
9.5.2 Illustrative ExampleOne illustrative example presented by Koza (1992) involves learning an algorithmfor stacking the blocks shown in Figure 9.3. The task isto develop a general algo- 
rithm for stacking the blocks into a single stack that spells the word "universal," 
FIGURE 9.3A block-stacking problem. The task for GPis to discover a program that can transform an arbitraryinitial configuration of blocks into a stack that spells the word "universal." A set of 166 such initialconfigurations was provided to evaluate fitness of candidate programs (after Koza 1992). 
independent of the initial configuration of blocks in the world. The actions avail- 
able for manipulating blocks allow moving only a single block ata time. Inparticular, the top block on the stack can be moved to the table surface, or ablock on the table surface can be moved to the top of the stack. 
Asin most GP applications, the choice of problem representation has asignificant impact on the ease of solving the problem. In Koza's formulation, theprimitive functions used to compose programs for this task include the followingthree terminal arguments: 
0 CS (current stack), which refers to the name of the top block on the stack, 
orF if there isno current stack. 
TB (top correct block), which refers to the name of the topmost block onthe stack, such that it and those blocks beneath it are in the correct order. 
0 NN (next necessary), which refers to the name of the next block neededabove TBin the stack, in order to spell the word "universal," orF if nomore blocks are needed. 
As can be seen, this particular choice of terminal arguments provides a natu- 
ral representation for describing programs for manipulating blocks for this task. 
Imagine, in contrast, the relative difficulty of the task ifwe were to instead definethe terminal arguments tobe the x and y coordinates of each block. 
In addition to these terminal arguments, the program language in this appli- 
cation included the following primitive functions: 
(MSx) (move to stack), if block xis on the table, this operator moves x tothe top of the stack and returns the value T. Otherwise, it does nothing andreturns the value F. 
0 (MTx) (move to table), if block xis somewhere in the stack, this moves theblock at the top of the stack to the table and returns the value T. Otherwise, 
it returns the value F. 
0 (EQx y) (equal), which returns Tif x equals y, and returns F otherwise. 
0 (NOT x), which returns Tif x = F, and returns Fif x = T. 
0 (DUx y) (do until), which executes the expression x repeatedly until ex- 
pression y returns the value T. 
To allow the system to evaluate the fitness of any given program, Kozaprovided a set of 166 training example problems representing a broad variety ofinitial block configurations, including problems of differing degrees of difficulty. 
The fitness of any given program was taken tobe the number of these examplessolved by the algorithm. The population was initialized toa set of 300 randomprograms. After 10 generations, the system discovered the following program, 
which solves all 166 problems. 
(EQ (DU (MTCS)(NOT CS)) (DU (MSNN)(NOT NN)) ) 
Notice this program contains a sequence of two DU, or "Do Until" state- 
ments. The first repeatedly moves the current top of the stack onto the table, untilthe stack becomes empty. The second "Do Until" statement then repeatedly movesthe next necessary block from the table onto the stack. The role played by thetop level EQ expression here isto provide a syntactically legal way to sequencethese two "Do Until" loops. 
Somewhat surprisingly, after only a few generations, this GP was able todiscover a program that solves all 166 training problems. Of course the abilityof the system to accomplish this depends strongly on the primitive argumentsand functions provided, and on the set of training example cases used to evaluatefitness. 
9.5.3 Remarks on Genetic ProgrammingAs illustrated in the above example, genetic programming extends genetic algo- 
rithms to the evolution of complete computer programs. Despite the huge size ofthe hypothesis space it must search, genetic programming has been demonstratedto produce intriguing results ina number of applications. A comparison of GPto other methods for searching through the space of computer programs, such ashillclimbing and simulated annealing, is given byO'Reilly and Oppacher (1994). 
While the above example ofGP search is fairly simple, Koza etal. (1996) 
summarize the use ofa GPin several more complex tasks such as designingelectronic filter circuits and classifying segments of protein molecules. The fil- 
ter circuit design problem provides an example ofa considerably more complexproblem. Here, programs are evolved that transform a simple fixed seed circuitinto a final circuit design. The primitive functions used by the GPto construct itsprograms are functions that edit the seed circuit by inserting or deleting circuitcomponents and wiring connections. The fitness of each program is calculatedby simulating the circuit it outputs (using the SPICE circuit simulator) tode- 
termine how closely this circuit meets the design specifications for the desiredfilter. More precisely, the fitness score is the sum of the magnitudes of errorsbetween the desired and actual circuit output at 101 different input frequen- 
cies. In this case, a population of size 640,000 was maintained, with selectionproducing 10% of the successor population, crossover producing 89%, and mu- 
tation producing 1%. The system was executed ona 64-node parallel proces- 
sor. Within the first randomly generated population, the circuits produced wereso unreasonable that the SPICE simulator could not even simulate the behav- 
ior of 98% of the circuits. The percentage of unsimulatable circuits dropped to84.9% following the first generation, to 75.0% following the second generation, 
and toan average of 9.6% over succeeding generations. The fitness score of thebest circuit in the initial population was 159, compared toa score of 39 after20 generations and a score of 0.8 after 137 generations. The best circuit, pro- 
duced after 137 generations, exhibited performance very similar to the desiredbehavior. 
In most cases, the performance of genetic programming depends cruciallyon the choice of representation and on the choice of fitness function. For thisreason, an active area of current research is aimed at the automatic discoveryand incorporation of subroutines that improve on the original set of primitivefunctions, thereby allowing the system to dynamically alter the primitives fromwhich it constructs individuals. See, for example, Koza (1994). 
9.6 MODELS OF EVOLUTION AND LEARNINGIn many natural systems, individual organisms learn to adapt significantly duringtheir lifetime. At the same time, biological and social processes allow their speciesto adapt over a time frame of many generations. One interesting question regardingevolutionary systems is "What is the relationship between learning during thelifetime ofa single individual, and the longer time frame species-level learningafforded by evolution?' 
9.6.1 Lamarckian EvolutionLarnarck was a scientist who, in the late nineteenth century, proposed that evo- 
lution over many generations was directly influenced by the experiences of indi- 
vidual organisms during their lifetime. In particular, he proposed that experiencesof a single organism directly affected the genetic makeup of their offspring: Ifan individual learned during its lifetime to avoid some toxic food, it could passthis trait on genetically to its offspring, which therefore would not need to learnthe trait. This isan attractive conjecture, because it would presumably allow formore efficient evolutionary progress than a generate-and-test process (like that ofGAS and GPs) that ignores the experience gained during an individual's lifetime. 
Despite the attractiveness of this theory, current scientific evidence overwhelm- 
ingly contradicts Lamarck's model. The currently accepted view is that the geneticmakeup ofan individual is, in fact, unaffected by the lifetime experience of one'sbiological parents. Despite this apparent biological fact, recent computer studieshave shown that Lamarckian processes can sometimes improve the effectivenessof computerized genetic algorithms (see Grefenstette 1991; Ackley and Littman1994; and Hart and Belew 1995). 
9.6.2 Baldwin EffectAlthough Lamarckian evolution is not an accepted model of biological evolution, 
other mechanisms have been suggested by which individual learning can alterthe course of evolution. One such mechanism is called the Baldwin effect, afterJ. M. Baldwin (1896), who first suggested the idea. The Baldwin effect is basedon the following observations: 
0 Ifa species is evolving ina changing environment, there will be evolution- 
ary pressure to favor individuals with the capability to learn during theirlifetime. For example, ifa new predator appears in the environment, thenindividuals capable of learning to avoid the predator will be more successfulthan individuals who cannot learn. In effect, the ability to learn allows anindividual to perform a small local search during its lifetime to maximize itsfitness. In contrast, nonlearning individuals whose fitness is fully determinedby their genetic makeup will operate ata relative disadvantage. 
0 Those individuals who are able to learn many traits will rely less stronglyon their genetic code to "hard-wire" traits. Asa result, these individualscan support a more diverse gene pool, relying on individual learning toovercome the "missing" or "not quite optimized" traits in the genetic code. 
This more diverse gene pool can, in turn, support more rapid evolutionaryadaptation. Thus, the ability of individuals to learn can have an indirectaccelerating effect on the rate of evolutionary adaptation for the entire pop- 
ulation. 
To illustrate, imagine some new change in the environment of some species, 
such asa new predator. Such a change will selectively favor individuals capa- 
ble of learning to avoid the predator. As the proportion of such self-improvingindividuals in the population grows, the population will be able to support amore diverse gene pool, allowing evolutionary processes (even non-Lamarckiangenerate-and-test processes) to adapt more rapidly. This accelerated adaptationmay in turn enable standard evolutionary processes to more quickly evolve agenetic (nonlearned) trait to avoid the predator (e.g., an instinctive fear of thisanimal). Thus, the Baldwin effect provides an indirect mechanism for individ- 
ual learning to positively impact the rate of evolutionary progress. By increas- 
ing survivability and genetic diversity of the species, individual learning sup- 
ports more rapid evolutionary progress, thereby increasing the chance that thespecies will evolve genetic, nonlearned traits that better fit the new environ- 
ment. 
There have been several attempts to develop computational models to studythe Baldwin effect. For example, Hinton and Nowlan (1987) experimented withevolving a population of simple neural networks, in which some network weightswere fixed during the individual network "lifetime," while others were trainable. 
The genetic makeup of the individual determined which weights were train- 
able and which were fixed. In their experiments, when no individual learning268 MACHINE LEARNINGwas allowed, the population failed to improve its fitness over time. However, 
when individual learning was allowed, the population quickly improved its fit- 
ness. During early generations of evolution the population contained a greaterproportion of individuals with many trainable weights. However, as evolutionproceeded, the number of fixed, correct network weights tended to increase, asthe population evolved toward genetically given weight values and toward lessdependence on individual learning of weights. Additional computational stud- 
ies of the Baldwin effect have been reported by Belew (1990), Harvey (1993), 
and French and Messinger (1994). An excellent overview of this topic can befound in Mitchell (1996). A special issue of the journal Evolutionary Computa- 
tion on this topic (Turney etal. 1997) contains several articles on the Baldwineffect. 
9.7 PARALLELIZING GENETIC ALGORITHMSGAS are naturally suited to parallel implementation, and a number of approachesto parallelization have been explored. Coarse grain approaches to paralleliza- 
tion subdivide the population into somewhat distinct groups of individuals, calleddemes. Each deme is assigned toa different computational node, and a standardGA search is performed at each node. Communication and cross-fertilization be- 
tween demes occurs ona less frequent basis than within demes. Transfer betweendemes occurs bya migration process, in which individuals from one deme arecopied or transferred to other demes. This process is modeled after the kind ofcross-fertilization that might occur between physically separated subpopulationsof biological species. One benefit of such approaches is that it reduces the crowd- 
ing problem often encountered in nonparallel GAS, in which the system falls intoa local optimum due to the early appearance ofa genotype that comes to dominatethe entire population. Examples of coarse-grained parallel GAS are described byTanese (1989) and by Cohoon etal. (1987). 
In contrast to coarse-grained parallel implementations of GAS, fine-grainedimplementations typically assign one processor per individual in the population. 
Recombination then takes place among neighboring individuals. Several differ- 
ent types of neighborhoods have been proposed, ranging from planar grid totorus. Examples of such systems are described by Spiessens and Manderick 
(1991). An edited collection of papers on parallel GAS is available in Stender 
(1993). 
9.8 SUMMARY AND FURTHER READINGThe main points of this chapter include: 
0 Genetic algorithms (GAS) conduct a randomized, parallel, hill-climbingsearch for hypotheses that optimize a predefined fitness function. 
0 The search performed by GAS is based onan analogy to biological evolu- 
tion. A diverse population of competing hypotheses is maintained. At eachiteration, the most fit members of the population are selected to produce newoffspring that replace the least fit members of the population. Hypothesesare often encoded by strings that are combined by crossover operations, and . 
subjected to random mutations. 
a GAS illustrate how learning can be viewed asa special case of optimization. 
In particular, the learning task isto find the optimal hypothesis, according tothe predefined fitness function. This suggests that other optimization tech- 
niques such as simulated annealing can also be applied to machine learningproblems. 
a GAS have most commonly been applied to optimization problems outsidemachine learning, such as design optimization problems. When applied tolearning tasks, GAS are especially suited to tasks in which hypotheses arecomplex (e.g., sets of rules for robot control, or computer programs), andin which the objective tobe optimized may bean indirect function ofthe hypothesis (e.g., that the set of acquired rules successfully controls arobot). 
0 Genetic programming isa variant of genetic algorithms in which the hy- 
potheses being manipulated are computer programs rather than bit strings. 
Operations such as crossover and mutation are generalized to apply to pro- 
grams rather than bit strings. Genetic programming has been demonstratedto learn programs for tasks such as simulated robot control (Koza 1992) andrecognizing objects in visual scenes (Teller and Veloso 1994). 
Evolution-based computational approaches have been explored since theearly days of computer science (e.g., Box 1957 and Bledsoe 1961). Severaldifferent evolutionary approaches were introduced during the 1960s and havebeen further explored since that time. Evolution strategies, developed by Rechen- 
berg (1965, 1973) to optimize numerical parameters in engineering design, werefollowed upby Schwefel (1975, 1977, 1995) and others. Evolutionary program- 
ming, developed by Folgel, Owens, and Walsh (1966) asa method for evolv- 
ing finite-state machines, was followed upby numerous researchers (e.g., 
Fogel and Atmar 1993). Genetic algorithms, introduced by Holland (1962, 1975) 
included the notion of maintaining a large population of individuals and em- 
phasized crossover asa key operation in such systems. Genetic programming, 
introduced by Koza (1992), applies the search strategy of genetic algorithms tohypotheses consisting of computer programs. As computer hardware continues tobecome faster and less expensive, interest in evolutionary approaches continuesto grow. 
One approach to using GAS to learn sets of rules was developed byK. DeJong and his students at the University of Pittsburgh (e.g., Smith 1980). 
In this approach, each rule set is one member in the population of competinghypotheses, asin the GABIL system discussed in this chapter. A somewhat dif- 
ferent approach was developed at University of Michigan by Holland and hisstudents (Holland 1986), in which each rule isa member of the population, andthe population itself is the rule set. A biological perspective on the roles of muta- 
tion, inbreeding, cross-breeding, and selection in evolution is provided by Wright 
(1977). 
Mitchell (1996) and Goldberg (1989) are two textbooks devoted to the sub- 
ject of genetic algorithms. Forrest (1993) provides an overview of the technicalissues in GAS, and Goldberg (1994) provides an overview of several recent ap- 
plications. Koza's (1992) monograph on genetic programming is the standardreference for this extension of genetic algorithms to manipulation of computerprograms. The primary conference in which new results are published is the In- 
ternational Conference on Genetic Algorithms. Other relevant conferences includethe Conference on Simulation of Adaptive Behavior, the International Confer- 
ence on Artijicial Neural Networks and Genetic Algorithms, and the IEEE In- 
ternational Conference on Evolutionary Computation. An annual conference isnow held on genetic programming, as well (Koza etal. 1996b). The Evolution- 
ary Computation Journal is one source of recent research results in the field. 
Several special issues of the journal Machine Learning have also been devotedto GAS. 
EXERCISES9.1. Design a genetic algorithm to learn conjunctive classification rules for the Play- 
Tennis problem described in Chapter 3. Describe precisely the bit-string encodingof hypotheses and a set of crossover operators. 
9.2. Implement a simple GA for Exercise 9.1. Experiment with varying population size p, 
the fraction rof the population replaced at each generation, and the mutation rate m. 
9.3. Represent the program discovered by the GP (described in Section 9.5.2) asa tree. 
Illustrate the operation of the GP crossover operator by applying it using two copiesof your tree as the two parents. 
9.4. Consider applying GAS to the task of finding an appropriate set of weights foran artificial neural network (in particular, a feedforward network identical to thosetrained by BACKPROPAGATION (Chapter 4)). Consider a 3 x 2 x 1 layered, feedfor- 
ward network. Describe an encoding of network weights asa bit string, and describean appropriate set of crossover operators. Hint: Do not allow all possible crossoveroperations on bit strings. State one advantage and one disadvantage of using GASin contrast to BACKPROPAGATION to train network weights. 
REFERENCESAckley, D., & Littman, M. (1994). A case for Lamarckian evolution. InC. Langton (Ed.), Am$ciallife III. Reading, MA: Addison Wesley. 
Back, T. (1996). Evolutionary algorithms in theory andpractice. Oxford, England: Oxford UniversityPress. 
Baldwin, J. M. (1896). A new factor in evolution. American Naturalist, 3, 441-451, 536-553. 
http://www.santafe.edu/sfi/publications/BBelew, R. (1990). Evolution, learning, and culture: Computational metaphors for adaptive algorithms. 
Complex Systems, 4, 11-49. 
Belew, R. K., & Mitchell, M. (Eds.). (1996). Adaptive individuals in evolving populations: Modelsand algorithms. Reading, MA: Addison-Wesley. 
Bledsoe, W. (1961). The use of biological concepts in the analytical study of systems. Proceedingsof the ORSA-TIMS National Meeting, San Francisco. 
Booker, L. B., Goldberg, D. E., & Holland, J. H. (1989). Classifier systems and genetic algorithms. 
Artificial Intelligence, 40, 235-282. 
Box, G. (1957). Evolutionary operation: A method for increasing industrial productivity. Jountal ofthe Royal Statistical Society, 6(2), 81-101. 
Cohoon, J. P., Hegde, S. U., Martin, W. N., & Richards, D. (1987). Punctuated equilibria: A parallelgenetic algorithm. Proceedings of the Second International Conference on Genetic Algorithms 
(pp. 148-154). 
DeJong, K. A. (1975). An analysis of behavior ofa class of genetic adaptive systems (Ph.D. disser- 
tation). University of Michigan. 
DeJong, K. A., Spears, W. M., & Gordon, D. F. (1993). Using genetic algorithms for concept learning. 
Machine Learning, 13, 161-188. 
Folgel, L. J., Owens, A. J., & Walsh, M. J. (1966). Artificial intelligence through simulated evolution. 
New York: John Wiley & Sons. 
Fogel, L. J., & Atmar, W. (Eds.). (1993). Proceedings of the Second Annual Conference on Evolu- 
tionary Programming. Evolutionary Programming Society. 
Forrest, S. (1993). Genetic algorithms: Principles of natural selection applied to computation. Science, 
261, 872-878. 
French, R., & Messinger A. (1994). Genes, phenes, and the Baldwin effect: Learning and evolutionin a simulated population. InR. Brooks and P. Maes (Eds.), ArtiJicial Life IV. Cambridge, 
MA: MIT Press. 
Goldberg, D. (1989). Genetic algorithms in search, optimization, and machine learning. Reading, 
MA: Addison-Wesley. 
Goldberg, D. (1994). Genetic and evolutionary algorithms come of age. Communications of the ACM, 
37(3), 113-1 19. 
Green, D. P., & Smith, S. F. (1993). Competition based induction of decision models from examples. 
Machine Learning, 13,229-257. 
Grefenstette, J. J. (1988). Credit assignment in rule discovery systems based on genetic algorithms. 
Machine Learning, 3, 225-245. 
Grefenstette, J. J. (1991). Lamarckian learning in multi-agent environments. InR. Belew and L. 
Booker (Eds.), Proceedings of the Fourth International Conference on Genetic Algorithms. 
San Mateo, CA: Morgan Kaufmann. 
Hart, W., & Belew, R. (1995). Optimization with genetic algorithm hybrids that use local search. InR. Below and M. Mitchell (Eds.), Adaptive individuals in evolving populations: Models andalgorithms. Reading, MA: Addison-Wesley. 
Harvey, I. (1993). The puzzle of the persistent question marks: A case study of genetic drift. InForrest (Ed.), Proceedings of the Fzfth International Conference on Genetic Algorithms. SanMateo, CA: Morgan Kaufmann. 
Hinton, G. &, & Nowlan, S. J. (1987). How learning can guide evolution. Complex Systems, 1, 
495-502. 
Holland, J. H. (1962). Outline for a logical theory of adaptive systems. Journal of the Associationfor Computing Machinery, 3, 297-314. 
Holland, J. H. (1975). Adaptation in natural and art$cial systems. University of Michigan Press 
(reprinted in 1992 by MIT Press, Cambridge, MA). 
Holland, J. H. (1986). Escaping brittleness: The possibilities of general-purpose learning algorithmsapplied to parallel rule-based systems. InR. Michalski, J. Carbonell, & T. Mitchell (Eds.), 
Machine learning: An artijicial intelligence approach (Vol. 2). San Mateo, CA: Morgan Kauf- 
mann. 
Holland, J. H. (1989). Searching nonlinear functions for high values. Applied Mathematics and Com- 
putation, 32, 255-274. 
Janikow, C. Z. (1993). A knowledge-intensive GA for supervised learning. Machine Learning, 13, 
189-228. 
Koza, J. (1992). Genetic programming: On the programming of computers by means of natural se- 
lection. Cambridge, MA: MIT Press. 
Koza, J. R. (1994). Genetic Programming 11: Automatic discovery of reusable programs. Cambridge, 
MA: The MIT Press. 
Koza, J. R., Bennett 111, F. H., Andre, D., & Keane, M. A. (1996). Four problems for which acomputer program evolved by genetic programming is competitive with human performance. 
Proceedings of the 1996 IEEE International Conference on Evolutionary Computation (pp. 
1-10). IEEE Press. 
Koza, J. R., Goldberg, D. E., Fogel, D. B., & Riolo, R. L. (Eds.). (1996b). Genetic programming19%: Proceedings of the First Annual Conference. Cambridge, MA: MIT Press. 
Machine Learning: Special Issue on Genetic Algorithms (1988) 3:2-3, October. 
Machine Learning: Special Issue on Genetic Algorithms (1990) 5:4, October. 
Machine karning: Special Issue on Genetic Algorithms (1 993) l3:2,3, November. 
Mitchell, M. (1996). An introduction to genetic algorithms. Cambridge, MA: MIT Press. 
O'Reilly, U-M., & Oppacher, R. (1994). Program search with a hierarchical variable length repre- 
sentation: Genetic programming, simulated annealing, and hill climbing. InY. Davidor etal. 
(Eds.), Parallel problem solving from nature-PPSN I11 (Vol. 866) (Lecture notes in computerscience). Springer-Verlag. 
Rechenberg, I. (1965). Cybernetic solution path ofan experimental problem. Ministry of aviation, 
Royal Aircraft Establishment, U.K. 
Rechenberg, I. (1973). Evolutionsstrategie: Optimierung technischer systeme nach prinzipien derbiolgischen evolution. Stuttgart: Frommann-Holzboog. 
Schwefel, H. P. (1975). Evolutionsstrategie und numerische optimiemng (Ph.D. thesis). TechnicalUniversity of Berlin. 
Schwefel, H. P. (1977). Numerische optimierung von computer-modellen mittels der evolutionsstrate- 
gie. Basel: Birkhauser. 
Schwefel, H. P. (1995). Evolution and optimum seeking. New York: John Wiley & Sons. 
Spiessens, P., & Manderick, B. (1991). A massively parallel genetic algorithm: Implementation andfirst analysis. Proceedings of the 4th International Conference on Genetic Algorithms (pp. 
279-286). 
Smith, S. (1980). A learning system based on genetic adaptive algorithms (Ph.D. dissertation). Com- 
puter Science, University of Pittsburgh. 
Stender, J. (Ed.) (1993). Parallel genetic algorithms. Amsterdam: IOS Publishing. 
Tanese, R. (1989). Distributed genetic algorithms. Proceedings of the 3rd International Conferenceon Genetic Algorithms (pp. 434-439). 
Teller, A., & Veloso, M. (1994). PADO: A new learning architecture for object recognition. InK. 
Ikeuchi & M. Veloso (Eds.), Symbolic visual learning @p. 81-116). Oxford, England: OxfordUniv. Press. 
Turney, P. D. (1995). Cost-sensitive classification: Empirical evaluation ofa hybrid genetic decisiontree induction algorithm. Journal ofAl Research, 2, 369-409. http://www.cs.washington.edu/ 
research/jair/home.htmI. 
Tumey, P. D., Whitley, D., & Anderson, R. (1997). Evolutionary Computation. Special issue: 
The Baldwin effect, 4(3). Cambridge, MA: MIT Press. http://www-mitpress.mit.eduljmls- 
catalog/evolution-abstracts/evol.html. 
Whitley, L. D., & Vose, M. D. (Eds.). (1995). Foundations of genetic algorithms 3. Morgan Kauf- 
mann. 
Wright, S. (1977). Evolution and the genetics of populations. Vol. 4: Variability within and amongNatural Populations. Chicago: University of Chicago Press. 
Zbignlew, M. (1992). Genetic algorithms + data structures = evolution programs. Berlin: Springer- 
Verlag. 
CHAPTERLEARNINGSETS OF RULESOne of the most expressive and human readable representations for learned hypothe- 
ses is sets ofif-then rules. This chapter explores several algorithms for learning suchsets of rules. One important special case involves learning sets of rules containingvariables, called first-order Horn clauses. Because sets of first-order Horn clausescan be interpreted as programs in the logic programming language PROLOG, learningthem is often called inductive logic programming (ILP). This chapter examines sev- 
eral approaches to learning sets of rules, including an approach based on invertingthe deductive operators of mechanical theorem provers. 
10.1 INTRODUCTIONIn many cases itis useful to learn the target function represented asa set ofif-then rules that jointly define the function. As shown in Chapter 3, one way tolearn sets of rules isto first learn a decision tree, then translate the tree into anequivalent set of rules-one rule for each leaf node in the tree. A second method, 
illustrated in Chapter 9, isto use a genetic algorithm that encodes each rule setas a bit string and uses genetic search operators to explore this hypothesis space. 
In this chapter we explore a variety of algorithms that directly learn rule sets andthat differ from these algorithms in two key respects. First, they are designed tolearn sets of first-order rules that contain variables. This is significant becausefirst-order rules are much more expressive than propositional rules. Second, thealgorithms discussed here use sequential covering algorithms that learn one ruleat a time to incrementally grow the final set of rules. 
Asan example of first-order rule sets, consider the following two rulesthat jointly describe the target concept Ancestor. Here we use the predicateParent(x, y) to indicate that yis the mother or father ofx, and the predicateAncestor(x, y) to indicate that yis an ancestor ofx related byan arbitrary num- 
ber of family generations. 
IF Parent (x, y) THEN Ancestor(x,y) 
IF Parent(x, z) A Ancestor(z, y) THEN Ancestor(x, y) 
Note these two rules compactly describe a recursive function that would be verydifficult to represent using a decision tree or other propositional representation. 
One way to see the representational power of first-order rules isto consider thegeneral purpose programming language PROLOG. In PROLOG, programs are sets offirst-order rules such as the two shown above (rules of this form are also calledHorn clauses). In fact, when stated ina slightly different syntax the above rulesform a valid PROLOG program for computing the Ancestor relation. In this light, 
a general purpose algorithm capable of learning such rule sets may be viewedas an algorithm for automatically inferring PROLOG programs from examples. Inthis chapter we explore learning algorithms capable of learning such rules, givenappropriate sets of training examples. 
In practice, learning systems based on first-order representations have beensuccessfully applied to problems such as learning which chemical bonds fragmentin a mass spectrometer (Buchanan 1976; Lindsay 1980), learning which chemicalsubstructures produce mutagenic activity (a property related to carcinogenicity) 
(Srinivasan etal. 1994), and learning to design finite element meshes to analyzestresses in physical structures (Dolsak and Muggleton 1992). In each of theseapplications, the hypotheses that must be represented involve relational assertionsthat can be conveniently expressed using first-order representations, while theyare very difficult to describe using propositional representations. 
In this chapter we begin by considering algorithms that learn sets of propo- 
sitional rules; that is, rules without variables. Algorithms for searching the hy- 
pothesis space to learn disjunctive sets of rules are most easily understood inthis setting. We then consider extensions of these algorithms to learn first-orderrules. Two general approaches to inductive logic programming are then consid- 
ered, and the fundamental relationship between inductive and deductive inferenceis explored. 
10.2 SEQUENTIAL COVERING ALGORITHMSHere we consider a family of algorithms for learning rule sets based on the strategyof learning one rule, removing the data it covers, then iterating this process. Suchalgorithms are called sequential covering algorithms. To elaborate, imagine wehave a subroutine LEARN-ONE-RULE that accepts a set of positive and negativetraining examples as input, then outputs a single rule that covers many of thepositive examples and few of the negative examples. We require that this iaarputrule have high accuracy, but not necessarily high coverage. By high accuracy, wemean the predictions it makes should be correct. By accepting low coverage, wemean it need not make predictions for every training example. 
Given this LEARN-ONE-RULE subroutine for learning a single rule, one obvi- 
ous approach to learning a set of rules isto invoke LEARN-ONE-RULE on all theavailable training examples, remove any positive examples covered by the rule itlearns, then invoke it again to learn a second rule based on the remaining train- 
ing examples. This procedure can be iterated as many times as desired to learna disjunctive set of rules that together cover any desired fraction of the positiveexamples. This is called a sequential covering algorithm because it sequentiallylearns a set of rules that together cover the full set of positive examples. Thefinal set of rules can then be sorted so that more accurate rules will be consideredfirst when a new instance must be classified. A prototypical sequential coveringalgorithm is described in Table 10.1. 
This sequential covering algorithm is one of the most widespread approachesto learning disjunctive sets of rules. It reduces the problem of learning a disjunc- 
tive set of rules toa sequence of simpler problems, each requiring that a singleconjunctive rule be learned. Because it performs a greedy search, formulating asequence of rules without backtracking, itis not guaranteed to find the smallestor best set of rules that cover the training examples. 
How shall we design LEARN-ONE-RULE to meet the needs of the sequentialcovering algorithm? We require an algorithm that can formulate a single rulewith high accuracy, but that need not cover all of the positive examples. In thissection we present a variety of algorithms and describe the main variations thathave been explored in the research literature. In this section we consider learningonly propositional rules. In later sections, we extend these algorithms to learnfirst-order Horn clauses. 
SEQUENTIAL-COVERING(T~~~~~~~~~~~~~~, Attributes, Examples, Threshold) 
0 Learnedxules c {} 
0 Rule c ~~~~~-o~~-~~~~(Targetattribute, Attributes, Examples) 
0 while PERFORMANCE(RU~~, Examples) > Threshold, do0 Learned~ules c Learnedxules + Rule0 Examples c Examples - {examples correctly classified by Rule] 
0 Rule c ~~~~~-oN~-RuL~(Targetllttribute, Attributes, Examples) 
0 Learnedxules c sort Learned-rules accord to PERFORMANCE over Examples0 return LearnedxulesTABLE 10.1The sequential covering algorithm for learning a disjunctive set of rules. LEARN-ONE-RULE mustreturn a single rule that covers at least some of the Examples. PERFORMANCE isa user-providedsubroutine to evaluate rule quality. This covering algorithm learns rules until it can no longer learna rule whose performance is above the given Threshold. 
10.2.1 General to Specific Beam SearchOne effective approach to implementing LEARN-ONE-RULE isto organize the hy- 
pothesis space search in the same general fashion as the ID3 algorithm, but tofollow only the most promising branch in the tree at each step. As illustrated in thesearch tree of Figure 10.1, the search begins by considering the most general ruleprecondition possible (the empty test that matches every instance), then greed- 
ily adding the attribute test that most improves rule performance measured overthe training examples. Once this test has been added, the process is repeated bygreedily adding a second attribute test, and soon. Like ID3, this process grows thehypothesis by greedily adding new attribute tests until the hypothesis reaches anacceptable level of performance. Unlike ID3, this implementation of LEARN-ONE- 
RULE follows only a single descendant at each search step-the attribute-valuepair yielding the best performance-rather than growing a subtree that covers allpossible values of the selected attribute. 
This approach to implementing LEARN-ONE-RULE performs a general-to- 
specific search through the space of possible rules in search ofa rule with highaccuracy, though perhaps incomplete coverage of the data. Asin decision treelearning, there are many ways to define a measure to select the "best" descendant. 
To follow the lead of ID3 let us for now define the best descendant as the onewhose covered examples have the lowest entropy (recall Equation f3.31). 
The general-to-specific search suggested above for the LEARN-ONE-RULE al- 
gorithm isa greedy depth-first search with no backtracking. As with any greedyIFTHEN PlayTennis=yesIF Wind=strong tIF Humidity=highTHEN PlayTennis=noIF Hum'ditv=norntal THEN PlayTennis=noTHEN PlayTennis=yesIF Humidify=normalWind=weakTHEN PlayTennis=yes IF Humidity=normalA/\\----- IF Humidity=nowl ... Wind=strong IF Humidity=normal Outlook=rainTHEN PlayTennis=yes Outlook=sunny THEN PlnyTennis=yesTHEN PlayTennis=yesFIGURE 10.1The search for rule preconditions as LEARN-ONE-RULE proceeds from general to specific. At eachstep, the preconditions of the best rule are specialized in all possible ways. Rule postconditions aredetermined by the examples found to satisfy the preconditions. This figure illustrates a beam searchof width 1. 
search, there isa danger that a suboptimal choice will be made at any step. Toreduce this risk, we can extend the algorithm to perform a beam search; that is, 
a search in which the algorithm maintains a list of the k best candidates at eachstep, rather than a single best candidate. On each search step, descendants (spe- 
cializations) are generated for each of these k best candidates, and the resultingset is again reduced to the k most promising members. Beam search keeps trackof the most promising alternatives to the current top-rated hypothesis, so that allof their successors can be considered at each search step. This general to specificbeam search algorithm is used by the CN2 program described by Clark and Niblett 
(1989). The algorithm is described in Table 10.2. 
LEARN-ONE-RULE(T~~~~~~~~~~~U~~, Attributes, Examples, k) 
Returns a single rule that covers some of the Examples. Conducts a generalJotospec$cgreedy beam search for the best rule, guided by the PERFORMANCE metric. 
a Initialize Besthypothesis to the most general hypothesis 0a Initialize Candidatehypotheses to the set (Besthypothesis) 
a While Candidatehypotheses is not empty, DoI. Generate the next more spec@ candidatehypothesesa Allronstraints c the set of all constraints of the form (a = v), where ais a memberof Attributes, and vis a value ofa that occurs in the current set of Examplesa Newrandidatehypotheses cfor each hin Candidatehypotheses, 
for each cin Alll-onstraints, 
create a specialization ofh by adding the constraint ca Remove from Newl-andidatehypotheses any hypotheses that are duplicates, inconsis- 
tent, or not maximally specific2. Update Besthypothesisa For all hin Newnandidatehypotheses doa If PERFORMANCE(^, Examples, Targetattribute) 
z PERFORMANCE(Besthypothesis, Examples, Targetattribute)) 
Then Besthypothesis t h3. Update Candidatehypothesesa Candidatehypotheses c the k best members of New-candidatehypotheses, accordingto the PERFORMANCE measure. 
a Return a rule of the form 
"IF Best hypothesis THEN prediction" 
where prediction is the most frequent value of Targetattribute among those Examplesthat match Besthypothesis. 
PERM)RMANCE(~, Examples, Target attribute) 
a hxxamples t the subset of Examples that match hreturn -Entropy(hxxarnples), where entropy is with respect to TargetattributeTABLE 10.2One implementation for LEARN-ONE-RULE isa general-to-specific beam search. The frontier of currenthypotheses is represented by the variable Candidatehypotheses. This algorithm is similar to thatused by the CN2 program, described by Clark and Niblett (1989). 
A few remarks on the LEARN-ONE-RULE algorithm of Table 10.2 are in order. 
First, note that each hypothesis considered in the main loop of the algorithm isa conjunction of attribute-value constraints. Each of these conjunctive hypothesescorresponds toa candidate set of preconditions for the rule tobe learned and isevaluated by the entropy of the examples it covers. The search considers increas- 
ingly specific candidate hypotheses until it reaches a maximally specific hypothesisthat contains all available attributes. The rule that is output by the algorithm is therule encountered during the search whose PERFORMANCE is greatest-not necessar- 
ily the final hypothesis generated in the search. The postcondition for the outputrule is chosen only in the final step of the algorithm, after its precondition (rep- 
resented by the variable Besthypothesis) has been determined. The algorithmconstructs the rule postcondition to predict the value of the target attribute thatis most common among the examples covered by the rule precondition. Finally, 
note that despite the use of beam search to reduce the risk, the greedy search maystill produce suboptimal rules. However, even when this occurs the SEQUENTIAL- 
COVERING algorithm can still learn a collection of rules that together cover thetraining examples, because it repeatedly calls LEARN-ONE-RULE on the remaininguncovered examples. 
10.2.2 VariationsThe SEQUENTIAL-COVERING algorithm, together with the LEARN-ONE-RULE algo- 
rithm, learns a set ofif-then rules that covers the training examples. Many varia- 
tions on this approach have been explored. For example, in some cases it mightbe desirable to have the program learn only rules that cover positive examplesand to include a "default" that assigns a negative classification to instances notcovered by any rule. This approach might be desirable, say, if one is attemptingto learn a target concept such as "pregnant women who are likely to have twins." 
In this case, the fraction of positive examples in the entire population is small, sothe rule set will be more compact and intelligible to humans ifit identifies onlyclasses of positive examples, with the default classification of all other examplesas negative. This approach also corresponds to the "negation-as-failure" strategyof PROLOG, in which any expression that cannot be proven tobe true isby defaultassumed tobe false. In order to learn such rules that predict just a single targetvalue, the LEARN-ONE-RULE algorithm can be modified to accept an additional in- 
put argument specifying the target value of interest. The general-to-specific beamsearch is conducted just as before, changing only the PERFORMANCE subroutinethat evaluates hypotheses. Note the definition of PERFORMANCE as negative en- 
tropy isno longer appropriate in this new setting, because it assigns a maximalscore to hypotheses that cover exclusively negative examples, as well as thosethat cover exclusively positive examples. Using a measure that evaluates the frac- 
tion of positive examples covered by the hypothesis would be more appropriatein this case. 
Another variation is provided bya family of algorithms called AQ (Michal- 
ski 1969, Michalski etal. 1986), that predate the CN2 algorithm on which theabove discussion is based. Like CN2, AQ learns a disjunctive set of rules thattogether cover the target function. However, AQ differs in several ways fromthe algorithms given here. First, the covering algorithm ofAQ differs from theSEQUENTIAL-COVERING algorithm because it explicitly seeks rules that cover a par- 
ticular target value, learning a disjunctive set of rules for each target value inturn. Second, AQ's algorithm for learning a single rule differs from LEARN-ONE- 
RULE. While it conducts a general-to-specific beam search for each rule, it uses asingle positive example to focus this search. In particular, it considers only thoseattributes satisfied by the positive example asit searches for progressively morespecific hypotheses. Each time it learns a new rule it selects a new positive ex- 
ample from those that are not yet covered, to act asa seed to guide the search forthis new disjunct. 
10.3 LEARNING RULE SETS: SUMMARYThe SEQUENTIAL-COVERING algorithm described above and the decision tree learn- 
ing algorithms of Chapter 3 suggest a variety of possible methods for learningsets of rules. This section considers several key dimensions in the design spaceof such rule learning algorithms. 
First, sequential covering algorithms learn one rule ata time, removingthe covered examples and repeating the process on the remaining examples. Incontrast, decision tree algorithms such as ID3 learn the entire set of disjunctssimultaneously as part of the single search for an acceptable decision tree. Wemight, therefore, call algorithms such as ID3 simultaneous covering algorithms, incontrast to sequential covering algorithms such as CN2. Which should we prefer? 
The key difference occurs in the choice made at the most primitive step in thesearch. At each search step ID3 chooses among alternative attributes by com- 
paring the partitions of the data they generate. In contrast, CN2 chooses amongalternative attribute-value pairs, by comparing the subsets of data they cover. 
One way to see the significance of this difference isto compare the number ofdistinct choices made by the two algorithms in order to learn the same set ofrules. To learn a set ofn rules, each containing k attribute-value tests in theirpreconditions, sequential covering algorithms will perform n . k primitive searchsteps, making an independent decision to select each precondition of each rule. 
In contrast, simultaneous covering algorithms will make many fewer independentchoices, because each choice ofa decision node in the decision tree correspondsto choosing the precondition for the multiple rules associated with that node. Inother words, if the decision node tests an attribute that has m possible values, thechoice of the decision node corresponds to choosing a precondition for each of them corresponding rules (see Exercise 10.1). Thus, sequential covering algorithmssuch as CN2 make a larger number of independent choices than simultaneouscovering algorithms such as ID3. Still, the question remains, which should weprefer? The answer may depend on how much training data is available. If data isplentiful, then it may support the larger number of independent decisions requiredby the sequential covering algorithm, whereas if data is scarce, the "sharing" ofdecisions regarding preconditions of different rules may be more effective. Anadditional consideration is the task-specific question of whether itis desirablethat different rules test the same attributes. In the simultaneous covering deci- 
sion tree learning algorithms, they will. In sequential covering algorithms, theyneed not. 
A second dimension along which approaches vary is the direction of thesearch in LEARN-ONE-RULE. In the algorithm described above, the search is fromgeneral to specijic hypotheses. Other algorithms we have discussed (e.g., FIND-Sfrom Chapter 2) search from specijic to general. One advantage of general tospecific search in this case is that there isa single maximally general hypothesisfrom which to begin the search, whereas there are very many specific hypothesesin most hypothesis spaces (i.e., one for each possible instance). Given manymaximally specific hypotheses, itis unclear which to select as the starting point ofthe search. One program that conducts a specific-to-general search, called GOLEM 
(Muggleton and Feng 1990), addresses this issue by choosing several positiveexamples at random to initialize and to guide the search. The best hypothesisobtained through multiple random choices is then selected. 
A third dimension is whether the LEARN-ONE-RULE search isa generate thentest search through the syntactically legal hypotheses, asit isin our suggestedimplementation, or whether itis example-driven so that individual training exam- 
ples constrain the generation of hypotheses. Prototypical example-driven searchalgorithms include the FIND-S and CANDIDATE-ELIMINATION algorithms of Chap- 
ter 2, the AQ algorithm, and the CIGOL algorithm discussed later in this chapter. 
In each of these algorithms, the generation or revision of hypotheses is drivenby the analysis ofan individual training example, and the result isa revisedhypothesis designed to correct performance for this single example. This con- 
trasts to the generate and test search of LEARN-ONE-RULE in Table 10.2, in whichsuccessor hypotheses are generated based only on the syntax of the hypothesisrepresentation. The training data is considered only after these candidate hypothe- 
ses are generated and is used to choose among the candidates based on theirperformance over the entire collection of training examples. One important ad- 
vantage of the generate and test approach is that each choice in the search isbased on the hypothesis performance over many examples, so that the impactof noisy data is minimized. In contrast, example-driven algorithms that refinethe hypothesis based on individual examples are more easily misled bya sin- 
gle noisy training example and are therefore less robust to errors in the trainingdata. 
A fourth dimension is whether and how rules are post-pruned. Asin decisiontree learning, itis possible for LEARN-ONE-RULE to formulate rules that performvery well on the training data, but less well on subsequent data. Asin decisiontree learning, one way to address this issue isto post-prune each rule after itis learned from the training data. In particular, preconditions can be removedfrom the rule whenever this leads to improved performance over a set of pruningexamples distinct from the training examples. A more detailed discussion of rulepost-pruning is provided in Section 3.7.1.2. 
A final dimension is the particular definition of rule PERFORMANCE used toguide the search in LEARN-ONE-RULE. Various evaluation functions have been used. 
Some common evaluation functions include: 
0 Relative frequency. Let n denote the number of examples the rule matchesand let nc denote the number of these that it classifies correctly. The relativefrequency estimate of rule performance isRelative frequency is used to evaluate rules in the AQ program. 
0 m-estimate of accuracy. This accuracy estimate is biased toward the defaultaccuracy expected of the rule. Itis often preferred when data is scarce andthe rule must be evaluated based on few examples. As above, let n and ncdenote the number of examples matched and correctly predicted by the rule. 
Let pbe the prior probability that a randomly drawn example from the entiredata set will have the classification assigned by the rule (e.g., if 12 out of100 examples have the value predicted by the rule, then p = .12). Finally, 
let mbe the weight, or equivalent number of examples for weighting thisprior p. The m-estimate of rule accuracy isNote ifm is set to zero, then the m-estimate becomes the above relative fre- 
quency estimate. Asm is increased, a larger number of examples is neededto override the prior assumed accuracy p. The m-estimate measure is advo- 
cated by Cestnik and Bratko (1991) and has been used in some versions ofthe CN2 algorithm. Itis also used in the naive Bayes classifier discussed inSection 6.9.1. 
0 Entropy. This is the measure used by the PERFORMANCE subroutine in thealgorithm of Table 10.2. Let Sbe the set of examples that match the rulepreconditions. Entropy measures the uniformity of the target function valuesfor this set of examples. We take the negative of the entropy so that betterrules will have higher scores. 
C 
-Entropy (S) = pi logl piwhere cis the number of distinct values the target function may take on, 
and where piis the proportion of examples from S for which the targetfunction takes on the ith value. This entropy measure, combined with a testfor statistical significance, is used in the CN2 algorithm of Clark and Niblett 
(1989). Itis also the basis for the information gain measure used by manydecision tree learning algorithms. 
10.4 LEARNING FIRST-ORDER RULESIn the previous sections we discussed algorithms for learning sets of propositional 
(i.e., variable-free) rules. In this section, we consider learning rules that con- 
tain variables-in particular, learning first-order Horn theories. Our motivationfor considering such rules is that they are much more expressive than proposi- 
tional rules. Inductive learning of first-order rules or theories is often referred toas inductive logic programming (orLP for short), because this process can beviewed as automatically inferring PROLOG programs from examples. PROLOG is ageneral purpose, Turing-equivalent programming language in which programs areexpressed as collections of Horn clauses. 
10.4.1 First-Order Horn ClausesTo see the advantages of first-order representations over propositional (variable- 
free) representations, consider the task of learning the simple target conceptDaughter (x, y), defined over pairs of people x and y. The value of Daughter(x, y) 
is True when xis the daughter ofy, and False otherwise. Suppose each personin the data is described by the attributes Name, Mother, Father, Male, Female. 
Hence, each training example will consist of the description of two people interms of these attributes, along with the value of the target attribute Daughter. 
For example, the following isa positive example in which Sharon is the daughterof Bob: 
(Namel = Sharon, Motherl = Louise, Fatherl = Bob, 
Malel = False, Female1 = True, 
Name2 = Bob, Mother2 = Nora, Father2 = Victor, 
Male2 = True, Female2 = False, Daughterl.2 = True) 
where the subscript on each attribute name indicates which of the.two persons isbeing described. Now ifwe were to collect a number of such training examples forthe target concept Daughterlv2 and provide them toa propositional rule learnersuch as CN2 or C4.5, the result would bea collection of very specific rulessuch asIF (Father1 = Bob) A (Name2 = Bob) A (Femalel = True) 
THEN daughter^,^ = TrueAlthough itis correct, this rule isso specific that it will rarely, if ever, be useful inclassifying future pairs of people. The problem is that propositional representationsoffer no general way to describe the essential relations among the values of theattributes. In contrast, a program using first-order representations could learn thefollowing general rule: 
IF Father(y, x) r\ Female(y), THEN Daughter(x, y) 
where x and y are variables that can be bound to any person. 
First-order Horn clauses may also refer to variables in the preconditions thatdo not occur in the postconditions. For example, one rule for GrandDaughtermight beIF Father(y, z) A Mother(z, x) A Female(y) 
THEN GrandDaughter(x, y) 
Note the variable zin this rule, which refers to the father ofy, is not present in therule postconditions. Whenever such a variable occurs only in the preconditions, 
itis assumed tobe existentially quantified; that is, the rule preconditions aresatisfied as long as there exists at least one binding of the variable that satisfiesthe corresponding literal. 
Itis also possible to use the same predicates in the rule postconditions andpreconditions, enabling the description of recursive rules. For example, the tworules at the beginning of this chapter provide a recursive definition of the conceptAncestor (x, y). ILP learning methods such-as those described below have beendemonstrated to learn a variety of simple recursive functions, such as the aboveAncestor function, and functions for sorting the elements ofa list, removing aspecific element from a list, and appending two lists. 
POs4.2 TerminologyBefore moving onto algorithms for learning sets of Horn clauses, let us intro- 
duce some basic terminology from formal logic. All expressions are composedof constants (e.g., Bob, Louise), variables (e.g., x, y), predicate symbols (e.g., 
Married, Greater-Than), and function symbols (e.g., age). The difference be- 
tween predicates and functions is that predicates take on values of True or False, 
whereas functions may take on any constant as their value. We will use lowercasesymbols for variables and capitalized symbols for constants. Also, we will uselowercase for functions and capitalized symbols for predicates. 
From these symbols, we build up expressions as follows: A term is any con- 
stant, any variable, or any function applied to any term (e.g., Bob, x, age(Bob)). 
A literal is any predicate or its negation applied to any term (e.g., Married(Bob, 
Louise), -Greater-Than(age(Sue), 20)). Ifa literal contains a negation (1) sym- 
bol, we call ita negative literal, otherwise a positive literal. 
A clause is any disjunction of literals, where all variables are assumed to beuniversally quantified. A Horn clause isa clause containing at most one positiveliteral, such aswhere His the positive literal, and -Ll . . . -Ln are negative literals. Because ofthe equalities (Bv -A) = (Bt A) and -(AA B) = (-Av -B), the above Hornclause can alternatively be written in the formEvery well-formed expression is composed of constants (e.g., Mary, 23, or Joe), variables (e.g., 
x), predicates (e.g., Female, asin Female(Mary)), and functions (e.g., age, asin age(Mary)). 
A term is any constant, any variable, or any function applied to any term. Examples include Mary, 
x, age(Mary), age(x). 
A literal is any predicate (or its negation) applied to any set of terms. Examples includeFemal e(Mary), - Female(x), Greaterf han (age(Mary), 20). 
A ground literal isa literal that does not contain any variables (e.g., -Female(Joe)). 
A negative literal isa literal containing a negated predicate (e.g., -Female(Joe)). 
A positive literal isa literal with no negation sign (e.g., Female(Mary)). 
A clause is any disjunction of literals M1 v . . . Mn whose variables are universally quantified. 
A Horn clause isan expression of the formwhere H, L1 . . . Ln are positive literals. His called the head or consequent of the Horn clause. 
The conjunction of literals L1 A L2 A .. .AL, is called the body or antecedents of the Horn clause. 
For any literals A and B, the expression (At B) is equivalent to (Av -B), and the expression 
-(AA B) is equivalent to (-Av -B). Therefore, a Horn clause can equivalently be written as thedisjunctionHv-L1 v...v-L, 
A substitution is any function that replaces variables by terms. For example, the substitution 
{x/3, y/z) replaces the variable xby the term 3 and replaces the variable yby the term z. Givena substitution 0 and a literal Lwe write LOto denote the result of applying substitution 0 toL. 
A unrfying substitution for two literals L1 and L2 is any substitution 0 such that L10 = L1B. 
TABLE 10.3Basic definitions from first-order logic. 
which is equivalent to the following, using our earlier rule notationIF L1 A ... AL,, THEN HWhatever the notation, the Horn clause preconditions L1 A . . . AL, are called theclause body or, alternatively, the clause antecedents. The literal H that forms thepostcondition is called the clause head or, alternatively, the clause consequent. 
For easy reference, these definitions are summarized in Table 10.3, along withother definitions introduced later in this chapter. 
10.5 LEARNING SETS OF FIRST-ORDER RULES: FOILA variety of algorithms has been proposed for learning first-order rules, or Hornclauses. In this section we consider a program called FOIL (Quinlan 1990) thatemploys an approach very similar to the SEQUENTIAL-COVERING and LEARN-ONE- 
RULE algorithms of the previous section. In fact, the FOIL program is the naturalextension of these earlier algorithms to first-order representations. Formally, thehypotheses learned by FOIL are sets of first-order rules, where each rule is sim- 
ilar toa Horn clause with two exceptions. First, the rules learned by FOIL aremore restricted than general Horn clauses, because the literals are not pennittedto contain function symbols (this reduces the complexity of the hypothesis spacesearch). Second, FOIL rules are more expressive than Horn clauses, because theliterals appearing in the body of the rule may be negated. FOIL has been appliedto a variety of problem domains. For example, it has been demonstrated to learn arecursive definition of the QUICKSORT algorithm and to learn to discriminate legalfrom illegal chess positions. 
The FOIL algorithm is summarized in Table 10.4. Notice the outer loopcorresponds toa variant of the SEQUENTIAL-COVERING algorithm discussed earlier; 
that is, it learns new rules one ata time, removing the positive examples covered bythe latest rule before attempting to learn the next rule. The inner loop correspondsto a variant of our earlier LEARN-ONE-RULE algorithm, extended to accommodatefirst-order rules. Note also there are a few minor differences between FOIL andthese earlier algorithms. In particular, FOIL seeks only rules that predict whenthe target literal is True, whereas our earlier algorithm would seek both rulesthat predict when itis True and rules that predict when itis False. Also, FOILperforms a simple hillclimbing search rather than a beam search (equivalently, ituses a beam of width one). 
The hypothesis space search performed by FOIL is best understood by view- 
ing it hierarchically. Each iteration through FOIL'S outer loop adds a new rule toits disjunctive hypothesis, Learned~ules. The effect of each new rule isto gen- 
-- 
FOIL(Target-predicate, Predicates, Examples) 
Pos c those Examples for which the Target-predicate is TrueNeg c those Examples for which the Target-predicate is Falsewhile Pos, doLearn a NewRuleNew Rule t the rule that predicts Target-predicate with no preconditionsNewRuleNeg t Negwhile NewRuleNeg, doAdd a new literal to specialize New RuleCandidateliterals t generate candidate new literals for NewRule, based onPredicatesBestliteralt argmax Foil-Gain(L,NewRule) 
LECandidateliteralsadd Bestliteral to preconditions of NewRuleNewRuleNeg c subset of NewRuleNeg that satisfies NewRule preconditionsLearned~ules c Learned-rules + NewRulePos t Pos - {members of Pos covered by NewRule) 
Return Learned-rulesTABLE 10.4The basic FOIL algorithm. The specific method for generating Candidateliterals and the defini- 
~ 
tion of Foil-Gain are given in the text. This basic algorithm can be modified slightly to betteraccommodate noisy data, as described in the text. 
eralize the current disjunctive hypothesis (i.e., to increase the number of instancesit classifies as positive), by adding a,new disjunct. Viewed at this level, the searchis a specific-to-general search through the space of hypotheses, beginning with themost specific empty disjunction and terminating when the hypothesis is sufficientlygeneral to cover all positive training examples. The inner loop of FOIL performs afiner-grained search to determine the exact definition of each new rule. This innerloop searches a second hypothesis space, consisting of conjunctions of literals, tofind a conjunction that will form the preconditions for the new rule. Within thishypothesis space, it conducts a general-to-specific, hill-climbing search, beginningwith the most general preconditions possible (the empty precondition), then addingliterals one ata time to specialize the rule until it avoids all negative examples. 
The two most substantial differences between FOIL and our earlierSEQUENTIAL-COVERING and LEARN-ONE-RULE algorithm follow from the require- 
ment that it accommodate first-order rules. These differences are: 
1. In its general-to-specific search to 'learn each new rule, FOIL employs dif- 
ferent detailed steps to generate candidate specializations of the rule. Thisdifference follows from the need to accommodate variables in the rule pre- 
conditions. 
2. FOIL employs a PERFORMANCE measure, Foil-Gain, that differs from theentropy measure shown for LEARN-ONE-RULE in Table 10.2. This differencefollows from the need to distinguish between different bindings of the rulevariables and from the fact that FOIL seeks only rules that cover positiveexamples. 
The following two subsections consider these two differences in greaterdetail. 
10.5.1 Generating Candidate Specializations in FOILTo generate candidate specializations of the current rule, FOIL generates a varietyof new literals, each of which may be individually added to the rule preconditions. 
More precisely, suppose the current rule being considered iswhere L1.. . L, are literals forming the current rule preconditions and whereP(x1, x2, . . . , xk) is the literal that forms the rule head, or postconditions. FOILgenerates candidate specializations of this rule by considering new literals L,+Ithat fit one of the following forms: 
Q(vl, . . . , v,), where Qis any predicate name occurring in Predicates andwhere the vi are either new variables or variables already present in the rule. 
At least one of the viin the created literal must already exist asa variablein the rule. 
a Equal(xj, xk), where xi and xk are variables already present in the rule. 
0 The negation of either of the above forms of literals. 
To illustrate, consider learning rules to predict the target literal Grand- 
Daughter(x, y), where the other predicates used to describe examples are Fatherand Female. The general-to-specific search in FOIL begins with the most generalruleGrandDaughter(x, y) twhich asserts that GrandDaughter(x, y) is true of any x and y. To specializethis initial rule, the above procedure generates the following literals as candi- 
date additions to the rule preconditions: Equal (x, y) , Female(x), Female(y), 
Father(x, y), Father(y, x), Father(x, z), Father(z, x), Father(y, z), Father- 
(z, y), and the negations of each of these literals (e.g., -Equal(x, y)). Note thatz isa new-variable here, whereas x and y exist already within the current rule. 
Now suppose that among the above literals FOIL greedily selects Father- 
(y, z) as the most promising, leading to the more specific ruleGrandDaughter(x, y) t Father(y, z) 
In generating candidate literals to further specialize this rule, FOIL will now con- 
sider all of the literals mentioned in the previous step, plus the additional literalsFemale(z), Equal(z, x), Equal(z, y), Father(z, w), Father(w, z), and their nega- 
tions. These new literals are considered at this point because the variable z wasadded to the rule in the previous step. Because of this, FOIL now considers anadditional new variable w. 
If FOIL at this point were to select the literal Father(z, x) and on thenext iteration select the literal Female(y), this would lead to the following rule, 
which covers only positive examples and hence terminates the search for furtherspecializations of the rule. 
At this point, FOIL will remove all positive examples covered by this newrule. If additional positive examples remain tobe covered, then it will begin yetanother general-to-specific search for an additional rule. 
10.5.2 Guiding the Search in FOILTo select the most promising literal from the candidates generated at each step, 
FOIL considers the performance of the rule over the training data. In doing this, 
it considers all possible bindings of each variable in the current rule. To illustratethis process, consider again the example in which we seek to learn a set of rulesfor the target literal GrandDaughter(x, y). For illustration, assume the trainingdata includes the following simple set of assertions, where we use the conventionthat P(x, y) can be read as "The Pof xis y ." 
GrandDaughter(Victor, Sharon) Father(Sharon, Bob) Father(Tom, Bob) 
Female(Sharon) Father(Bob, Victor) 
Here let us also make the closed world assumption that any literal involving thepredicate GrandDaughter, Father, or Female and the constants Victor, Sharon, 
Bob, and Tom that is not listed above can be assumed tobe false (i.e., we also im- 
plicitly assert -.GrandDaughter(Tom, Bob), -GrandDaughter(Victor, Victor), 
etc.). 
To select the best specialization of the current rule, FOIL considers eachdistinct way in which the rule variables can bind to constants in the trainingexamples. For example, in the initial step when the rule isthe rule variables x and y are not constrained by any preconditions and maytherefore bind in any combination to the four constants Victor, Sharon, Bob, andTom. We will use the notation {x/Bob, y/Shar on} to denote a particular variablebinding; that is, a substitution mapping each variable toa constant. Given the fourpossible constants, there are 16 possible variable bindings for this initial rule. Thebinding {xlvictor, ylSharon} corresponds toa positive example binding, be- 
cause the training data includes the assertion GrandDaughter(Victor, Sharon). 
The other 15 bindings allowed by the rule (e.g., the binding {x/Bob, y/Tom}) 
constitute negative evidence for the rule in the current example, because no cor- 
responding assertion can be found in the training data. 
At each stage, the rule is evaluated based on these sets of positive and neg- 
ative variable bindings, with preference given to rules that possess more positivebindings and fewer negative bindings. As new literals are added to the rule, thesets of bindings will change. Note ifa literal is added that introduces a newvariable, then the bindings for the rule will grow in length (e.g., if Father(y, z) 
is added to the above rule, then the original binding {xlvictor, y/Sharon) willbecome the more lengthy {xlvictor, ylSharon, z/Bob}. Note also that if the newvariable can bind to several different constants, then the number of bindings fittingthe extended rule can be greater than the number associated with the original rule. 
The evaluation function used by FOIL to estimate the utility of adding anew literal is based on the numbers of positive and negative bindings coveredbefore and after adding the new literal. More precisely, consider some rule R, anda candidate literal L that might be added to the body ofR. Let R' be the rulecreated by adding literal Lto rule R. The value Foil-Gain(L, R) of adding L toR is defined asP1 ) (10.1) Foil -Gain(L, R) = t - - log2 - 
P1+ nlPO + nowhere pois the number of positive bindings of rule R, nois the number ofnegative bindings ofR, plis the number of positive bindings of rule R', andnl is the number of negative bindings ofR'. Finally, tis the number of positivebindings of rule R that are still covered after adding literal Lto R. When a newvariable is introduced into Rby adding L, then any original binding is consideredto be covered so long as some binding extending itis present in the bindingsof R'. 
This Foil-Gain function has a straightforward interpretation in terms ofinformation theory. According to information theory, - log2 --/& is the minimumnumber of bits needed to encode the classification ofan arbitrary positive bindingamong the bindings covered by rule R. Similarly, -log2 Ais the numberof bits required if the binding is one of those covered by rule R'. Since t isjust the number of positive bindings covered byR that remain covered byR', 
Foil-Gain(L, R) can be seen as the reduction due toL in the total number ofbits needed to encode the classification of all positive bindings ofR. 
10.5.3 Learning Recursive Rule SetsIn the above discussion, we ignored the possibility that new literals added to therule body could refer to the target predicate itself (i.e., the predicate occurringin the rule head). However, ifwe include the target predicate in the input list ofPredicates, then FOIL will consider itas well when generating candidate literals. 
This will allow itto form recursive rules-rules that use the same predicate inthe body and the head of the rule. For instance, recall the following rule set thatprovides a recursive definition of the Ancestor relation. 
IF Parent (x, y) THEN Ancestor(x, y) 
IF Parent (x, z) A Ancestor(z, y) THEN Ancestor@, y) 
Given an appropriate set of training examples, these two rules can be learnedfollowing a trace similar to the one above for GrandDaughter. Note the secondrule is among the rules that are potentially within reach of FOIL'S search, providedAncestor is included in the list Predicates that determines which predicates maybe considered when generating new literals. Of course whether this particularrule would be learned or not depends on whether these particular literals outscorecompeting candidates during FOIL'S greedy search for increasingly specific rules. 
Cameron-Jones and Quinlan (1993) discuss several examples in which FOIL hassuccessfully discovered recursive rule sets. They also discuss important subtletiesthat arise, such as how to avoid learning rule sets that produce infinite recursion. 
10.5.4 Summary of FOILTo summarize, FOIL extends the sequential covering algorithm of CN2 to handlethe case of learning first-order rules similar to Horn clauses. To learn each ruleFOIL performs a general-to-specific search, at each step adding a single new literalto the rule preconditions. The new literal may refer to variables already mentionedin the rule preconditions or postconditions, and may introduce new variables aswell. At each step, it uses the Foil-Gain function of Equation (10.1) to selectamong the candidate new literals. If new literals are allowed to refer to the targetpredicate, then FOIL can, in principle, learn sets of recursive rules. While this in- 
troduces the complexity of avoiding rule sets that result in infinite recursion, FOILhas been demonstrated to successfully learn recursive rule sets in several cases. 
In the case of noise-free training data, FOIL may continue adding new literalsto the rule until it covers no negative examples. To handle noisy data, the searchis continued until some tradeoff occurs between rule accuracy, coverage, andcomplexity. FOIL uses a minimum description length approach to halt the growthof rules, in which new literals are added only when their description length isshorter than the description length of the training data they explain. The detailsof this strategy are given in Quinlan (1990). In addition, FOIL post-prunes eachrule it learns, using the same rule post-pruning strategy used for decision trees 
(Chapter 3). 
10.6 INDUCTION AS INVERTED DEDUCTIONA second, quite different approach to inductive logic programming is based onthe simple observation that induction is just the inverse of deduction! In general, 
machine learning involves building theories that explain the observed data. Givensome data D and some partial background knowledge B, learning can be describedas generating a hypothesis h that, together with B, explains D. Put more precisely, 
assume as usual that the training data Dis a set of training examples, each ofthe form (xi, f (xi)). Here xi denotes the ith training instance and f (xi) denotesits target value. Then learning is the problem of discovering a hypothesis h, suchthat the classification f (xi) of each training instance xi follows deductively fromthe hypothesis h, the description ofxi, and any other background knowledge Bknown to the system. 
(V(xi, f (xi)) ED) (BAh Axi) f (xi) (10.2) 
The expression XF Yis read "Y follows deductively from X," or alternatively 
"X entails Y." Expression (10.2) describes the constraint that must be satisfiedby the learned hypothesis h; namely, for every training instance xi, the targetclassification f (xi) must follow deductively from B, h, and xi. 
Asan example, consider the case where the target concept tobe learned is 
"pairs of people (u, v) such that the child ofu isv," represented by the predicateChild(u, v). Assume we are given a single positive example Child(Bob, Sharon), 
where the instance is described by the literals Male(Bob), Female(Sharon), andFather(Sharon, Bob). Furthermore, suppose we have the general backgroundknowledge Parent (u, v) t Father (u, v). We can describe this situation in theterms of Equation (10.2) as follows: 
xi : Male(Bob), Female(Sharon), Father(Sharon, Bob) 
f (xi) : Child(Bob, Sharon) 
In this case, two of the many hypotheses that satisfy the constraint (BAh Axi) t- 
f (xi) arehl : Child(u, v) t Father(v, u) 
h2 : Child(u, v) t Parent (v, u) 
Note that the target literal Child(Bob, Sharon) is entailed byhl AX^ with no needfor the background information B. In the case of hypothesis h2, however, thesituation is different. The target Child(Bob, Sharon) follows from B ~h2 AX^, butnot from h2 AX^ alone. This example illustrates the role of background knowledgein expanding the set of acceptable hypotheses for a given set of training data. It alsoillustrates how new predicates (e.g., Parent) can be introduced into hypotheses 
(e.g., h2), even when the predicate is not present in the original description of theinstance xi. This process of augmenting the set of predicates, based on backgroundknowledge, is often referred toas constructive induction. 
The significance of Equation (10.2) is that it casts the learning problem in theframework of deductive inference and formal logic. In the case of propositionaland first-order logics, there exist well-understood algorithms for automated deduc- 
tion. Interestingly, itis possible to develop inverses of these procedures in orderto automate the process of inductive generalization. The insight that inductionmight be performed by inverting deduction appears to have been first observedby the nineteenth century economist W. S. Jevons, who wrote: 
Induction is, in fact, the inverse operation of deduction, and cannot be con- 
ceived to exist without the corresponding operation, so that the question of relativeimportance cannot arise. Who thinks of asking whether addition or subtraction isthe more important process in arithmetic? But at the same time much difference indifficulty may exist between a direct and inverse operation; . . . it must be allowedthat inductive investigations are ofa far higher degree of difficulty and complexitythan any questions of deduction.. . . (Jevons 1874) 
In the remainder of this chapter we will explore this view of inductionas the inverse of deduction. The general issue we will be interested in here isdesigning inverse entailment operators. An inverse entailment operator, O(B, D) 
takes the training data D = {(xi, f (xi))} and background knowledge Bas inputand produces as output a hypothesis h satisfying Equation (10.2). 
O(B, D) = h such that (V(xi, f (xi)) ED) (B ~hA xi) Ff (xi) 
Of course there will, in general, be many different hypotheses h that satisfy 
(V(X~, f (xi)) ED) (BA hA xi) Ff (xi). One common heuristic in ILP for choos- 
ing among such hypotheses isto rely on the heuristic known as the MinimumDescription Length principle (see Section 6.6). 
There are several attractive features to formulating the learning task as find- 
ing a hypothesis h that solves the relation (V(xi, f (xi)) ED) (BA hA xi) Ff (xi). 
0 This formulation subsumes the common definition of learning as findingsome general concept that matches a given set of training examples (whichcorresponds to the special case where no background knowledge Bis avail- 
able). 
0 By incorporating the notion of background information B, this formulationallows a more rich definition of when a hypothesis may be said to "fit" 
the data. Up until now, we have always determined whether a hypothesis 
(e.g., neural network) fits the data based solely on the description of thehypothesis and data, independent of the task domain under study. In contrast, 
this formulation allows the domain-specific background information B tobecome part of the definition of "fit." In particular, h fits the training example 
(xi, f (xi)) as long asf (xi) follows deductively from BA hA xi. 
0 By incorporating background information B, this formulation invites learningmethods that use this background information to guide the search for h, 
rather than merely searching the space of syntactically legal hypotheses. 
The inverse resolution procedure described in the following section usesbackground knowledge in this fashion. 
At the same time, research on inductive logic programing following thisformulation has encountered several practical difficulties. 
a The requirement @'(xi, f (xi)) ED) (BA hA xi) tf (xi) does not naturallyaccommodate noisy training data. The problem is that this expression doesnot allow for the possibility that there may be errors in the observed de- 
scription of the instance xior its target value f (xi). Such errors can producean inconsistent set of constraints onh. Unfortunately, most formal logicframeworks completely lose their ability to distinguish between truth andfalsehood once they are given inconsistent sets of assertions. 
0 The language of first-order logic isso expressive, and the number ofhy- 
potheses that satisfy (V(xi , f (xi)) ED) (BA hA xi) tf (xi) isSO large, 
that the search through the space of hypotheses is intractable in the generalcase. Much recent work has sought restricted forms of first-order expres- 
sions, or additional second-order knowledge, to improve the tractability ofthe hypothesis space search. 
0 Despite our intuition that background knowledge B should help constrainthe search for a hypothesis, in most ILP systems (including all discussedin this chapter) the complexity of the hypothesis space search increases asbackground knowledge Bis increased. (However, see Chapters 11 and 12 foralgorithms that use background knowledge to decrease rather than increasesample complexity). 
In the following section, we examine one quite general inverse entailmentoperator that constructs hypotheses by inverting a deductive inference rule. 
10.7 INVERTING RESOLUTIONA general method for automated deduction is the resolution rule introduced byRobinson (1965). The resolution rule isa sound and complete rule for deductiveinference in first-order logic. Therefore, itis sensible to ask whether we can invertthe resolution rule to form an inverse entailment operator. The answer is yes, andit is just this operator that forms the basis of the CIGOL program introduced byMuggleton and Buntine (1988). 
Itis easiest to introduce the resolution rule in propositional form, though it isreadily extended to first-order representations. Let Lbe an arbitrary propositionalliteral, and let P and Rbe arbitrary propositional clauses. The resolution rule isPVL 
-Lv RPVRwhich should be read as follows: Given the two clauses above the line, concludethe clause below the line. Intuitively, the resolution rule is quite sensible. Giventhe two assertions Pv L and -Lv R, itis obvious that either Lor -L must befalse. Therefore, either Por R must be true. Thus, the conclusion Pv Rof theresolution rule is intuitively satisfying. 
The general form of the propositional resolution operator is described inTable 10.5. Given two clauses C1 and C2, the resolution operator first identifiesa literal L that occurs asa positive literal in one of these two clauses and asa negative literal in the other. It then draws the conclusion given by the aboveformula. For example, consider the application of the resolution operator illustratedon the left side of Figure 10.2. Given clauses C1 and C2, the first step of theprocedure identifies the literal L = -KnowMaterial, which is present in C1, andwhose negation -(-KnowMaterial) = KnowMaterial is present in C2. Thus theconclusion is the clause formed by the union of the literals C1- (L} = Pass Examand C2 - (-L} = -Study. As another example, the result of applying the resolutionrule to the clauses C1 = Av Bv Cv -D and C2 = -Bv Ev Fis the clauseAvCV-DvEvF. 
Itis easy to invert the resolution operator to form an inverse entailmentoperator O(C, C1) that performs inductive inference. In general, the inverse en- 
tailment operator must derive one of the initial clauses, C2, given the resolvent Cand the other initial clause C1. Consider an example in which we are given theresolvent C = Av B and the initial clause C1 = Bv D. How can we derive aclause C2 such that C1 A C2 FC? First, note that by the definition of the resolutionoperator, any literal that occurs inC but not in C1 must have been present in C2. 
In our example, this indicates that C2 must contain the literal A. Second, the literal1. Given initial clauses C1 and C2, find a literal L from clause C1 such that -L occurs in clause C2. 
2. Form the resolvent Cby including all literals from C1 and C2, except for L and -L. Moreprecisely, the set of literals occurring in the conclusion C iswhere u denotes set union, and "-" denotes set difference. 
TABLE 10.5Resolution operator (propositional form). Given clauses C1 and C2, the resolution operator constructsa clause C such that C1 A C2 kC. 
C : KnowMaterial v -Study C: KnowMaterial V 7SNdyC : Passh v ~KnawMaferial C : PIISS~ V 1KnowMafcrialI IFIGURE 10.2On the left, an application of the (deductive) resolution rule inferring clause C from the given clausesC1 and C2. On the right, an application of its (inductive) inverse, inferring Cz from C and C1. 
that occurs in C1 but not inC must be the literal removed by the resolution rule, 
and therefore its negation must occur in C2. In our example, this indicates that C2must contain the literal -D. Hence, C:! = Av -D. The reader can easily verifythat applying the resolution rule to C1 and C2 does, in fact, produce the desiredresolvent C. 
Notice there isa second possible solution for C2 in the above example. Inparticular, C2 can also be the more specific clause Av -Dv B. The differencebetween this and our first solution is that we have now included in C2 a lit- 
eral that occurred in C1. The general point here is that inverse resolution is notdeterministic-in general there may be multiple clauses C2 such that C1 and C2produce the resolvent C. One heuristic for choosing among the alternatives is toprefer shorter clauses over longer clauses, or equivalently, to assume C2 shares noliterals in common with C1. Ifwe incorporate this bias toward short clauses, thegeneral statement of this inverse resolution procedure isas shown in Table 10.6. 
We can develop rule-learning algorithms based on inverse entailment op- 
erators such as inverse resolution. In particular, the learning algorithm can useinverse entailment to construct hypotheses that, together with the backgroundinformation, entail the training data. One strategy isto use a sequential cover- 
ing algorithm to iteratively learn a set of Horn clauses in this way. On eachiteration, the algorithm selects a training example (xi, f (xi)) that is not yet cov- 
ered by previously learned clauses. The inverse resolution rule is then applied to 
- -- - - 
1. Given initial clauses C1 and C, find a literal L that occurs in clause C1, but not in clause C. 
2. Form the second clause Czby including the following literalsTABLE 10.6Inverse resolution operator (propositional form). Given two clauses C and Cl. this computes a clauseC2 such that C1 ACz I- C. 
generate candidate hypotheses hi that satisfy (BA hiA xi) I- f (xi), where Bis thebackground knowledge plus any clauses learned on previous iterations. Note thisis an example-driven search, because each candidate hypothesis is constructed tocover a particular example. Of course if multiple candidate hypotheses exist, thenone strategy for selecting among them isto choose the one with highest accuracyover the other examples as well. The CIGOL program uses inverse resolution withthis kind of sequential covering algorithm, interacting with the user along theway to obtain training examples and to obtain guidance in its search through thevast space of possible inductive inference steps. However, CIGOL uses first-orderrather than propositional representations. Below we describe the extension of theresolution rule required to accommodate first-order representations. 
10.7.1 First-Order ResolutionThe resolution rule extends easily to first-order expressions. Asin the propositionalcase, it takes two clauses as input and produces a third clause as output. The keydifference from the propositional case is that the process is now based on thenotion of unifying substitutions. 
We define a substitution tobe any mapping of variables to terms. For ex- 
ample, the substitution 6 = {x/Bob, y/z} indicates that the variable xis to bereplaced by the term Bob, and that the variable yis tobe replaced by the termz. We use the notation WOto denote the result of applying the substitution 6 tosome expression W. For example, ifL is the literal Father(x, Bill) and 6 is thesubstitution defined above, then LO = Father(Bob, Bill). 
We say that 6 isa unifying substitution for two literals L1 and L2, providedLlO = L2O. For example, if L1 = Father(x, y), L2 = Father(Bil1, z), and O = 
(x/Bill, z/y}, then 6 isa unifying substitution for L1 and L2 because LlO = 
L2O = Father(Bil1, y). The significance ofa unifying substitution is this: In thepropositional form of resolution, the resolvent of two clauses C1 and C2 is foundby identifying a literal L that appears in C1 such that -L appears in C2. In first- 
order resolution, this generalizes to finding one literal L1 from clause C1 and oneliteral L2 from C2, such that some unifying substitution 6 can be found for L1and -L2 (i.e., such that LIO = -L20). The resolution rule then constructs theresolvent C according to the equationThe general statement of the resolution rule is shown in Table 10.7. Toillustrate, suppose C1 = White(x) t Swan(x) and suppose C2 = Swan(Fred). 
To apply the resolution rule, we first re-express C1 in clause form as the equivalentexpression C1 = White(x) v -Swan(x). The resolution rule can now be applied. 
In the first step, it finds the literal L1 = -Swan(x) from C1 and the literal L2 = 
Swan(Fred) from C2. Ifwe choose the unifying substitution O = {x/Fred} thenthese two literals satisfy LIB = -L20 = -Swan(Fred). Therefore, the conclusionC is the union of (C1 - {L1})O = White(Fred) and (C2 - {L2})0 = 0, orC = 
White(Fred). 
CHAPTER 10 LEARNING SETS OF RULES 2!)71. Find a literal L1 from clause C1, literal Lz from clause Cz, and substitution 0 such that LIB = 
-L28. 
2. Form the resolvent Cby including all literals from CIB and C28, except for L1 B and -L2B. Moreprecisely, the set of literals occurring in the conclusion C isc = (Cl - (L11)OlJ (C2 - ILzI)@ 
TABLE 10.7Resolution operator (first-order form). 
10.7.2 Inverting Resolution: First-Order CaseWe can derive the inverse resolution operator analytically, by algebraic manipula- 
tion of Equation (10.3) which defines the resolution rule. First, note the unifyingsubstitution 8 in Equation (10.3) can be uniquely factored into 81 and 82, where0 = Ole2, where contains all substitutions involving variables from clause C1, 
and where O2 contains all substitutions involving variables from C2. This factor- 
ization is possible because C1 and C2 will always begin with distinct variablenames (because they are distinct universally quantified statements). Using thisfactorization of 8, we can restate Equation (10.3) asKeep in mind that "-" here stands for set difference. Now ifwe restrict inverseresolution to infer only clauses C2 that contain no literals in common with C1 
(corresponding toa preference for shortest C2 clauses), then we can re-expressthe above asc - (Cl - {LlHel = (C2 - IL2W2Finally we use the fact that by definition of the resolution rule L2 = -~1818;', 
and solve for C2 to obtainInverse resolution: 
cz = (c - (CI - {~~~)e~)e,-lu {-~,e~e;'~ (10.4) 
Equation (10.4) gives the inverse resolution rule for first-order logic. Asin thepropositional case, this inverse entailment operator is nondeterministic. In partic- 
ular, in applying itwe may in general find multiple choices for the clause Cr tobe resolved and for the unifying substitutions and 82. Each set of choices mayyield a different solution for C2. 
Figure 10.3 illustrates a multistep application of this inverse resolution rulefor a simple example. In this figure, we wish to learn rules for the target predicateGrandChild(y, x), given the training data D = GrandChild(Bob, Shannon) andthe background information B = {Father (Shannon, Tom), Father (Tom, Bob)). 
Consider the bottommost step in the inverse resolution tree of Figure 10.3. Here, 
we set the conclusion Cto the training example GrandChild(Bob, Shannon) 
GrandChild(Bob, Shannon) 
Father (Shannon, Tom) 
FIGURE 10.3A multistep inverse resolution. In each case, the boxed clause is the result of the inference step. Foreach step, Cis the clause at the bottom, C1 the clause to the left, and C2 the boxed clause to theright. In both inference steps here, elis the empty substitution (1, and 0;' is the substitution shownbelow C2. Note the final conclusion (the boxed clause at the top right) is the alternative form of theHorn clause GrandChild(y, x) c Father(x, z) A Father(z, y). 
GrandChild(Bob,x) v Father(x,Tom) Iand select the clause C1 = Father(Shannon, Tom) from the background in- 
formation. To apply the inverse resolution operator we have only one choicefor the literal L1, namely Father(Shannon, Tom). Suppose we choose the in- 
verse substitutions 9;' = {} and 9;' = {Shannon/x}. In this case, the result- 
ing clause C2 is the union of the clause (C - (C1 - {Ll})91)9;~ = (~91)9;' 
= GrandChild(Bob, x), and the clause {-~~9~9,') = -.Father(x, Tom). Hencethe result is the clause GrandChild(Bob, x) v -Father(x, Tom), or equivalently 
(GrandChild(Bob, x) t Father(x, Tom)). Note this general rule, together withC1 entails the training example GrandChild(Bob, Shannon). 
In similar fashion, this inferred clause may now be used as the conclusionC for a second inverse resolution step, as illustrated in Figure 10.3. At each suchstep, note there are several possible outcomes, depending on the choices for thesubstitutions. (See Exercise 10.7.) In the example of Figure 10.3, the particular setof choices produces the intuitively satisfying final clause GrandChild(y, x) tFather(x, 2) A Father(z, y). 
10.7.3 Summary of Inverse ResolutionTo summarize, inverse resolution provides a general approach to automaticallygenerating hypotheses h that satisfy the constraint (BA hA xi) t- f (xi). This isaccomplished by inverting the general resolution rule given by Equation (10.3). 
Beginning with the resolution rule and solving for the clause C2, the inverseresolution rule of Equation (10.4) is easily derived. 
Given a set of beginning clauses, multiple hypotheses may be generated byrepeated application of this inverse resolution rule. Note the inverse resolution rulehas the advantage that it generates only hypotheses that satisfy (B ~hAX^) t- f (xi). 
In contrast, the generate-and-test search of FOIL generates many hypotheses ateach search step, including some that do not satisfy this constraint. FOIL thenconsiders the data Dto choose among these hypotheses. Given this difference, 
we might expect the search based on inverse resolution tobe more focused andefficient. However, this will not necessarily be the case. One reason is that theinverse resolution operator can consider only a small fraction of the availabledata when generating its hypothesis at any given step, whereas FOIL considersall available data to select among its syntactically generated hypotheses. Thedifferences between search strategies that use inverse entailment and those thatuse generate-and-test search isa subject of ongoing research. Srinivasan etal. 
(1995) provide one experimental comparison of these two approaches. 
10.7.4 Generalization, 8-Subsumption, and EntailmentThe previous section pointed out the correspondence between induction and in- 
verse entailment. Given our earlier focus on using the general-to-specific orderingto organize the hypothesis search, itis interesting to consider the relationship be- 
tween the more-general~han relation and inverse entailment. To illuminate thisrelationship, consider the following definitions. 
0 more-general-than. In Chapter 2, we defined the more_general_than_or- 
equal20 relation (z,) as follows: Given two boolean-valued functions hj(x) 
and hk(x), we say that hj 2, hkif and only if (Vx)hk(x) + hj(x). This >, 
relation is used by many learning algorithms to guide search through thehypothesis space. 
0 8-subsumption. Consider two clauses Cj and Ck, both of the form Hv L1 v 
. . . L,, where His a positive literal, and the Li are arbitrary literals. ClauseCj is said to 8-subsume clause Ckif and only if there exists a substitution0 such that CjO GCk (where we here describe any clause Cby the set ofliterals in its disjunctive form). This definition is due to Plotkin (1970). 
0 Entailment. Consider two clauses Cj and Ck. Clause Cjis said to entailclause Ck (written Cjk Ck) if and only ifCk follows deductively from C,. 
What is the relationship among these three definitions? First, let usre-expressthe definition of 2, using the same first-order notation as the other two definitions. 
Ifwe consider a boolean-valued hypothesis h(x) for some target concept c(x), 
where h(x) is expressed bya conjunction of literals, then we can re-express thehypothesis as the clauseHere we follow the usual PROLOG interpretation that xis classified a negativeexample ifit cannot be proven tobe a positive example. Hence, we can see thatour earlier definition of 1, applies to the preconditions, or bodies, of Horn clauses. 
The implicit postcondition of the Horn clause is the target concept c(x). 
What is the relationship between this definition of 2, and the definitionof 8-subsumption? Note that ifhl p, h2, then the clause C1 : c(x) thl(x) 
8-subsumes the clause C2 : c(x) t h2(x). Furthermore, 8-subsumption can holdeven when the clauses have different heads. For example, clause A 8-subsumesclause Bin the following case: 
A : Mother(x, y) t Father(x, z) A Spouse(z, y) 
B : Motker(x, Louise) t Father(x, Bob) A Spouse(Bob, y) A Female@) 
because A8 GB ifwe choose 8 = {ylLouise, zlBob). The key difference here isthat >, implicitly assumes two clauses for which the heads are the same, whereas8-subsumption can hold even for clauses with different heads. 
Finally, 8-subsumption isa special case of entailment. That is, if clause A8-subsumes clause B, then Ak B. However, we can find clauses A and B suchthat AF B, but where A does not 8-subsume B. One example is the followingpair of clausesA : Elephant(father_of (x)) t Elephant (x) 
B : Elephant (f athersf (f ather-f (y))) t Elephant (y) 
where f ather-of (x) isa function that refers to the individual who is the fatherof x. Note that although B can be proven from A, there isno substitution 8 thatallows Bto be &subsumed byA. 
As shown by these examples, our earlier notion of more-general~han is aspecial case of 8-subsumption, which is itself a special case of entailment. There- 
fore, searching the hypothesis space by generalizing or specializing hypothesesis more limited than searching by using general inverse entailment operators. 
Unfortunately, in its most general form, inverse entailment produces intractablesearches. However, the intermediate notion of 8-subsumption provides one conve- 
nient notion that lies midway between our earlier definition of more-general~hanand entailment. 
Although inverse resolution isan intriguing method for generating candidate hy- 
potheses, in practice it can easily lead toa combinatorial explosion of candidatehypotheses. An alternative approach isto use inverse entailment to generate justthe single most specific hypothesis that, together with the background informa- 
tion, entails the observed data. This most specific hypothesis can then be usedto bound a general-to-specific search through the hypothesis space similar to thatused by FOIL, but with the additional constraint that the only hypotheses consid- 
ered are hypotheses more general than this bound. This approach is employed bythe PROGOL system, whose algorithm can be summarized as follows: 
1. The user specifies a restricted language of first-order expressions tobe usedas the hypothesis space H. Restrictions are stated using "mode declarations," 
which enable the user to specify the predicate and function symbols to beconsidered, and the types and formats of arguments for each. 
2. PROGOL uses a sequential covering algorithm to learn a set of expressionsfrom H that cover the data. For each example (xi, f (xi)) that is not yetcovered by these learned expressions, it first searches for the most specifichypothesis hi within H such that (BA hiA xi) l- f (xi). More precisely, itapproximates this by calculating the most specific hypothesis among thosethat entail f (xi) within k applications of the resolution rule (where kis auser-specified parameter). 
3. PROGOL then performs a general-to-specific search of the hypothesis spacebounded by the most general possible hypothesis and by the specific boundhi calculated in step 2. Within this set of hypotheses, it seeks the hypothesishaving minimum description length (measured by the number of literals). 
This part of the search is guided byan A*-like heuristic that allows pruningwithout running the risk of pruning away the shortest hypothesis. 
The details of the PROGOL algorithm are described by Muggleton (1992, 
1995). 
10.8 SUMMARY AND FURTHER READINGThe main points of this chapter include: 
The sequential covering algorithm learns a disjunctive set of rules by firstlearning a single accurate rule, then removing the positive examples coveredby this rule and iterating the process over the remaining training examples. 
It provides an efficient, greedy algorithm for learning rule sets, and anal- 
ternative to top-down decision tree learning algorithms such as ID3, whichcan be viewed as simultaneous, rather than sequential covering algorithms. 
0 In the context of sequential covering algorithms, a variety of methods havebeen explored for learning a single rule. These methods vary in the searchstrategy they use for examining the space of possible rule preconditions. Onepopular approach, exemplified by the CN2 program, isto conduct a general- 
to-specific beam search, generating and testing progressively more specificrules until a sufficiently accurate rule is found. Alternative approaches searchfrom specific to general hypotheses, use an example-driven search rather thangenerate and test, and employ different statistical measures of rule accuracyto guide the search. 
Sets of first-order rules (i.e., rules containing variables) provide a highlyexpressive representation. For example, the programming language PROLOGrepresents general programs using collections of first-order Horn clauses. 
The problem of learning first-order Horn clauses is therefore often referredto as the problem of inductive logic programming. 
One approach to learning sets of first-order rules isto extend the sequentialcovering algorithm of CN2 from propositional to first-order representations. 
This approach is exemplified by the FOIL program, which can learn sets offirst-order rules, including simple recursive rule sets. 
0 A second approach to learning first-order rules is based on the observationthat induction is the inverse of deduction. In other words, the problem ofinduction isto find a hypothesis h that satisfies the constraintwhere Bis general background information, XI. . . x, are descriptions of theinstances in the training data D, and f (XI). . . f (x,) are the target values ofthe training instances. 
0 Following the view of induction as the inverse of deduction, some programssearch for hypotheses by using operators that invert the well-known opera- 
tors for deductive reasoning. For example, CIGOL uses inverse resolution, anoperation that is the inverse of the deductive resolution operator commonlyused for mechanical theorem proving. PROGOL combines an inverse entail- 
ment strategy with a general-to-specific strategy for searching the hypothesisspace. 
Early work on learning relational descriptions includes Winston's (1970) 
well-known program for learning network-style descriptions for concepts suchas "arch." Banerji7s (1964, 1969) work and Michalski7s series ofAQ programs 
(e.g., Michalski 1969; Michalski etal. 1986) were among the earliest toex- 
plore the use of logical representations in learning. Plotkin's (1970) definition of8-subsumption provided an early formalization of the relationship between induc- 
tion and deduction. Vere (1975) also explored learning logical representations, 
and Buchanan's (1976) META-DENDRAL program learned relational descriptionsrepresenting molecular substructures likely to fragment ina mass spectrometer. 
This program succeeded in discovering useful rules that were subsequently pub- 
lished in the chemistry literature. Mitchell's (1979) CANDIDATE-ELIMINATION ver- 
sion space algorithm was applied to these same relational descriptions of chemicalstructures. 
With the popularity of the PROLOG language in the mid-1980~~ researchersbegan to look more carefully at learning relational descriptions represented byHorn clauses. Early work on learning Horn clauses includes Shapiro's (1983) 
MIS and Sammut and Banerji's (1986) MARVIN. Quinlan7s (1990) FOIL algo- 
rithm, discussed here, was quickly followed bya number of algorithms employ- 
ing a general-to-specific search for first-order rules including MFOIL (Dieroski1991), FOCL (Pazzani etal. 1991), CLAUDIEN (De Raedt and Bruynooghe1993), and MARKUS (Grobelnik 1992). The FOCL algorithm is described inChapter 12. 
An alternative line of research on learning Horn clauses by inverse entail- 
ment was spurred by Muggleton and Buntine (1988), who built on related ideasby Sammut and Banerji (1986) and Muggleton (1987). More recent work alongthis line has focused on alternative search strategies and methods for constrainingthe hypothesis space to make learning more tractable. For example, Kietz andWrobel (1992) use rule schemata in their RDT program to restrict the form ofexpressions that may be considered, during learning, and Muggleton and Feng 
(1992) discuss the restriction of first-order expressions toij-determinate literals. 
Cohen (1994) discusses the GRENDEL program, which accepts as input anex- 
plicit description of the language for describing the clause body, thereby allowingthe user to explicitly constrain the hypothesis space. 
LavraC and DZeroski (1994) provide a very readable textbook on inductivelogic programming. Other useful recent monographs and edited collections include 
(Bergadano and Gunetti 1995; Morik etal. 1993; Muggleton 1992, 1995b). Theoverview chapter by Wrobel(1996) also provides a good perspective on the field. 
Bratko and Muggleton (1995) summarize a number of recent applications of ILPto problems of practical importance. A series of annual workshops on ILP providesa good source of recent research papers (e.g., see De Raedt 1996). 
EXERCISES10.1. Consider a sequential covering algorithm such as CN2 and a simultaneous coveringalgorithm such as ID3. Both algorithms are tobe used to learn a target conceptdefined over instances represented by conjunctions ofn boolean attributes. If ID3learns a balanced decision tree of depth d, it will contain 2d - 1 distinct decisionnodes, and therefore will have made 2d - 1 distinct choices while constructing itsoutput hypothesis. How many rules will be formed if this tree isre-expressed ast a disjunctive set of rules? How many preconditions will each ru?e possess? Howmany distinct choices would a sequential covering algorithm have to make to learnthis same set of rules? Which system do you suspect would be more prone tooverfitting if both were given the same training data? 
10.2. Refine the LEARN-ONE-RULE algorithm of Table 10.2 so that it can learn rules whosepreconditions include thresholds on real-valued attributes (e.g., temperature > 
42). Specify your new algorithm asa set of editing changes to the algorithm ofTable 10.2. Hint: Consider how this is accomplished for decision tree learning. 
10.3. Refine the LEARN-ONE-RULE algorithm of Table 10.2 so that it can learn rules whosepreconditions include constraints such as nationality E {Canadian, Brazilian}, 
where a discrete-valued attribute is allowed to take on any value in some specifiedset. Your modified program should explore the hypothesis space containing all suchsubsets. Specify your new algorithm asa set of editing changes to the algorithmof Table 10.2. 
10.4. Consider the options for implementing LEARN-ONE-RULE in terms of the possiblestrategies for searching the hypothesis space. In particular, consider the followingattributes of the search 
(a) generate-and-test versus data-driven 
(b) general-to-specific versus specific-to-general 
(c) sequential cover versus simultaneous coverDiscuss the benefits of the choice made by the algorithm in Tables 10.1 and10.2. For each of these three attributes of the search strategy, discuss the (positiveand negative) impact of choosing the alternative option. 
10.5. Apply inverse resolution in propositional form to the clauses C = Av B, C1 = 
Av Bv G. Give at least two possible results for CZ. 
10.6. Apply inverse resolution to the clauses C = R(B, x) vP(x, A) and CI = S(B, y) vR(z, x). Give at least four possible results for C2. Here A and B are constants, xand y are variables. 
10.7. Consider the bottom-most inverse resolution step in Figure 10.3. Derive at leasttwo different outcomes that could result given different choices for the substi- 
tutions el and 02. Derive a result for the inverse resolution step if the clauseFather(Tom, Bob) is used in place of Father(Shannon, Tom). 
10.8. Consider the relationship between the definition of the induction problem in thischapterand our earlier definition of inductive bias from Chapter 2, Equation 2.1. There wedefined the inductive bias, Bbias, by the expressionwhere L(xi, D) is the classification that the learner assigns to the new instance xiafter learning from the training data D, and where Xis the entire instance space. 
Note the first expression is intended to describe the hypothesis we wish the learnerto output, whereas the second expression is intended to describe the learner's policyfor generalizing beyond the training data. Invent a learner for which the inductivebias Bbias of the learner is identical to the background knowledge B that it isprovided. 
REFERENCESBanerji, R. (1964). A language for the description of concepts. General Systems, 9, 135-141. 
Bane rji, R. (1969). Theory of problem solving-an approach to artijicial intelligence. New York: 
American Elsevier Publishing Company. 
Bergadano, F., & Gunetti, D. (1995). Inductive logic programming: From machine learning to soft- 
ware engineering. Cambridge, Ma: MIT Press. 
Bratko, I., & Muggleton, S. (1995). Applications of inductive logic programming. Communicationsof the ACM, 38(1 I), 65-70. 
Buchanan, B. G., Smith, D. H., White, W. C., Gritter, R., Feigenbaum, E. A., Lederberg, J., & 
Djerassi, C. (1976). Applications of artificial intelligence for chemical inference, XXII: Auto- 
matic rule formation in mass spectrometry by means of the meta-DENDRAL program. Journalof the American Chemical Society, 98, 6168. 
Buntine, W. (1986). Generalised subsumption. Proceedings of the European Conference on ArtijicialIntelligence, London. 
Buntine, W. (1988). Generalized subsumption and its applications to induction and redundancy. 
Artificial Intelligence, 36, 149-176. 
Cameron-Jones, R., & Quinlan, J. R. (1993). Avoiding pitfalls when learning recursive theories. 
Proceedings of the Eighth International Workshop on Machine Learning (pp 389-393). SanMatw, CA: Morgan Kaufmann. 
Cestnik, B., & Bratko, I. (1991). On estimating probabilities in tree pruning. Proceedings of theEuropean Working Session on Machine Learning @p. 138-150). Porto, Portugal. 
Clark, P., & Niblett, R. (1989). The CN2 induction algorithm. Machine Learning, 3, 261-284. 
Cohen, W. (1994). Grammatically biased learning: Learning logic programs using an explicit an- 
tecedent description language. ArtGcial Intelligence, 68(2), 303-366. 
De Raedt, L. (1992). Interactive theory revision: An inductive logic programming approach. London: 
Academic Press. 
De Raedt, L., & Bruynooghe, M. (1993). A theory of clausal discovery. Proceedings of the ThirteenthInternational Joint Conference on ArtGcial Intelligence. San Mateo, CA: Morgan Kaufmann. 
De Raedt, L. (Ed.). (1996). Advancm in inductive logic programming: Proceedings of the Fifh In- 
ternational Workshop on Inductive Logic Programming. Amsterdam: IOS Press. 
Dolsak, B., & Muggleton, S. (1992). The application of inductive logic programming to finite elementmesh design. InS. Muggleton (Ed.), Inductive Logic Programming. London: Academic Press. 
DZeroski, S. (1991). Handling noise in inductive logic programming (Master's thesis). ElectricalEngineering and Computer Science, University of Ljubljana, Ljubljana, Slovenia. 
Flener, P. (1994). Logic program synthesis from incomplete information. The Kluwer internationalseries in engineering and computer science. Boston: Kluwer Academic Publishers. 
Grobelnik, M. (1992). MARKUS: An optimized model inference system. Proceedings of the Work- 
shop on Logical Approaches to Machine Learning, Tenth European Conference onAI, Vienna, 
Austria. 
Jevons, W. S. (1874). The principles of science: A treatise on logic and scientijc method. London: 
Macmillan. 
Kietz, J-U., & Wrobel, S. (1992). Controlling the complexity of learning in logic through syntactic andtask-oriented models. InS. Muggleton (Ed.), Inductive logic programming. London: AcademicPress. 
LavraE, N., & Dieroski, S. (1994). Inductive logicprogramming: Techniques and applications. EllisHorwood. 
Lindsay, R. K., Buchanan, B. G., Feigenbaurn, E. A., & Lederberg, J. (1980). Applications of art@cialintelligence for organic chemistry. New York: McGraw-Hill. 
Michalski, R. S., (1969). On the quasi-minimal solution of the general covering problem. Proceed- 
ings of the First International Symposium on Information Processing (pp. 125-128). Bled, 
Yugoslavia. 
Michalski, R. S., Mozetic, I., Hong, J., and Lavrac, H. (1986). The multi-purpose incremental learningsystem AQ15 and its testing application to three medical domains. Proceedings of the FifhNational Conference on A1 (pp. 1041-1045). Philadelphia: Morgan-Kaufmann. 
Mitchell, T. M. (1979). Version spaces: An approach to concept learning (Ph.D. dissertation). Elec- 
trical Engineering Dept., Stanford University, Stanford, CA. 
Morik, K., Wrobel, S., Kietz, J.-U., & Emde, W. (1993). Knowledge acquisition and machine learning: 
Theory, methods, and applications. London: Academic Press. 
Muggleton, S. (1987). DUCE: An oracle based approach to constructive induction. Proceedings of theInternational Joint Conference onAI @p. 287-292). San Mateo, CA: Morgan Kaufmann. 
Muggleton, S. (1995a). Inverse entailment and PROGOL. New Generation Computing, 13, 245-286. 
Muggleton, S. (1995b). Foundations of inductive logic programming. Englewood Cliffs, NJ: PrenticeHall. 
Muggleton, S., & Buntine, W. (1988). Machine invention of first-order predicates by inverting res- 
olution. Proceedings of the Fzfth International Machine Learning Conference (pp. 339-352). 
Ann Arbor, Michigan: Morgan Kaufmann. 
Muggleton, S., & Feng, C. (1990). Efficient induction of logic programs. Proceedings of the FirstConference on Algorithmic Learning Theory. Ohrnsha, Tokyo. 
Muggleton, S., & Feng, C. (1992). Efficient induction of logic programs. In Muggleton (Ed.), Induc- 
tive logic programming. London: Academic Press. 
Muggleton, S. (Ed.). (1992). Inductive logic programming. London: Academic Press. 
Pazzani, M., Brunk, C., & Silverstein, G. (1991). A knowledge-intensive approach to learning rela- 
tional concepts. Proceedings of the Eighth International Workshop on Machine Learning (pp. 
432-436). San Francisco: Morgan Kaufmann. 
Plotkin, G. D. (1970). A note on inductive generalization. InB. Meltzer & D. Michie (Eds.), MachineIntelligence 5 (pp. 153-163). Edinburgh University Press. 
Plotkin, G. D. (1971). A further note on inductive generalization. InB. Meltzer & D. Michie (Eds.), 
Machine Intelligence 6. New York: Elsevier. 
Quinlan, J. R. (1990). Learning logical definitions from relations. Machine Learning, 5, 239-266. 
Quinlan, J. R. (1991). Improved estimates for the accuracy of small disjuncts (Technical Note). 
Machine Learning, 6(1), 93-98. Boston: Kluwer Academic Publishers. 
Rivest R. L. (1987). Learning decision lists. Machine Learning, 2(3), 229-246. 
Robinson, J. A. (1965). A machine-oriented logic based on the resolution principle. Journal of theACM, 12(1), 23-41. 
Sammut, C. A. (1981). Concept learning by experiment. Seventh International Joint Conference onArtijicial Intelligence, Vancouver. 
Sammut, C. A., & Banerji, R. B. (1986). Learning concepts by asking questions. InR. S. Michalski, 
J. G. Carbonell, & T. M. Mitchell (Eds.), Machine learning: An art$cial intelligence approach 
(Vol 2, pp. 167-192). Los Altos, California: Morgan Kaufmann. 
Shapiro, E. (1983). Algorithmic program debugging. Cambridge MA: MIT Press. 
Srinivasan, A., Muggleton, S., & King, R. D. (1995). Comparing the use of background knowl- 
edge by inductive logic programming systems (PRG Technical report PRG-TR-9-95). OxfordUniversity Computing Laboratory. 
Srinivasan, A,, Muggleton, S., King, R. D., & Stemberg, M. J. E. (1994). Mutagenesis: ILP ex- 
periments ina non-determinate biological domain. Proceedings of the Fourth Inductive LogicProgramming Workshop. 
Vere, S. (1975). Induction of concepts in the predicate calculus. Proceedings of the Fourth Intem- 
tional Joint Conference on Artijicial Intelligence (pp. 351-356). 
Winston, P. (1970). Learning structural descriptions from examples (Ph.D. dissertation) (MIT Tech- 
nical Report AI-TR-231). 
Wrobel, S. (1994). Concept formation and knowledge revision. Boston: Kluwer Academic Publishers. 
Wrobel, S. (1996). Inductive logic programming. InG. Brewka (Ed)., Principles of knowledge rep- 
resentation. Stanford, CA: CSLI Publications. 
CHAPTERANALYTICALLEARNINGInductive learning methods such as neural network and decision tree learning requirea certain number of training examples to achieve a given level of generalization ac- 
curacy, as reflected in the theoretical bounds and experimental results discussed inearlier chapters. Analytical learning uses prior knowledge and deductive reasoning toaugment the information provided by the training examples, so that itis not subjectto these same bounds. This chapter considers an analytical learning method calledexplanation-based learning (EBL). In explanation-based learning, prior knowledgeis used to analyze, or explain, how each observed training example satisfies thetarget concept. This explanation is then used to distinguish the relevant featuresof the training example from the irrelevant, so that examples can be generalizedbased on logical rather than statistical reasoning. Explanation-based learning hasbeen successfully applied to learning search control rules for a variety of planningand scheduling tasks. This chapter considers explanation-based learning when thelearner's prior knowledge is correct and complete. The next chapter considers com- 
bining inductive and analytical learning in situations where prior knowledge is onlyapproximately correct. 
11.1 INTRODUCTIONPrevious chapters have considered a variety of inductive learning methods: that is, 
methods that generalize from observed training examples by identifying featuresthat empirically distinguish positive from negative training examples. Decisiontree learning, neural network learning, inductive logic programming, and geneticalgorithms are all examples of inductive methods that operate in this fashion. Thekey practical limit on these inductive learners is that they perform poorly wheninsufficient data is available. In fact, as discussed in Chapter 7, theoretical analysisshows that there are fundamental bounds on the accuracy that can be achievedwhen learning inductively from a given number of training examples. 
Can we develop learning methods that are not subject to these fundamentalbounds on learning accuracy imposed by the amount of training data available? 
Yes, ifwe are willing to reconsider the formulation of the learning problem itself. 
One way isto develop learning algorithms that accept explicit prior knowledge asan input, in addition to the input training data. Explanation-based learning is onesuch approach. It uses prior knowledge to analyze, or explain, each training exam- 
ple in order to infer which example features are relevant to the target function andwhich are irrelevant. These explanations enable itto generalize more accuratelythan inductive systems that rely on the data alone. Aswe saw in the previous chap- 
ter, inductive logic programming systems such as CIGOL also use prior backgroundknowledge to guide learning. However, they use their background knowledge toinfer features that augment the input descriptions of instances, thereby increasingthe complexity of the hypothesis space tobe searched. In contrast, explanation- 
based learning uses prior knowledge to reduce the complexity of the hypothesisspace tobe searched, thereby reducing sample complexity and improving gener- 
alization accuracy of the learner. 
To capture the intuition underlying explanation-based learning, consider thetask of learning to play chess. In particular, suppose we would like our chessprogram to learn to recognize important classes of game positions, such as thetarget concept "chessboard positions in which black will lose its queen withintwo moves." Figure 11.1 shows a positive training example of this target concept. 
Inductive learning methods could, of course, be employed to learn this targetconcept. However, because the chessboard is fairly complex (there are 32 piecesthat may beon any of 64 squares), and because the particular patterns that capturethis concept are fairly subtle (involving the relative positions of various pieces onthe board), we would have to provide thousands of training examples similar tothe one in Figure 1 1.1 to expect an inductively learned hypothesis to generalizecorrectly to new situations. 
FIGURE 11.1A positive example of the target concept "chess positions inwhich black will lose its queen within two moves." Note thewhite knight is simultaneously attacking both the black king andqueen. Black must therefore move its king, enabling white tocapture its queen. 
What is interesting about this chess-learning task is that humans appear tolearn such target concepts from just a handful of training examples! In fact, afterconsidering only the single example shown in Figure 1 1.1, most people wouldbe willing to suggest a general hypothesis for the target concept, such as "boardpositions in which the black king and queen are simultaneously attacked," andwould not even consider the (equally consistent) hypothesis "board positions inwhich four white pawns are still in their original locations." How isit that humanscan generalize so successfully from just this one example? 
The answer appears tobe that people rely heavily on explaining, or analyz- 
ing, the training example in terms of their prior knowledge about the legal movesof chess. If asked to explain why the training example of Figure 11.1 isa positiveexample of "positions in which the queen will be lost in two moves," most peoplewould give an explanation similar to the following: "Because white's knight isattacking both the king and queen, black must move out of check, thereby al- 
lowing the knight to capture the queen." The importance of such explanations isthat they provide the information needed to rationally generalize from the detailsof the training example toa correct general hypothesis. Features of the trainingexample that are mentioned by the explanation (e.g., the position of the whiteknight, black king, and black queen) are relevant to the target concept and shouldbe included in the general hypothesis. In contrast, features of the example that arenot mentioned by the explanation (e.g., the fact that there are six black pawns onthe board) can be assumed tobe irrelevant details. 
What exactly is the prior knowledge needed bya learner to construct theexplanation in this chess example? Itis simply knowledge about the legal rules ofchess: knowledge of which moves are legal for the knight and other pieces, the factthat players must alternate moves in the game, and the fact that to win the game oneplayer must capture his opponent's king. Note that given just this prior knowledgeit is possible in principle to calculate the optimal chess move for any boardposition. However, in practice this calculation can be frustratingly complex anddespite the fact that we humans ourselves possess this complete, perfect knowledgeof chess, we remain unable to play the game optimally. Asa result, much of humanlearning in chess (and in other search-intensive problems such as scheduling andplanning) involves a long process of uncovering the consequences of our priorknowledge, guided by specific training examples encountered aswe play the game. 
This chapter describes learning algorithms that automatically construct andlearn from such explanations. In the remainder of this section we define moreprecisely the analytical learning problem. The next section presents a particularexplanation-based learning algorithm called PROLOG-EBG. Subsequent sectionsthen examine the general properties of this algorithm and its relationship toin- 
ductive learning algorithms discussed in other chapters. The final section describesthe application of explanation-based learning to improving performance at largestate-space search problems. In this chapter we consider the special case in whichexplanations are generated from prior knowledge that is perfectly correct, asit isfor us humans in the above chess example. In Chapter 12 we consider the moregeneral case of learning when prior knowledge is only approximately correct. 
11.1.1 Inductive and Analytical Learning ProblemsThe essential difference between analytical and inductive learning methods is thatthey assume two different formulations of the learning problem: 
0 In inductive learning, the learner is given a hypothesis space H from whichit must select an output hypothesis, and a set of training examples D = 
{(xl, f (x~)), . . . (x,, f (x,))} where f (xi) is the target value for the instancexi. The desired output of the learner isa hypothesis h from H that is con- 
sistent with these training examples. 
0 In analytical learning, the input to the learner includes the same hypothesisspace H and training examples Das for inductive learning. In addition, 
the learner is provided an additional input: A domain theory B consistingof background knowledge that can be used to explain observed trainingexamples. The desired output of ,the learner isa hypothesis h from H thatis consistent with both the training examples D and the domain theory B. 
To illustrate, in our chess example each instance xi would describe a particularchess position, and f (xi) would be True when xiis a position for which blackwill lose its queen within two moves, and False otherwise. We might definethe hypothesis space Hto consist of sets of Horn clauses (if-then rules) as inChapter 10, where the predicates used by the rules refer to the positions or relativepositions of specific pieces on the board. The domain theory B would consist of aformalization of the rules of chess, describing the legal moves, the fact that playersmust take turns, and the fact that the game is won when one player captures heropponent's king. 
Note in analytical learning, the learner must output a hypothesis that is con- 
sistent with both the training data and the domain theory. We say that hypothesish is consistent with domain theory B provided B does not entail the negation ofh (i.e., B -h). This additional constraint that the output hypothesis must beconsistent with B reduces the ambiguity faced by the learner when the data alonecannot resolve among all hypotheses inH. The net effect, provided the domaintheory is correct, isto increase the accuracy of the output hypothesis. 
Let us introduce in detail a second example ofan analytical learning prob- 
lem--one that we will use for illustration throughout this chapter. Consider aninstance space Xin which each instance isa pair of physical objects. Each of thetwo physical objects in the instance is described by the predicates Color, Volume, 
Owner, Material, Type, and Density, and the relationship between the two objectsis described by the predicate On. Given this instance space, the task isto learn thetarget concept "pairs of physical objects, such that one can be stacked safely onthe other," denoted by the predicate SafeToStack(x,y). Learning this target conceptmight be useful, for example, toa robot system that has the task of storing variousphysical objects within a limited workspace. The full definition of this analyticallearning task is given in Table 1 1.1. 
Given: 
rn Instance space X: Each instance describes a pair of objects represented by the predicates Type, 
Color, Volume, Owner, Material, Density, and On. 
rn Hypothesis space H: Each hypothesis isa set of Horn clause rules. The head of each Hornclause isa literal containing the target predicate SafeToStack. The body of each Horn clauseis a conjunction of literals based on the same predicates used to describe the instances, aswell as the predicates LessThan, Equal, GreaterThan, and the functions plus, minus, andtimes. For example, the following Horn clause isin the hypothesis space: 
Saf eToStack(x, y) t Volume(x, vx) r\ Volurne(y, vy) A LessThan(vx, vy) 
rn Target concept: SafeToStack(x,y) 
rn Training Examples: A typical positive example, SafeToStack(Obj1, ObjZ), is shown below: 
On(Objl.Obj2) Owner(0bj I, Fred) 
Type(0bj I, Box) Owner(Obj2, Louise) 
Type(Obj2, Endtable) Density(0bj 1 ,0.3) 
Color(Obj1, Red) Material(Obj1, Cardboard) 
Color(Obj2, Blue) Material (Obj2, Wood) 
Volume(Objl,2) 
Domain Theory B: 
SafeToStack(x, y) c -Fragile(y) 
SafeToStack(x, y) c Lighter(x, y) 
Lighter@, y) c Weight(x, wx) A Weight(y, wy) r\ LessThan(wx, wy) 
Weight(x, w) c Volume(x, v) A Density(x,d) A Equal(w, times(v, d)) 
Weight(x, 5) c Type(x, Endtable) 
Fragile(x) c Material (x, Glass) 
Determine: 
rnA hypothesis from H consistent with the training examples and domain theory. 
TABLE 11.1An analytical learning problem: SafeToStack(x,y). 
As shown in Table 11.1, we have chosen a hypothesis space Hin whicheach hypothesis isa set of first-order if-then rules, or Horn clauses (throughoutthis chapter we follow the notation and terminology for first-order Horn clausessummarized in Table 10.3). For instance, the example Horn clause hypothesisshown in the table asserts that itis SafeToStack any object xon any object y, ifthe Volume ofx is LessThan the Volume ofy (in this Horn clause the variablesvx and vy represent the volumes ofx and y, respectively). Note the Horn clausehypothesis can refer to any of the predicates used to describe the instances, as wellas several additional predicates and functions. A typical positive training example, 
SafeToStack(Obj1, Obj2), is also shown in the table. 
To formulate this task asan analytical learning problem we must also providea domain theory sufficient to explain why observed positive examples satisfy thetarget concept. In our earlier chess example, the domain theory corresponded toknowledge of the legal moves in chess, from which we constructed explanationsdescribing why black would lose its queen. In the current example, the domaintheory must similarly explain why certain pairs of objects can be safely stackedon one another. The domain theory shown in the table includes assertions suchas "itis safe to stack xon yif yis not Fragile," and "an object xis Fragile ifthe Material from which xis made is Glass." Like the learned hypothesis, thedomain theory is described bya collection of Horn clauses, enabling the system inprinciple to incorporate any learned hypotheses into subsequent domain theories. 
Notice that the domain theory refers to additional predicates such as Lighter andFragile, which are not present in the descriptions of the training examples, butwhich can be inferred from more primitive instance attributes such as Material, 
Density, and Volume, using other other rules in the domain theory. Finally, noticethat the domain theory shown in the table is sufficient to prove that the positiveexample shown there satisfies the target concept SafeToStack. 
11.2 LEARNING WITH PERFECT DOMAIN THEORIES: 
PROLOG-EBGAs stated earlier, in this chapter we consider explanation-based learning fromdomain theories that are perfect, that is, domain theories that are correct andcomplete. A domain theory is said tobe correct if each of its assertions is atruthful statement about the world. A domain theory is said tobe complete withrespect toa given target concept and instance space, if the domain theory coversevery positive example in the instance space. Put another way, itis complete ifevery instance that satisfies the target concept can be proven by the domain theoryto satisfy it. Notice our definition of completeness does not require that the domaintheory be able to prove that negative examples do not satisfy the target concept. 
However, ifwe follow the usual PROLOG convention that unprovable assertions areassumed tobe false, then this definition of completeness includes full coverageof both positive and negative examples by the domain theory. 
The reader may well ask at this point whether itis reasonable to assume thatsuch perfect domain theories are available to the learner. After all, if the learnerhad a perfect domain theory, why would it need to learn? There are two responsesto this question. 
First, there are cases in which itis feasible to provide a perfect domaintheory. Our earlier chess problem provides one such case, in which the legalmoves of chess form a perfect domain theory from which the optimal chessplaying strategy can (in principle) be inferred. Furthermore, although it isquite easy to write down the legal moves of chess that constitute this domaintheory, itis extremely difficult to write down the optimal chess-playingstrategy. In such cases, we prefer to provide the domain theory to the learnerand rely on the learner to formulate a useful description of the target concept 
(e.g., "board states in which Iam about to lose my queen") by examiningand generalizing from specific training examples. Section 11.4 describes thesuccessful application of explanation-based learning with perfect domaintheories to automatically improve performance at several search-intensiveplanning and optimization problems. 
0 Second, in many other cases itis unreasonable to assume that a perfectdomain theory is available. Itis difficult to write a perfectly correct andcomplete theory even for our relatively simple SafeToStack problem. A morerealistic assumption is that plausible explanations based on imperfect domaintheories must be used, rather than exact proofs based on perfect knowledge. 
Nevertheless, we can begin to understand the role of explanations in learningby considering the ideal case of perfect domain theories. In Chapter 12 wewill consider learning from imperfect domain theories. 
This section presents an algorithm called PROLOG-EBG (Kedar-Cabelli andMcCarty 1987) that is representative of several explanation-based learning algo- 
rithms. PROLOG-EBG isa sequential covering algorithm (see Chapter 10). In otherwords, it operates by learning a single Horn clause rule, removing the positivetraining examples covered by this rule, then iterating this process on the remain- 
ing positive examples until no further positive examples remain uncovered. Whengiven a complete and correct domain theory, PROLOG-EBG is guaranteed to outputa hypothesis (set of rules) that is itself correct and that covers the observed pos- 
itive training examples. For any set of training examples, the hypothesis outputby PROLOG-EBG constitutes a set of logically sufficient conditions for the targetconcept, according to the domain theory. PROLOG-EBG isa refinement of the EBGalgorithm introduced by Mitchell etal. (1986) and is similar to the EGGS algo- 
rithm described by DeJong and Mooney (1986). The PROLOG-EBG algorithm issummarized in Table 1 1.2. 
11.2.1 An Illustrative TraceTo illustrate, consider again the training example and domain theory shown inTable 1 1.1. As summarized in Table 1 1.2, the PROLOG-EBG algorithm isa se- 
quential covering algorithm that considers the training data incrementally. Foreach new positive training example that is not yet covered bya learned Hornclause, it forms a new Horn clause by: (1) explaining the new positive trainingexample, (2) analyzing this explanation to determine an appropriate generaliza- 
tion, and (3) refining the current hypothesis by adding a new Horn clause rule tocover this positive example, as well as other similar instances. Below we examineeach of these three steps in turn. 
11.2.1.1 EXPLAIN THE TRAINING EXAMPLEThe first step in processing each novel training example isto construct an expla- 
nation in terms of the domain theory, showing how this positive example satisfiesthe target concept. When the domain theory is correct and complete this expla- 
nation constitutes a proof that the training example satisfies the target concept. 
When dealing with imperfect prior knowledge, the notion of explanation must beextended to allow for plausible, approximate arguments rather than perfect proofs. 
PROWG-EBG(TargetConcept, TrainingExamples, DomainTheory) 
0 LearnedRules c (10 Pos c the positive examples from TrainingExamples0 for each PositiveExample in Pos that is not covered by LearnedRules, doI. Explain: 
Explanation can explanation (proof) in terms of the DomainTheory that PositiveEx- 
ample satisfies the TargetConcept2. Analyze: 
SufJicientConditions t the most general set of features of PositiveExample sufficientto satisfy the TargetConcept according to the Explanation. 
3. Rejine: 
0 LearnedRules c LearnedRules + NewHornClause, where NewHornCIause is ofthe formTargetConcept c SufJicientConditions0 Return LearnedRulesTABLE 11.2The explanation-based learning algorithm PROLOG-EBG. For each positive example that is not yetcovered by the set of learned Horn clauses (LearnedRules), a new Horn clause is created. Thisnew Horn clause is created by (1) explaining the training example in terms of the domain theory, 
(2) analyzing this explanation to determine the relevant features of the example, then (3) constructinga new Horn clause that concludes the target concept when this set of features is satisfied. 
The explanation for the current training example is shown in Figure 11.2. 
Note the bottom of this figure depicts in graphical form the positive trainingexample SafeToStack ( Objl , 0bj2 ) from Table 1 1.1. The top of the figure depictsthe explanation constructed for this training example. Notice the explanation, orproof, states that itis SafeToStack Objl on 0bj2 because Objl is Lighter thanObj2. Furthermore, Objl is known tobe Lighter, because its Weight can beinferred from its Density and Volume, and because the Weight of 0bj2 can beinferred from the default weight ofan Endtable. The specific Horn clauses thatunderlie this explanation are shown in the domain theory of Table 1 1.1. Notice thatthe explanation mentions only a small fraction of the known attributes of Objland 0bj2 (i.e., those attributes corresponding to the shaded region in the figure). 
While only a single explanation is possible for the training example anddomain theory shown here, in general there may be multiple possible explanations. 
In such cases, any or all of the explanations may be used. While each may giverise toa somewhat different generalization of the training example, all will bejustified by the given domain theory. In the case of PROLOG-EBG, the explanationis generated using a backward chaining search as performed by PROLOG. PROLOG- 
EBG, like PROLOG, halts once it finds the first valid proof. 
11.2.1.2 ANALYZE THE EXPLANATIONThe key question faced in generalizing the training example is "of the many fea- 
tures that happen tobe true of the current training example, which ones are gen- 
Explanation: 
Training Example: 
FIGURE 11.2Explanation ofa training example. The network at the bottom depicts graphically the training ex- 
ample SafeToStack(Obj1, Obj2) described in Table 11.1. The top portion of the figure depicts theexplanation of how this example satisfies the target concept, SafeToStack. The shaded region ofthe training example indicates the example attributes used in the explanation. The other, irrelevant, 
example attributes will be dropped from the generalized hypothesis formed from this analysis. 
erally relevant to the target concept?' The explanation constructed by the learnerprovides a direct answer to this question: precisely those features mentioned in theexplanation. For example, the explanation of Figure 11.2 refers to the Density ofObjl, but not to its Owner. Therefore, the hypothesis for SafeToStack(x,y) shouldinclude Density(x, 0.3), but not Owner(x, Fred). By collecting just the featuresmentioned in the leaf nodes of the explanation in Figure 11.2 and substitutingvariables x and y for Objl and Obj2, we can form a general rule that is justifiedby the domain theory: 
SafeToStack(x, y) t Volume(x, 2) A Density(x, 0.3) A Type(y, Endtable) 
The body of the above rule includes each leaf node in the proof tree, except forthe leaf nodes "Equal(0.6, times(2,0.3)" and "LessThan(0.6,5)." We omit thesetwo because they are by definition always satisfied, independent ofx and y. 
Along with this learned rule, the program can also provide its justification: 
The explanation of the training example forms a proof for the correctness of thisrule. Although this explanation was formed to cover the observed training exam- 
ple, the same explanation will apply to any instance that matches this general rule. 
The above rule constitutes a significant generalization of the training ex- 
ample, because it omits many properties of the example (e.g., the Color of thetwo objects) that are irrelevant to the target concept. However, an even moregeneral rule can be obtained by more careful analysis of the explanation. PROLOG- 
EBG computes the most general rule that can be justified by the explanation, bycomputing the weakest preimage of the explanation, defined as follows: 
Definition: The weakest preimage ofa conclusion C with respect toa proof P isthe most general set of initial assertions A, such that A entails C according toP. 
For example, the weakest preimage of the target concept SafeToStack(x,y), 
with respect to the explanation from Table 11.1, is given by the body of thefollowing rule. This is the most general rule that can be justified by the explanationof Figure 1 1.2: 
SafeToStack(x, y) t Volume(x, vx) A Density(x, dx)~ 
Equal(wx, times(vx, dx)) A LessThan(wx, 5)~ 
Type(y, Endtable) 
Notice this more general rule does not require the specific values for Volumeand Density that were required by the first rule. Instead, it states a more generalconstraint on the values of these attributes. 
PROLOG-EBG computes the weakest preimage of the target concept with re- 
spect to the explanation, using a general procedure called regression (Waldinger1977). The regression procedure operates ona domain theory represented by anarbitrary set of Horn clauses. It works iteratively backward through the explana- 
tion, first computing the weakest preimage of the target concept with respect tothe final proof step in the explanation, then computing the weakest preimage ofthe resulting expressions with respect to the preceding step, and soon. The pro- 
cedure terminates when it has iterated over all steps :in the explanation, yieldingthe weakest precondition of the target concept with respect to the literals at theleaf nodes of the explanation. 
A trace of this regression process is illustrated in Figure 11.3. In this fig- 
ure, the explanation from Figure 11.2 is redrawn in standard (nonitalic) font. Thefrontier of regressed expressions created at each step by the regression proce- 
dure is shown underlined in italics. The process begins at the root of the tree, 
with the frontier initialized to the general target concept SafeToStack(x,y). Thefirst step isto compute the weakest preimage of this frontier expression withrespect to the final (top-most) inference rule in the explanation. The rule inthis case is SafeToStack(x, y) t Lighter(x, y), so the resulting weakest preim- 
age is Lighter@, y). The process now continues by regressing the new frontier, 
{Lighter(x, y)], through the next Horn clause in the explanation, resulting in theregressed expressions (Weight(x, wx), LessThan(wx, wy), Weight(y, wy)}. Thisindicates that the explanation will hold for any x and y such that the weight wxof xis less than the weight wyof y. The regression of this frontier back tothe leaf nodes of the explanation continues in this step-by-step fashion, finallyFIGURE 11.3Computing the weakest preimage of SafeToStack(0 bj 1, Obj2) with respect to the explanation. Thetarget concept is regressed from the root (conclusion) of the explanation, down to the leaves. At eachstep (indicated by the dashed lines) the current frontier set of literals (underlined in italics) is regressedbackward over one rule in the explanation. When this process is completed, the conjunction ofresulting literals constitutes the weakest preimage of the target concept with respect to the explanation. 
This weakest preimage is shown by the italicized literals at the bottom of the figure. 
resulting ina set of generalized literals for the leaf nodes of the tree. This finalset of literals, shown at the bottom of Figure 11.3, forms the body of the finalrule. 
The heart of the regression procedure is the algorithm that at each step re- 
gresses the current frontier of expressions through a single Horn clause from thedomain theory. This algorithm is described and illustrated in Table 11.3. The illus- 
trated example in this table corresponds to the bottommost single regression stepof Figure 11.3. As shown in the table, the REGRESS algorithm operates by findinga substitution that unifies the head of the Horn clause rule with the correspondingliteral in the frontier, replacing this expression in the frontier by the rule body, 
then applying a unifying substitution to the entire frontier. 
The final Horn clause rule output by PROLOG-EBG is formulated as follows: 
The clause body is defined tobe the weakest preconditions calculated by the aboveprocedure. The clause head is the target concept itself, with each substitution fromeach regression step (i.e., the substitution Oh[ in Table 11.3) applied toit. Thissubstitution is necessary in order to keep consistent variable names between thehead and body of the created clause, and to specialize the clause head when theR~~~~ss(Frontier, Rule, Literal, &i) 
Frontier: Set of literals tobe regressed through RuleRule: A Horn clauseLiteral: A literal in Frontier that is inferred by Rule in the explanationOki: The substitution that unijies the head of Rule to the corresponding literal in the explanationReturns the set of literals forming the weakest preimage of Frontier with respect to Rulehead t head of Rulebody t body of RuleBkl t the most general unifier of head with Literal such that there exists a substitution Blifor whichOri (Bkl (head)) = Bhi (head) 
Return Okl (Frontier - head + body) 
Example (the bottommost regression step in Figure 11.3): 
h?~~~ss(Frontier, Rule, Literd, @hi) whereFrontier = {Volume(x, us), Density(x, dx), Equal(wx, times(vx,dx)), LessThan(wx, wy), 
Weight(y, wy)) 
Rule = Weight(z, 5) c Type(z, Endtable) 
Literal = Weight(y, wy) 
6ki = {z/Obj21head c Weight (z, 5) 
body c Type(z, Endtable) 
Bhl e {z/y, wy/5], where Bri = (ylObj2) 
Return {Volume(x, us), Density(x, dx), Equal (wx, times(vx, dx)), LessThan(wx, 5). 
Type(y, Endtable)] 
TABLE 11.3Algorithm for regressing a set of literals through a single Horn clause. The set of literals givenby Frontier is regressed through Rule. Literal is the member of Frontier inferred by Rule inthe explanation. The substitution Bki gives the binding of variables from the head of Rule to thecorresponding literal in the explanation. The algorithm first computes a substitution Bhl that unifiesthe Rule head to Literal, ina way that is consistent with the substitution Bki. It then applies thissubstitution Oh[ to construct the preimage of Frontier with respect to Rule. The symbols "+" and 
"-" in the algorithm denote set union and set difference. The notation {zly] denotes the substitutionof yin place ofz. An example trace is given. 
explanation applies to only a special case of the target concept. As noted earlier, 
for the current example the final rule isSafeToStack(x, y) t Volume(x, vx) A Density(x, dx)~ 
Equal(wx, times(vx, dx)) A LessThan(wx, 5)~ 
Type(y, Endtable) 
11.2.1.3 REFINE THE CURRENT HYPOTHESISThe current hypothesis at each stage consists of the set of Horn clauses learnedthus far. At each stage, the sequential covering algorithm picks a new positiveexample that is not yet covered by the current Horn clauses, explains this newexample, and formulates a new rule'according to the procedure described above. 
Notice only positive examples are covered in the algorithm aswe have definedit, and the learned set of Horn clause rules predicts only positive examples. Anew instance is classified as negative if the current rules fail to predict that it ispositive. This isin keeping with the standard negation-as-failure approach usedin Horn clause inference systems such as PROLOG. 
11.3 REMARKS ON EXPLANATION-BASED LEARNINGAs we saw in the above example, PROLOG-EBG conducts a detailed analysis ofindividual training examples to determine how best to generalize from the specificexample toa general Horn clause hypothesis. The following are the key propertiesof this algorithm. 
0 Unlike inductive methods, PROLOG-EBG produces justified general hypothe- 
ses by using prior knowledge to analyze individual examples. 
0 The explanation of how the example satisfies the target concept determineswhich example attributes are relevant: those mentioned by the explanation. 
0 The further analysis of the explanation, regressing the target concept tode- 
termine its weakest preimage with respect to the explanation, allows derivingmore general constraints on the values of the relevant features. 
0 Each learned Horn clause corresponds toa sufficient condition for satisfy- 
ing the target concept. The set of learned Horn clauses covers the positivetraining examples encountered by the learner, as well as other instances thatshare the same explanations. 
0 The generality of the learned Horn clauses will depend on the formulationof the domain theory and on the sequence in which training examples areconsidered. 
0 PROLOG-EBG implicitly assumes that the domain theory is correct and com- 
plete. If the domain theory is incorrect or incomplete, the resulting learnedconcept may also be incorrect. 
There are several related perspectives on explanation-based learning thathelp to understand its capabilities and limitations. 
0 EBL as theory-guided generalization of examples. EBL uses its given domaintheory to generalize rationally from examples, distinguishing the relevant ex- 
ample attributes from the irrelevant, thereby allowing itto avoid the boundson sample complexity that apply to purely inductive learning. This is theperspective implicit in the above description of the PROLOG-EBG algorithm. 
0 EBL as example-guided reformulation of theories. The PROLOG-EBG algo- 
rithm can be viewed asa method for reformulating the domain theory into amore operational form. In particular, the original domain theory is reformu- 
lated by creating rules that (a) follow deductively from the domain theory, 
and (b) classify the observed training examples ina single inference step. 
Thus, the learned rules can be seen asa reformulation of the domain theoryinto a set of special-case rules capable of classifying instances of the targetconcept ina single inference step. 
0 EBL as "just" restating what the learner already "knows. " In one sense, thelearner in our SafeToStack example begins with full knowledge of the Safe- 
ToStack concept. That is, if its initial domain theory is sufficient to explainany observed training examples, then itis also sufficient to predict theirclassification in advance. In what sense, then, does this qualify as learning? 
One answer is that in many tasks the difference between what one knowsin principle and what one can efficiently compute in practice may be great, 
and in such cases this kind of "knowledge reformulation" can bean impor- 
tant form of learning. In playing chess, for example, the rules of the gameconstitute a perfect domain theory, sufficient in principle to play perfectchess. Despite this fact, people still require considerable experience to learnhow to play chess well. This is precisely a situation in which a complete, 
perfect domain theory is already known to the (human) learner, and furtherlearning is "simply" a matter of reformulating this knowledge into a formin which it can be used more effectively to select appropriate moves. Abe- 
ginning course in Newtonian physics exhibits the same property-the basiclaws of physics are easily stated, but students nevertheless spend a largepart ofa semester working out the consequences so they have this knowl- 
edge in more operational form and need not derive every problem solutionfrom first principles come the final exam. PROLOG-EBG performs this typeof reformulation of knowledge-its learned rules map directly from observ- 
able instance features to the classification relative to the target concept, in away that is consistent with the underlying domain theory. Whereas it mayrequire many inference steps and considerable search to classify an arbi- 
trary instance using the original domain theory, the learned rules classifythe observed instances ina single inference step. 
Thus, in its pure form EBL involves reformulating the domain theory toproduce general rules that classify examples ina single inference step. This kindof knowledge reformulation is sometimes referred toas knowledge compilation, 
indicating that the transformation isan efficiency improving one that does notalter the correctness of the system's knowledge. 
11.3.1 Discovering New FeaturesOne interesting capability of PROLOG-EBG is its ability to formulate new featuresthat are not explicit in the description of the training examples, but that are neededto describe the general rule underlying the training example. This capability isillustrated by the algorithm trace and the learned rule in the previous section. Inparticular, the learned rule asserts that the essential constraint on the Volume andDensity ofx is that their product is less than 5. In fact, the training examplescontain no description of such a product, orof the value it should take on. Instead, 
this constraint is formulated automatically by the learner. 
Notice this learned "feature" is similar in kind to the types of features repre- 
sented by the hidden units of neural networks; that is, this feature is one ofa verylarge set of potential features that can be computed from the available instanceattributes. Like the BACKPROPAGATION algorithm, PROLOG-EBG automatically for- 
mulates such features in its attempt to fit the training data. However, unlike thestatistical process that derives hidden unit features in neural networks from manytraining examples, PROLOG-EBG employs an analytical process to derive new fea- 
tures based on analysis of single training examples. Above, PROLOG-EBG derivesthe feature Volume . Density > 5 analytically from the particular instantiationof the domain theory used to explain a single training example. For example, 
the notion that the product of Volume and Density is important arises from thedomain theory rule that defines Weight. The notion that this product should beless than 5 arises from two other domain theory rules that assert that Obj 1 shouldbe Lighter than the Endtable, and that the Weight of the Endtable is 5. Thus, 
itis the particular composition and instantiation of these primitive terms from thedomain theory that gives rise to defining this new feature. 
The issue of automatically learning useful features to augment the instancerepresentation isan important issue for machine learning. The analytical derivationof new features in explanation-based learning and the inductive derivation of newfeatures in the hidden layer of neural networks provide two distinct approaches. 
Because they rely on different sources of information (statistical regularities overmany examples versus analysis of single examples using the domain theory), itmay be useful to explore new methods that combine both sources. 
11.3.2 Deductive LearningIn its pure form, PROLOG-EBG isa deductive, rather than inductive, learning pro- 
cess. That is, by calculating the weakest preimage of the explanation it producesa hypothesis h that follows deductively from the domain theory B, while coveringthe training data D. Tobe more precise, PROLOG-EBG outputs a hypothesis h thatsatisfies the following two constraints: 
where the training data D consists ofa set of training examples in which xiis theith training instance and f (xi) is its target value (fis the target function). Noticethe first of these constraints is simply a formalization of the usual requirement inmachine learning, that the hypothesis h correctly predict the target value f (xi) foreach instance xiin the training data.tOf course there will, in general, be manyt~ere we include PROLOG-S~Y~ negation-by-failure in our definition of entailment (F), so that ex- 
amples are entailed tobe negative examples if they cannot be proven tobe positive. 
alternative hypotheses that satisfy this first constraint. The second constraint de- 
scribes the impact of the domain theory in PROLOG-EBL: The output hypothesis isfurther constrained so that it must follow from the domain theory and the data. Thissecond constraint reduces the ambiguity faced by the learner when it must choosea hypothesis. Thus, the impact of the domain theory isto reduce the effective sizeof the hypothesis space and hence reduce the sample complexity of learning. 
Using similar notation, we can state the type of knowledge that is requiredby PROLOG-EBG for its domain theory. In particular, PROLOG-EBG assumes thedomain theory B entails the classifications of the instances in the training data: 
This constraint on the domain theory B assures that an explanation can be con- 
structed for each positive example. 
Itis interesting to compare the PROLOG-EBG learning setting to the settingfor inductive logic programming (ILP) discussed in Chapter 10. In that chapter, 
we discussed a generalization of the usual inductive learning task, in which back- 
ground knowledge B' is provided to the learner. We will use B' rather than B todenote the background knowledge used by ILP, because it does not typically sat- 
isfy the constraint given by Equation (1 1.3). ILP isan inductive learning system, 
whereas PROLOG-EBG is deductive. ILP uses its background knowledge B' toen- 
large the set of hypotheses tobe considered, whereas PROLOG-EBG uses its domaintheory Bto reduce the set of acceptable hypotheses. As stated in Equation (10.2), 
ILP systems output a hypothesis h that satisfies the following constraint: 
Note the relationship between this expression and the constraints onh imposedby PROLOG-EBG (given by Equations (1 1.1) and (1 1.2)). This ILP constraint onh isa weakened form of the constraint given by Equation (1 1.1)-the ILP con- 
straint requires only that (B' Ah /\xi) kf (xi), whereas the PROLOG-EBG constraintrequires the more strict (hxi) kf (xi). Note also that ILP imposes no constraintcorresponding to the PROLOG-EBG constraint of Equation (1 1.2). 
11.3.3 Inductive Bias in Explanation-Based LearningRecall from Chapter 2 that the inductive bias ofa learning algorithm isa setof assertions that, together with the training examples, deductively entail sub- 
sequent predictions made by the learner. The importance of inductive bias isthat it characterizes how the learner generalizes beyond the observed trainingexamples. 
What is the inductive bias of PROLOG-EBG? In PROLOG-EBG the output hy- 
pothesis h follows deductively from DAB, as described by Equation (1 1.2). There- 
fore, the domain theory Bis a set of assertions which, together with the trainingexamples, entail the output hypothesis. Given that predictions of the learner followfrom this hypothesis h, it appears that the inductive bias of PROLOG-EBG is simplythe domain theory B input to the learner. In fact, this is the case except for oneadditional detail that must be considered: There are many alternative sets of Hornclauses entailed by the domain theory. The remaining component of the inductivebias is therefore the basis by which PROLOG-EBG chooses among these alternativesets of Horn clauses. Aswe saw above, PROLOG-EBG employs a sequential cover- 
ing algorithm that continues to formulate additional Horn clauses until all positivetraining examples have been covered. Furthermore, each individual Horn clauseis the most general clause (weakest preimage) licensed by the explanation of thecurrent training example. Therefore, among the sets of Horn clauses entailed bythe domain theory, we can characterize the bias of PROLOG-EBG asa preferencefor small sets of maximally general Horn clauses. In fact, the greedy algorithm ofPROLOG-EBG is only a heuristic approximation to the exhaustive search algorithmthat would be required to find the truly shortest set of maximally general Hornclauses. Nevertheless, the inductive bias of PROLOG-EBG can be approximatelycharacterized in this fashion. 
Approximate inductive bias of PROLOG-EBG: The domain theory B, plus a pref- 
erence for small sets of maximally general Horn clauses. 
The most important point here is that the inductive bias of PROLOG-EBG- 
the policy by which it generalizes beyond the training data-is largely determinedby the input domain theory. This lies in stark contrast to most of the other learningalgorithms we have discussed (e.g., neural networks, decision tree learning), inwhich the inductive bias isa fixed property of the learning algorithm, typicallydetermined by the syntax of its hypothesis representation. Why isit importantthat the inductive bias bean input parameter rather than a fixed property of thelearner? Because, aswe have discussed in Chapter 2 and elsewhere, there isno universally effective inductive bias and because bias-free learning is futile. 
Therefore, any attempt to develop a general-purpose learning method must atminimum allow the inductive bias to vary with the learning problem at hand. 
Ona more practical level, in many tasks itis quite natural to input domain- 
specific knowledge (e.g., the knowledge about Weight in the SafeToStack ex- 
ample) to influence how the learner will generalize beyond the training data. 
In contrast, itis less natural to "implement" an appropriate bias by restrictingthe syntactic form of the hypotheses (e.g., prefer short decision trees). Finally, 
ifwe consider the larger issue of how an autonomous agent may improve itslearning capabilities over time, then itis attractive to have a learning algorithmwhose generalization capabilities improve asit acquires more knowledge of itsdomain. 
11.3.4 Knowledge Level LearningAs pointed out in Equation (1 1.2), the hypothesis h output by PROLOG-EBG followsdeductively from the domain theory B and training data D. In fact, by examiningthe PROLOG-EBG algorithm itis easy to see that h follows directly from B alone, 
independent ofD. One way to see this isto imagine an algorithm that we mightcall LEMMA-ENUMERATOR. The LEMMA-ENUMERATOR algorithm simply enumeratesall proof trees that conclude the target concept based on assertions in the domaintheory B. For each such proof tree, LEMMA-ENUMERATOR calculates the weakestpreimage and constructs a Horn clause, in the same fashion as PROLOG-EBG. Theonly difference between LEMMA-ENUMERATOR and PROLOG-EBG is that LEMMA- 
ENUMERATOR ignores the training data and enumerates all proof trees. 
Notice LEMMA-ENUMERATOR will output a superset of the Horn clauses outputby PROLOG-EBG. Given this fact, several questions arise. First, if its hypothesesfollow from the domain theory alone, then what is the role of training data inPROLOG-EBG? The answer is that training examples focus the PROLOG-EBG al- 
gorithm on generating rules that cover the distribution of instances that occur inpractice. In our original chess example, for instance, the set of all possible lemmasis huge, whereas the set of chess positions that occur in normal play is only asmall fraction of those that are syntactically possible. Therefore, by focusing onlyon training examples encountered in practice, the program is likely to develop asmaller, more relevant set of rules than ifit attempted to enumerate all possiblelemmas about chess. 
The second question that arises is whether PROLOG-EBG can ever learn ahypothesis that goes beyond the knowledge that is already implicit in the domaintheory. Put another way, will it ever learn to classify an instance that could notbe classified by the original domain theory (assuming a theorem prover withunbounded computational resources)? Unfortunately, it will not. IfB Fh, thenany classification entailed byh will also be entailed byB. Is this an inherentlimitation of analytical or deductive learning methods? No, itis not, as illustratedby the following example. 
To produce an instance of deductive learning in which the learned hypothesish entails conclusions that are not entailed byB, we must create an example whereB yh but where DA BF h (recall the constraint given by Equation (11.2)). 
One interesting case is when B contains assertions such as "Ifx satisfies thetarget concept, then so will g(x)." Taken alone, this assertion does not entail theclassification of any instances. However, once we observe a positive example, itallows generalizing deductively to other unseen instances. For example, considerlearning the PlayTennis target concept, describing the days on which our friendRoss would like to play tennis. Imagine that each day is described only by thesingle attribute Humidity, and the domain theory B includes the single assertion 
"If Ross likes to play tennis when the humidity isx, then he will also like to playtennis when the humidity is lower than x," which can be stated more formally as 
(Vx) IF ((PlayTennis = Yes) t (Humidity = x)) 
THEN ((PlayTennis = Yes) t (Humidity 5 x)) 
Note that this domain theory does not entail any conclusions regarding whichinstances are positive or negative instances of PlayTennis. However, once thelearner observes a positive example day for which Humidity = .30, the domaintheory together with this positive example entails the following general hypothe- 
CHAPTER 11 ANALYTICAL LEARNING 325sis h: 
(PlayTennis = Yes) t- (Humidity 5 .30) 
To summarize, this example illustrates a situation where BI+ h, but whereB AD I- h. The learned hypothesis in this case entails predictions that are notentailed by the domain theory alone. The phrase knowledge-level learning is some- 
times used to refer to this type of learning, in which the learned hypothesis entailspredictions that go beyond those entailed by the domain theory. The set of allpredictions entailed bya set of assertions Yis often called the deductive closureof Y. The key distinction here is that in knowledge-level learning the deductiveclosure ofB isa proper subset of the deductive closure ofB + h. 
A second example of knowledge-level analytical learning is provided by con- 
sidering a type of assertions known as determinations, which have been exploredin detail by Russell (1989) and others. Determinations assert that some attribute ofthe instance is fully determined by certain other attributes, without specifying theexact nature of the dependence. For example, consider learning the target concept 
"people who speak Portuguese," and imagine we are given asa domain theory thesingle determination assertion "the language spoken bya person is determined bytheir nationality." Taken alone, this domain theory does not enable usto classifyany instances as positive or negative. However, ifwe observe that "Joe, a 23- 
year-old left-handed Brazilian, speaks Portuguese," then we can conclude fromthis positive example and the domain theory that "all Brazilians speak Portuguese." 
Both of these examples illustrate how deductive learning can produce outputhypotheses that are not entailed by the domain theory alone. In both of these cases, 
the output hypothesis h satisfies BA DI- h, but does not satisfy BI- h. In bothcases, the learner deduces a justified hypothesis that does not follow from eitherthe domain theory alone or the training data alone. 
11.4 EXPLANATION-BASED LEARNING OF SEARCH CONTROLKNOWLEDGEAs noted above, the practical applicability of the PROLOG-EBG algorithm isre- 
stricted by its requirement that the domain theory be correct and complete. Oneimportant class of learning problems where this requirement is easily satisfied islearning to speed up complex search programs. In fact, the largest scale attempts toapply explanation-based learning have addressed the problem of learning to con- 
trol search, or what is sometimes called "speedup" learning. For example, playinggames such as chess involves searching through a vast space of possible movesand board positions to find the best move. Many practical scheduling and optimiza- 
tion problems are easily formulated as large search problems, in which the task isto find some move toward the goal state. In such problems the definitions of thelegal search operators, together with the definition of the search objective, providea complete and correct domain theory for learning search control knowledge. 
Exactly how should we formulate the problem of learning search control sothat we can apply explanation-based learning? Consider a general search problemwhere Sis the set of possible search states, 0 isa set of legal search operators thattransform one search state into another, and Gis a predicate defined over S thatindicates which states are goal states. The problem in general isto find a sequenceof operators that will transform an arbitrary initial state sito some final state sfthat satisfies the goal predicate G. One way to formulate the learning problem is tohave our system learn a separate target concept for each of the operators in 0. Inparticular, for each operator oin 0 it might attempt to learn the target concept "theset of states for which o leads toward a goal state." Of course the exact choiceof which target concepts to learn depends on the internal structure of problemsolver that must use this learned knowledge. For example, if the problem solveris a means-ends planning system that works by establishing and solving subgoals, 
then we might instead wish to learn target concepts such as "the set of planningstates in which subgoals of type A should be solved before subgoals of type B." 
One system that employs explanation-based learning to improve its searchis PRODIGY (Carbonell etal. 1990). PRODIGY isa domain-independent planningsystem that accepts the definition ofa problem domain in terms of the statespace S and operators 0. It then solves problems of the form "find a sequenceof operators that leads from initial state sito a state that satisfies goal predicateG." PRODIGY uses a means-ends planner that decomposes problems into subgoals, 
solves them, then combines their solutions into a solution for the full problem. 
Thus, during its search for problem solutions PRODIGY repeatedly faces questionssuch as "Which subgoal should be solved next?'and "Which operator shouldbe considered for solving this subgoal?' Minton (1988) describes the integrationof explanation-based learning into PRODIGY by defining a set of target conceptsappropriate for these kinds of control decisions that it repeatedly confronts. Forexample, one target concept is "the set of states in which subgoal A should besolved before subgoal B." An example ofa rule learned by PRODIGY for this targetconcept ina simple block-stacking problem domain isIF One subgoal tobe solved isOn@, y), andOne subgoal tobe solved isOn(y, z) 
THEN Solve the subgoal On(y, z) before On(x, y) 
To understand this rule, consider again the simple block stacking problem illus- 
trated in Figure 9.3. In the problem illustrated by that figure, the goal isto stackthe blocks so that they spell the word "universal." PRODIGY would decompose thisproblem into several subgoals tobe achieved, including On(U, N), On(N, I), etc. 
Notice the above rule matches the subgoals On(U, N) and On(N, I), and recom- 
mends solving the subproblem On(N, I) before solving On(U, N). The justifica- 
tion for this rule (and the explanation used by PRODIGY to learn the rule) is thatif we solve the subgoals in the reverse sequence, we will encounter a conflict inwhich we must undo the solution to the On(U, N) subgoal in order to achieve theother subgoal On(N, I). PRODIGY learns by first encountering such a conflict, thenexplaining to itself the reason for this conflict and creating a rule such as the oneabove. The net effect is that PRODIGY uses domain-independent knowledge aboutpossible subgoal conflicts, together with domain-specific knowledge of specificoperators (e.g., the fact that the robot can pick up only one block ata time), tolearn useful domain-specific planning rules such as the one illustrated above. 
The use of explanation-based learning to acquire control knowledge forPRODIGY has been demonstrated ina variety of problem domains including thesimple block-stacking problem above, as well as more complex scheduling andplanning problems. Minton (1988) reports experiments in three problem domains, 
in which the learned control rules improve problem-solving efficiency bya factorof two to four. Furthermore, the performance of these learned rules is comparableto that of handwritten rules across these three problem domains. Minton also de- 
scribes a number of extensions to the basic explanation-based learning procedurethat improve its effectiveness for learning control knowledge. These include meth- 
ods for simplifying learned rules and for removing learned rules whose benefitsare smaller than their cost. 
A second example ofa general problem-solving architecture that incorpo- 
rates a form of explanation-based learning is the SOAR system (Laird etal. 1986; 
Newel1 1990). SOAR supports a broad variety of problem-solving strategies thatsubsumes PRODIGY'S means-ends planning strategy. Like PRODIGY, however, SOARlearns by explaining situations in which its current search strategy leads to ineffi- 
ciencies. When it encounters a search choice for which it does not have a definiteanswer (e.g., which operator to apply next) SOAR reflects on this search impasse, 
using weak methods such as generate-and-test to determine the correct course ofaction. The reasoning used to resolve this impasse can be interpreted asan expla- 
nation for how to resolve similar impasses in the future. SOAR uses a variant ofexplanation-based learning called chunking to extract the general conditions un- 
der which the same explanation applies. SOAR has been applied ina great numberof problem domains and has also been proposed asa psychologically plausiblemodel of human learning processes (see Newel1 1990). 
PRODIGY and SOAR demonstrate that explanation-based learning methods canbe successfully applied to acquire search control knowledge ina variety of problemdomains. Nevertheless, many or most heuristic search programs still use numericalevaluation functions similar to the one described in Chapter 1, rather than rulesacquired by explanation-based learning. What is the reason for this? In fact, thereare significant practical problems with applying EBL to learning search control. 
First, in many cases the number of control rules that must be learned is very large 
(e.g., many thousands of rules). As the system learns more and more control rulesto improve its search, it must pay a larger and larger cost at each step to match thisset of rules against the current search state. Note this problem is not specific toexplanation-based learning; it will occur for any system that represents its learnedknowledge bya growing set of rules. Efficient algorithms for matching rules canalleviate this problem, but not eliminate it completely. Minton (1988) discussesstrategies for empirically estimating the computational cost and benefit of eachrule, learning rules only when the estimated benefits outweigh the estimated costsand deleting rules later found to have negative utility. He describes how usingthis kind of utility analysis to determine what should be learned and what shouldbe forgotten significantly enhances the effectiveness of explanation-based learningin PRODIGY. For example, ina series of robot block-stacking problems, PRODIGYencountered 328 opportunities for learning a new rule, but chose to exploit only 69of these, and eventually reduced the learned rules toa set of 19, once low-utilityrules were eliminated. Tambe etal. (1990) and Doorenbos (1993) discuss how toidentify types of rules that will be particularly costly to match, as well as methodsfor re-expressing such rules in more efficient forms and methods for optimizingrule-matching algorithms. Doorenbos (1993) describes how these methods enabledSOAR to efficiently match a set of 100,000 learned rules in one problem domain, 
without a significant increase in the cost of matching rules per state. 
A second practical problem with applying explanation-based learning tolearning search control is that in many cases itis intractable even to constructthe explanations for the desired target concept. For example, in chess we mightwish to learn a target concept such as "states for which operator A leads towardthe optimal solution." Unfortunately, to prove or explain why A leads toward theoptimal solution requires explaining that every alternative operator leads toa lessoptimal outcome. This typically requires effort exponential in the search depth. 
Chien (1993) and Tadepalli (1990) explore methods for "lazy" or "incremental" 
explanation, in which heuristics are used to produce partial and approximate, buttractable, explanations. Rules are extracted from these imperfect explanations asthough the explanations were perfect. Of course these learned rules may bein- 
correct due to the incomplete explanations. The system accommodates this bymonitoring the performance of the rule on subsequent cases. If the rule subse- 
quently makes an error, then the original explanation is incrementally elaboratedto cover the new case, and a more refined rule is extracted from this incrementallyimproved explanation. 
Many additional research efforts have explored the use of explanation-basedlearning for improving the efficiency of search-based problem solvers (for exam- 
ple, Mitchell 1981; Silver 1983; Shavlik 1990; Mahadevan etal. 1993; Gervasioand DeJong 1994; DeJong 1994). Bennett and DeJong (1996) explore explanation- 
based learning for robot planning problems where the system has an imperfectdomain theory that describes its world and actions. Dietterich and Flann (1995) 
explore the integration of explanation-based learning with reinforcement learningmethods discussed in Chapter 13. Mitchell and Thrun (1993) describe the appli- 
cation ofan explanation-based neural network learning method (see the EBNNalgorithm discussed in Chapter 12) to reinforcement learning problems. 
11.5 SUMMARY AND FURTHER READINGThe main points of this chapter include: 
In contrast to purely inductive learning methods that seek a hypothesis tofit the training data, purely analytical learning methods seek a hypothesisthat fits the learner's prior knowledge and covers the training examples. 
Humans often make use of prior knowledge to guide the formation of newhypotheses. This chapter examines purely analytical learning methods. Thenext chapter examines combined inductive-analytical learning. 
a Explanation-based learning isa form of analytical learning in which thelearner processes each novel training example by (1) explaining the observedtarget value for this example in terms of the domain theory, (2) analyzing thisexplanation to determine the general conditions under which the explanationholds, and (3) refining its hypothesis to incorporate these general conditions. 
a PROLOG-EBG isan explanation-based learning algorithm that uses first-orderHorn clauses to represent both its domain theory and its learned hypothe- 
ses. In PROLOG-EBG an explanation isa PROLOG proof, and the hypothesisextracted from the explanation is the weakest preimage of this proof. As aresult, the hypotheses output by PROLOG-EBG follow deductively from itsdomain theory. 
a Analytical learning methods such as PROLOG-EBG construct useful interme- 
diate features asa side effect of analyzing individual training examples. Thisanalytical approach to feature generation complements the statistically basedgeneration of intermediate features (eg., hidden unit features) in inductivemethods such as BACKPROPAGATION. 
a Although PROLOG-EBG does not produce hypotheses that extend the deduc- 
tive closure of its domain theory, other deductive learning procedures can. 
For example, a domain theory containing determination assertions (e.g., "na- 
tionality determines language") can be used together with observed data todeductively infer hypotheses that go beyond the deductive closure of thedomain theory. 
a One important class of problems for which a correct and complete domaintheory can be found is the class of large state-space search problems. Systemssuch as PRODIGY and SOAR have demonstrated the utility of explanation- 
based learning methods for automatically acquiring effective search controlknowledge that speeds up problem solving in subsequent cases. 
a Despite the apparent usefulness of explanation-based learning methods inhumans, purely deductive implementations such as PROLOG-EBG suffer thedisadvantage that the output hypothesis is only as correct as the domaintheory. In the next chapter we examine approaches that combine inductiveand analytical learning methods in order to learn effectively from imperfectdomain theories and limited training data. 
The roots of analytical learning methods can be traced to early work byFikes etal. (1972) on learning macro-operators through analysis of operatorsin ABSTRIPS and to somewhat later work by Soloway (1977) on the use ofexplicit prior knowledge in learning. Explanation-based learning methods similarto those discussed in this chapter first appeared ina number of systems developedduring the early 1980s, including DeJong (1981); Mitchell (1981); Winston etal. 
(1983); and Silver (1983). DeJong and Mooney (1986) and Mitchell etal. (1986) 
provided general descriptions of the explanation-based learning paradigm, whichhelped spur a burst of research on this topic during the late 1980s. A collection ofresearch on explanation-based learning performed at the University of Illinois isdescribed by DeJong (1993), including algorithms that modify the structure of theexplanation in order to correctly generalize iterative and temporal explanations. 
More recent research has focused on extending explanation-based methods toaccommodate imperfect domain theories and to incorporate inductive togetherwith analytical learning (see Chapter 12). An edited collection exploring the roleof goals and prior knowledge in human and machine learning is provided by Ramand Leake (1995), and a recent overview of explanation-based learning is givenby DeJong (1997). 
The most serious attempts to employ explanation-based learning with perfectdomain theories have been in the area of learning search control, or "speedup" 
learning. The SOAR system described by Laird etal. (1986) and the PRODIGYsystem described by Carbonell etal. (1990) are among the most developed sys- 
tems that use explanation-based learning methods for learning in problem solv- 
ing. Rosenbloom and Laird (1986) discuss the close relationship between SOAR'Slearning method (called "chunking") and other explanation-based learning meth- 
ods. More recently, Dietterich and Flann (1995) have explored the combinationof explanation-based learning with reinforcement learning methods for learningsearch control. 
While our primary purpose here isto study machine learning algorithms, itis interesting to note that experimental studies of human learning provide supportfor the conjecture that human learning is based on explanations. For example, 
Ahn etal. (1987) and Qin etal. (1992) summarize evidence supporting the con- 
jecture that humans employ explanation-based learning processes. Wisniewski andMedin (1995) describe experimental studies of human learning that suggest a richinterplay between prior knowledge and observed data to influence the learningprocess. Kotovsky and Baillargeon (1994) describe experiments that suggest even1 1-month old infants build on prior knowledge as they learn. 
The analysis performed in explanation-based learning is similar to certainkinds of program optimization methods used for PROLOG programs, such as par- 
tial evaluation; van Harmelen and Bundy (1988) provide one discussion of therelationship. 
EXERCISES11.1. Consider the problem of learning the target concept "pairs of people who live inthe same house," denoted by the predicate HouseMates(x, y). Below isa positiveexample of the concept. 
HouseMates(Joe, Sue) 
Person( Joe) Person(Sue) 
Sex(Joe, Male) Sex(Sue, Female) 
Hair Color (Joe, Black) Haircolor (Sue, Brown) 
Height (Joe, Short) Height(Sue, Short) 
Nationality(Joe, US) Nationality(Sue, US) 
Mother(Joe, Mary) Mother(Sue, Mary) 
Age (Joe, 8) Age(Sue, 6) 
The following domain theory is helpful for acquiring the HouseMatesconcept: 
HouseMates(x, y) t InSameFamily(x, y) 
HouseMates(x, y) t FraternityBrothers(x, y) 
InSameFamily(x, y) t Married(x, y) 
InSameFamily (x, y) t Youngster(x) A Youngster (y) A SameMother (x, y) 
SameMother(x, y) t Mother(x, z) A Mother(y, z) 
Youngster(x) t Age(x, a) A LessThan(a, 10) 
Apply the PROLOG-EBG algorithm to the task of generalizing from the aboveinstance, using the above domain theory. In particular, 
(a) Show a hand-trace of the PROLOG-EBG algorithm applied to this problem; thatis, show the explanation generated for the training instance, show the result ofregressing the target concept through this explanation, and show the resultingHorn clause rule. 
(b) Suppose that the target concept is "people who live with Joe" instead of "pairsof people who live together." Write down this target concept in terms of theabove formalism. Assuming the same training instance and domain theory asbefore, what Horn clause rule will PROLOG-EBG produce for this new targetconcept? 
As noted in Section 11.3.1, PROLOG-EBG can construct useful new features that arenot explicit features of the instances, but that are defined in terms of the explicitfeatures and that are useful for describing the appropriate generalization. Thesefeatures are derived asa side effect of analyzing the training example explanation. Asecond method for deriving useful features is the BACKPROPAGATION algorithm formultilayer neural networks, in which new features are learned by the hidden unitsbased on the statistical properties ofa large number of examples. Can you suggesta way in which one might combine these analytical and inductive approaches togenerating new features? (Warning: This isan open research problem.) 
REFERENCESAhn, W., Mooney, R. J., Brewer, W. F., & DeJong, G. F. (1987). Schema acquisition from oneexample: Psychological evidence for explanation-based learning. Ninth Annual Conference ofthe Cognitive Science Society (pp. 50-57). Hillsdale, NJ: Lawrence Erlbaum Associates. 
Bennett, S. W., & DeJong, G. F. (1996). Real-world robotics: Learning to plan for robust execution. 
Machine kaming, 23, 121. 
Carbonell, J., Knoblock, C., & Minton, S. (1990). PRODIGY: An integrated architecture for planningand learning. InK. VanLehn (Ed.), Architectures for Intelligence. Hillsdale, NJ: LawrenceErlbaum Associates. 
Chien, S. (1993). NONMON: Learning with recoverable simplifications. InG. DeJong (Ed.), Znvesti- 
gating explanation-based learning (pp. 41M34). Boston, MA: Kluwer Academic Publishers. 
Davies, T. R., and Russell, S. J. (1987). A logical approach to reasoning by analogy. Proceedings ofthe 10th International Joint Conference on ArtiJcial Intelligence (pp. 264-270). San Mateo, 
CA: Morgan Kaufmann. 
DeJong, G. (1981). Generalizations based on explanations. Proceedings of the Seventh InternationalJoint Conference on ArtiJicial Intelligence (pp. 67-70). 
DeJong, G., & Mooney, R. (1986). Explanation-based learning: An alternative view. Machine Learn- 
ing, 1(2), 145-176. 
DeJong, G. (Ed.). (1993). Investigating explanation-based learning. Boston, MA: Kluwer AcademicPublishers. 
DeJong, G. (1994). Learning to plan in continuous domains. ArtiJicial Intelligence, 64(1), 71-141. 
DeJong, G. (1997). Explanation-based learning. InA. Tucker (Ed.), The Computer Science andEngineering Handbook (pp. 499-520). Boca Raton, FL: CRC Press. 
Dietterich, T. G., Flann, N. S. (1995). Explanation-based learning and reinforcement learning: Aunified view. Proceedings of the 12th International Conference on Machine Learning (pp. 
176-184). San Mateo, CA: Morgan Kaufmann. 
Doorenbos, R. E. (1993). Matching 100,000 learned rules. Proceedings of the Eleventh NationalConference on ArtiJicial Intelligence (pp. 290-296). AAAI Press/MIT Press. 
Fikes, R., Hart, P., & Nisson, N. (1972). Learning and executing generalized robot plans. ArtiJicialIntelligence, 3(4), 251-288. 
Fisher, D., Subrarnanian, D., & Tadepalli, P. (1992). An overview of current research on knowl- 
edge compilation and speedup learning. Proceedings of the Second International Workshop onKnowledge Compilation and Speedup Learning. 
Flann, N. S., & Dietterich, T. G. (1989). A study of explanation-based methods for inductive learning. 
Machine Learning, 4, 187-226. 
Gervasio, M. T., & DeJong, G. F. (1994). An incremental learning approach to completable planning. 
Proceedings of the Eleventh International Conference on Machine Learning, New Brunswick, 
NJ. San Mateo, CA: Morgan Kaufmann. 
van Harmelen, F., & Bundy, A. (1988). Explanation-based generalisation = partial evaluation. Arti- 
ficial Intelligence, 36(3), 401-412. 
Kedar-Cabelli, S., & McCarty, T. (1987). Explanation-based generalization as resolution theoremproving. Proceedings of the Fourth International Workshop on Machine Learning (pp. 383- 
389). San Francisco: Morgan Kaufmann. 
Kotovsky, L., & Baillargeon, R. (1994). Calibration-based reasoning about collision events in 11- 
month-old infants. Cognition, 51, 107-129. 
Laird, J. E., Rosenbloom, P. S., & Newell, A. (1986). Chunking in SOAR: The anatomy ofa generallearning mechanism. Machine Learning, 1, 11. 
Mahadevan, S., Mitchell, T., Mostow, D. J., Steinberg, L., & Tadepalli, P. (1993). An apprentice- 
based approach to knowledge acquisition. InS. Mahadevan, T. Mitchell, D. J. Mostow, L. 
Steinberg, & P. Tadepalli (Eds.), ArtiiJicial Intelligence, 64(1), 1-52. 
Minton, S. (1988). Learning search control knowledge: An explanation-based approach. Boston, MA: 
Kluwer Academic Publishers. 
Miton, S., Carbonell, J., Knoblock, C., Kuokka, D., Etzioni, O., & Gil, Y. (1989). Explanation-basedleaming: A problem solving perspective. ArtiJicial Intelligence, 40, 63-1 18. 
Minton, S. (1990). Quantitative results concerning the utility of explanation-based leaming. ArtiJicialIntelligence, 42, 363-391. 
Mitchell, T. M. (1981). Toward combining empirical and analytical methods for inferring heuristics 
(Technical Report LCSR-TR-27), Rutgers Computer Science Department. (Also reprinted inA. Elithorn & R. Banerji (Eds), ArtiJicial and Human Intelligence. North-Holland, 1984.) 
Mitchell, T. M. (1983). Learning and problem-solving. Proceedings of the Eighth International JointConference on ArtiiJicial Intelligence. San Francisco: Morgan Kaufmann. 
Mitchell, T. M., Keller, R., & Kedar-Cabelli, S. (1986). Explanation-based generalization: A unifyingview. Machine Learning, 1(1), 47-80. 
Mitchell, T. M. (1990). Becoming increasingly reactive. Proceedings of the Eighth National Confer- 
ence on ArtQicial Intelligence. Medo Park, CA: AAAI Press. 
Mitchell, T. M., & Thrun, S. B. (1993). Explanation-based neural network learning for robot control. 
InS. Hanson etal. (Eds.), Advances in neural infomtionprocessing systems 5 (pp. 287-2941. 
San Mateo, CA: Morgan-Kaufmann Press. 
CHAF'TER 11 ANALYTICAL LEARNING 333Newell, A. (1990). Unified theories of cognition. Cambridge, MA: Harvard University Press. 
Qin, Y., Mitchell, T., & Simon, H. (1992). Using explanation-based generalization to simulate hu- 
man learning from examples and learning by doing. Proceedings of the Florida A1 ResearchSymposium (pp. 235-239). 
Ram, A., & Leake, D. B. (Eds.). (1995). Goal-driven learning. Cambridge, MA: MIT Press. 
Rosenblwm, P., & Laird, J. (1986). Mapping explanation-based generalization onto SOAR. FifihNational Conference on Artificial Intelligence (pp. 561-567). AAAI Press. 
Russell, S. (1989). The use of knowledge in analogy and induction. San Francisco: Morgan Kaufmann. 
Shavlik, J. W. (1990). Acquiring recursive and iterative concepts with explanation-based learning. 
Machine Learning, 5, 39. 
Silver, B. (1983). Learning equation solving methods from worked examples. Proceedings of theI983 International Workshop on Machine Learning (pp. 99-104). CS Department, Universityof Illinois at Urbana-Champaign. 
Silver, B. (1986). Precondition analysis: Learning control information. InR. Michalski etal. (Eds.), 
Machine Learning: AnAI approach (pp. 647470). San Mateo, CA. Morgan Kaufmann. 
Soloway, E. (1977). Knowledge directed learning using multiple levels of description (Ph.D. thesis). 
University of Massachusetts, Arnherst. 
Tadepalli, P. (1990). Tractable learning and planning in games (Technical report ML-TR-3 1) (Ph.D. 
dissertation). Rutgers University Computer Science Department. 
Tambe, M., Newell, A., & Rosenbloom, P. S. (1990). The problem of expensive chunks and itssolution by restricting expressiveness. Machine Learning, 5(4), 299-348. 
Waldinger, R. (1977). Achieving several goals simultaneously. InE. Elcock & D. Michie Pds.), 
Machine Intelligence 8. London: Ellis Horwood Ltd. 
Winston, P., Binford, T., Katz, B., & Lowry, M. (1983). Learning physical descriptions from func- 
tional definitions, examples, and precedents. Proceedings of the National Conference on Arti- 
jcial Intelligence (pp. 433-439). san Mateo, CA: Morgan Kaufmann. 
Wisniewski, E. J., & Medin, D. L. (1995). Harpoons and long sticks: The interaction of theoryand similarity in rule induction. InA. Ram & D. B. Leake (Eds.), Goal-driven learning @p. 
177-210). Cambridge, MA: MIT Press. 
CHAPTERCOMBININGINDUCTIVE ANDANALYTICALLEARNINGPurely inductive learning methods formulate general hypotheses by finding empir- 
ical regularities over the training examples. Purely analytical methods use priorknowledge to derive general hypotheses deductively., This chapter considers meth- 
ods that combine inductive and analytical mechanisms to obtain the benefits of bothapproaches: better generalization accuracy when prior knowledge is available and re- 
liance on observed training data to overcome shortcomings in prior knowledge. Theresulting combined methods outperform both purely inductive and purely analyti- 
cal learning methods. This chapter considers inductive-analytical learning methodsbased on both symbolic and artificial neural network representations. 
12.1 MOTIVATIONIn previous chapters we have seen two paradigms for machine learning: inductivelearning and analytical learning. Inductive methods, such as decision tree induc- 
tion and neural network BACKPROPAGATION, seek general hypotheses that fit theobserved training data. Analytical methods, such as PROLOG-EBG, seek generalhypotheses that fit prior knowledge while covering the observed data. These twolearning paradigms are based on fundamentally different justifications for learnedhypotheses and offer complementary advantages and disadvantages. Combiningthem offers the possibility of more powerful learning methods. 
CHAPTER 12 COMBINING INDUCTIVE AND ANALYTICAL LEARNING 335Purely analytical learning methods offer the advantage of generalizing moreaccurately from less data by using prior knowledge to guide learning. However, 
they can be misled when given incorrect or insufficient prior knowledge. Purelyinductive methods offer the advantage that they require no explicit prior knowl- 
edge and learn regularities based solely on the training data. However, they canfail when given insufficient training data, and can be misled by the implicit in- 
ductive bias they must adopt in order to generalize beyond the observed data. 
Table 12.1 summarizes these complementary advantages and pitfalls of induc- 
tive and analytical learning methods. This chapter considers the question of howto combine the two into a single algorithm that captures the best aspects ofboth. 
The difference between inductive and analytical learning methods can beseen in the nature of the justiJications that can be given for their learned hypothe- 
ses. Hypotheses output by purely analytical learning methods such as PROLOG- 
EBG carry a logical justification; the output hypothesis follows deductively fromthe domain theory and training examples. Hypotheses output by purely inductivelearning methods such as BACKPROPAGATION carry a statistical justification; theoutput hypothesis follows from statistical arguments that the training sample issufficiently large that itis probably representative of the underlying distributionof examples. This statistical justification for induction is clearly articulated in thePAC-learning results discussed in Chapter 7. 
Given that analytical methods provide logically justified hypotheses and in- 
ductive methods provide statistically justified hypotheses, itis easy to see whycombining them would be useful: Logical justifications are only as compelling asthe assumptions, or prior knowledge, on which they are built. They are suspect orpowerless if prior knowledge is incorrect or unavailable. Statistical justificationsare only as compelling as the data and statistical assumptions on which they rest. 
They are suspect or powerless when assumptions about the underlying distribu- 
tions cannot be trusted or when data is scarce. In short, the two approaches workwell for different types of problems. By combining them we can hope to devisea more general learning approach that covers a more broad range of learningtasks. 
Figure 12.1 summarizes a spectrum of learning problems that varies by theavailability of prior knowledge and training data. At one extreme, a large volumeInductive learning Analytical learningGoal: Hypothesis fits data Hypothesis fits domain theoryJustification: Statistical inference Deductive inferenceAdvantagex Requires little prior knowledge Learns from scarce dataPitfalls: Scarce data, incorrect bias Imperfect domain theoryTABLE 12.1Comparison of purely analytical and purely inductive learning. 
Inductive learning Analytical learningplentiful dataNo prior knowledgePerfect priorknowledgeScarce dataFIGURE 12.1A spectrum of learning tasks. At the left extreme, no prior knowledge is available, and purelyinductive learning methods with high sample complexity are therefore necessary. At the rightmostextreme, a perfect domain theory is available, enabling the use of purely analytical methods such asPROLOG-EBG. Most practical problems lie somewhere between these two extremes. 
of training data is available, but no prior knowledge. At the other extreme, strongprior knowledge is available, but little training data. Most practical learning prob- 
lems lie somewhere between these two extremes of the spectrum. For example, inanalyzing a database of medical records to learn "symptoms for which treatmentx is more effective than treatment y," one often begins with approximate priorknowledge (e.g., a qualitative model of the cause-effect mechanisms underlyingthe disease) that suggests the patient's temperature is more likely tobe relevantthan the patient's middle initial. Similarly, in analyzing a stock market databaseto learn the target concept "companies whose stock value will double over thenext 10 months," one might have approximate knowledge of economic causesand effects, suggesting that the gross revenue of the company is more likely tobe relevant than the color of the company logo. In both of these settings, ourown prior knowledge is incomplete, but is clearly useful in helping discriminaterelevant features from irrelevant. 
The question considered in this chapter is "What kinds of learning algo- 
rithms can we devise that make use of approximate prior knowledge, togetherwith available data, to form general hypotheses?' Notice that even when usinga purely inductive learning algorithm, one has the opportunity to make designchoices based on prior knowledge of the particular learning task. For example, 
when applying BACKPROPAGATION toa problem such as speech recognition, onemust choose the encoding of input and output data, the error function tobe rnin- 
imized during gradient descent, the number of hidden units, the topology of thenetwork, the learning rate and momentum, etc. In making these choices, humandesigners have the opportunity to embed task-specific knowledge into the learningalgorithm. The result, however, isa purely inductive instantiation of BACKPROPA- 
GATION, specialized by the designer's choices to the task of speech recognition. 
Our interest here lies in something different. We are interested in systems thattake prior knowledge asan explicit input to the learner, in the same sense thatthe training data isan explicit input, so that they remain general purpose algo- 
rithms, even while taking advantage of domain-specific knowledge. In brief, ourinterest here lies in domain-independent algorithms that employ explicitly inputdomain-dependent knowledge. 
What criteria should we use to compare alternative approaches to combininginductive and analytical learning? Given that the learner will generally not knowthe quality of the domain theory or the training data in advance, we are interestedCHAF'TER 12 COMBINING INDUCTIVE AND ANALYTICAL LEARNING 337in general methods that can operate robustly over the entire spectrum of problemsof Figure 12.1. Some specific properties we would like from such a learningmethod include: 
a Given no domain theory, it should learn at least as effectively as purelyinductive methods. 
Given a perfect domain theory, it should learn at least as effectively aspurely analytical methods. 
a Given an imperfect domain theory and imperfect training data, it shouldcombine the two to outperform either purely inductive or purely analyticalmethods. 
eIt should accommodate an unknown level of error in the training data. 
aIt should accommodate an unknown level of error in the domain theory. 
Notice this list of desirable properties is quite ambitious. For example, ac- 
commodating errors in the training data is problematic even for statistically basedinduction without at least some prior knowledge or assumption regarding the dis- 
tribution of errors. Combining inductive and analytical learning isan area of activecurrent research. While the above list isa fair summary of what we would likeour algorithms to accomplish, wedo not yet have algorithms that satisfy all theseconstraints ina fully general fashion. 
The next section provides a more detailed discussion of the combinedinductive-analytical learning problem. Subsequent sections describe three differ- 
ent approaches to combining approximate prior knowledge with available trainingdata to guide the learner's search for an appropriate hypothesis. Each of thesethree approaches has been demonstrated to outperform purely inductive meth- 
ods in multiple task domains. For ease of comparison, we use a single exampleproblem to illustrate all three approaches. 
12.2 INDUCTIVE-ANALYTICAL APPROACHES TO LEARNING12.2.1 The Learning ProblemTo summarize, the learning problem considered in this chapter isGiven: 
0 A set of training examples D, possibly containing errors0 A domain theory B, possibly containing errorsA space of candidate hypotheses HDetermine: 
A hypothesis that best fits the training examples and domain theoryWhat precisely shall we mean by "the hypothesis that best fits the trainingexamples and domain theory?'In particular, shall we prefer hypotheses that fitthe data a little better at the expense of fitting the theory less well, or vice versa? 
We can be more precise by defining measures of hypothesis error with respectto the data and with respect to the domain theory, then phrasing the question interms of these errors. Recall from Chapter 5 that errorD(h) is defined tobe theproportion of examples from D that are misclassified byh. Let us define the errorerror~(h) ofh with respect toa domain theory Bto be the probability that hwill disagree with Bon the classification ofa randomly drawn instance. We canattempt to characterize the desired output hypothesis in terms of these errors. Forexample, we could require the hypothesis that minimizes some combined measureof these errors, such asargmin kDerrorD (h) + kBerrorB (h) 
h€ HWhile this appears reasonable at first glance, itis not clear what values to assignto k~ and kgto specify the relative importance of fitting the data versus fitting thetheory. Ifwe have a very poor theory and a great deal of reliable data, it will bebest to weight error~(h) more heavily. Given a strong theory and a small sampleof very noisy data, the best results would be obtained by weighting errorB(h) 
more heavily. Of course if the learner does not know in advance the quality ofthe domain theory or training data, it will be unclear how it should weight thesetwo error components. 
An alternative perspective on the question of how to weight prior knowl- 
edge and data is the Bayesian perspective. Recall from Chapter 6 that Bayestheorem describes how to compute the posterior probability P(h1D) of hypothe- 
sis h given observed training data D. In particular, Bayes theorem computes thisposterior probability based on the observed data D, together with prior knowledgein the form ofP(h), P(D), and P(Dlh). Thus we can think ofP(h), P(D), andP(Dlh) asa form of background knowledge or domain theory, and we can thinkof Bayes theorem asa method for weighting this domain theory, together withthe observed data D, to assign a posterior probability P(hlD) toh. The Bayesianview is that one should simply choose the hypothesis whose posterior probabilityis greatest, and that Bayes theorem provides the proper method for weightingthe contribution of this prior knowledge and observed data. Unfortunately, Bayestheorem implicitly assumes pe$ect knowledge about the probability distributionsP(h), P(D), and P(Dlh). When these quantities are only imperfectly known, 
Bayes theorem alone does not prescribe how to combine them with the observeddata. (One possible approach in such cases isto assume prior probability distri- 
butions over P(h), P(D), and P(D1h) themselves, then calculate the expectedvalue of the posterior P (h 1 D) . However, this requires additional knowledge aboutthe priors over P(h), P(D), and P(Dlh), soit does not really solve the generalproblem.) 
We will revisit the question of what we mean by "best" fit to the hypothesisand data aswe examine specific algorithms. For now, we will simply say thatthe learning problem isto minimize some combined measure of the error of thehypothesis over the data and the domain theory. 
CAAPrW 12 COMBINJNG INDUCTIVE AND ANALYTICAL LEARNING 33912.2.2 Hypothesis Space SearchHow can the domain theory and training data best be combined to constrain thesearch for an acceptable hypothesis? This remains an open question in machinelearning. This chapter surveys a variety of approaches that have been proposed, 
many of which consist of extensions to inductive methods we have already studied 
(e.g., BACKPROPAGATION, FOIL). 
One way to understand the range of possible approaches isto return to ourview of learning asa task of searching through the space of alternative hypotheses. 
We can characterize most learning methods as search algorithms by describingthe hypothesis space H they search, the initial hypothesis hoat which they begintheir search, the set of search operators 0 that define individual search steps, andthe goal criterion G that specifies the search objective. In this chapter we explorethree different methods for using prior knowledge to alter the search performedby purely inductive methods. 
Use prior knowledge to derive an initial hypothesis from which to begin thesearch. In this approach the domain theory Bis used to construct an ini- 
tial hypothesis ho that is consistent with B. A standard inductive methodis then applied, starting with the initial hypothesis ho. For example, theKBANN system described below learns artificial neural networks in thisway. It uses prior knowledge to design the interconnections and weightsfor an initial network, so that this initial network is perfectly consistentwith the given domain theory. This initial network hypothesis is then re- 
fined inductively using the BACKPROPAGATION algorithm and available data. 
Beginning the search ata hypothesis consistent with the domain theorymakes it more likely that the final output hypothesis will better fit thistheory. 
Use prior knowledge to alter the objective of the hypothesis space search. 
In this approach, the goal criterion Gis modified to require that the out- 
put hypothesis fits the domain theory as well as the training examples. Forexample, the EBNN system described below learns neural networks in thisway. Whereas inductive learning of neural networks performs gradient de- 
scent search to minimize the squared error of the network over the trainingdata, EBNN performs gradient descent to optimize a different criterion. Thismodified criterion includes an additional term that measures the error of thelearned network relative to the domain theory. 
0 Use prior knowledge to alter the available search steps. In this approach, theset of search operators 0 is altered by the domain theory. For example, theFOCL system described below learns sets of Horn clauses in this way. It isbased on the inductive system FOIL, which conducts a greedy search throughthe space of possible Horn clauses, at each step revising its current hypoth- 
esis by adding a single new literal. FOCL uses the domain theory to expandthe set of alternatives available when revising the hypothesis, allowing theaddition of multiple literals ina single search step when warranted by thedomain theory. In this way, FOCL allows single-step moves through thehypothesis space that would correspond to many steps using the originalinductive algorithm. These "macro-moves" can dramatically alter the courseof the search, so that the final hypothesis found consistent with the data isdifferent from the one that would be found using only the inductive searchsteps. 
The following sections describe each of these approaches in turn. 
12.3 USING PRIOR KNOWLEDGE TO INITIALIZE THEHYPOTHESISOne approach to using prior knowledge isto initialize the hypothesis to perfectly fitthe domain theory, then inductively refine this initial hypothesis as needed to fit thetraining data. This approach is used by the KBANN (Knowledge-Based ArtificialNeural Network) algorithm to learn artificial neural networks. In KBANN an initialnetwork is first constructed so that for every possible instance, the classificationassigned by the network is identical to that assigned by the domain theory. TheBACKPROPAGATION algorithm is then employed to adjust the weights of this initialnetwork as needed to fit the training examples. 
Itis easy to see the motivation for this technique: if the domain theory iscorrect, the initial hypothesis will correctly classify all the training examples andthere will beno need to revise it. However, if the initial hypothesis is foundto imperfectly classify the training examples, then it will be refined inductivelyto improve its fit to the training examples. Recall that in the purely inductiveBACKPROPAGATION algorithm, weights are typically initialized to small randomvalues. The intuition behind KBANN is that even if the domain theory is onlyapproximately correct, initializing the network to fit this domain theory will give abetter starting approximation to the target function than initializing the network torandom initial weights. This should lead, in turn, to better generalization accuracyfor the final hypothesis. 
This initialize-the-hypothesis approach to using the domain theory has beenexplored by several researchers, including Shavlik and Towel1 (1989), Towel1and Shavlik (1994), Fu (1989, 1993), and Pratt (1993a, 1993b). We will usethe KBANN algorithm described in Shavlik and Towel1 (1989) to illustrate thisapproach. 
12.3.1 The KBANN AlgorithmThe KBANN algorithm exemplifies the initialize-the-hypothesis approach to usingdomain theories. It assumes a domain theory represented bya set of proposi- 
tional, nonrecursive Horn clauses. A Horn clause is propositional ifit contains novariables. The input and output of KBANN are as follows: 
- 
KBANN(Domain-Theory, Training_Examples) 
Domain-Theory: Set of propositional, nonrecursive Horn clauses. 
TrainingJxamples: Set of (input output) pairs of the targetfunction. 
Analytical step: Create an initial network equivalent to the domain theory. 
1. For each instance attribute create a network input. 
2. For each Horn clause in the Domain-Theory, create a network unit as follows: 
0 Connect the inputs of this unit to the attributes tested by the clause antecedents. 
For each non-negated antecedent of the clause, assign a weight ofW to the correspond- 
ing sigmoid unit input. 
For each negated antecedent of the clause, assign a weight of - Wto the correspondingsigmoid unit input. 
0 Set the threshold weight wo for this unit to -(n - .5)W, where nis the number ofnon-negated antecedents of the clause. 
3. Add additional connections among the network units, connecting each network unit at depthi from the input layer to all network units at depth i + 1. Assign random near-zero weights tothese additional connections. 
Inductive step: Refine the initial network. 
4. Apply the BACKPROPAGATION algorithm to adjust the initial network weights to fit theTraining-Examples. 
TABLE 12.2The KBANN algorithm. The domain theory is translated into an equivalent neural network (steps1-3), which is inductively refined using the BACKPROPAGATION algorithm (step 4). A typical valuefor the constant Wis 4.0. 
Given: 
0 A set of training examples0 A domain theory consisting of nonrecursive, propositional Horn clausesDetermine: 
0 An artificial neural network that fits the training examples, biased by thedomain theoryThe two stages of the KBANN algorithm are first to create an artificial neuralnetwork that perfectly fits the domain theory and second to use the BACKPROPA- 
CATION algorithm to refine this initial network to fit the training examples. Thedetails of this algorithm, including the algorithm for creating the initial network, 
are given in Table 12.2 and illustrated in Section 12.3.2. 
12.3.2 An Illustrative ExampleTo illustrate the operation of KBANN, consider the simple learning problem sum- 
marized in Table 12.3, adapted from Towel1 and Shavlik (1989). Here each in- 
stance describes a physical object in terms of the material from which itis made, 
whether itis light, etc. The task isto learn the target concept Cup defined oversuch physical objects. Table 12.3 describes a set of training examples and ado- 
main theory for the Cup target concept. Notice the domain theory defines a CupDomain theory: 
Cup t Stable, Lzpable, OpenVesselStable t BottomIsFlatLijiable t Graspable, LightGraspable t HasHandleOpenVessel t HasConcavity, ConcavityPointsUpTraining examples: 
BottomIsFlatConcavitjPointsUpExpensiveFragileHandleOnTopHandleOnSideHasConcavityHasHandleLightMadeOfCeramicMadeOfPaperMadeOfstyrofoamcupsJJJJJJJJJ JJJJ 4JJJJJ JJJJJJ 
JJ JNon-Cups2/44 JJ JJJ JJJ JJJ JJJ JJJJJ J JJJJ JJ JJJ 
J JTABLE 12.3The Cup learning task. An approximate domain theory and a set of training examples for the targetconcept Cup. 
asan object that is Stable, Liftable, and an OpenVessel. The domain theory alsodefines each of these three attributes in terms of more primitive attributes, tenni- 
nating in the primitive, operational attributes that describe the instances. Note thedomain theory is not perfectly consistent with the training examples. For example, 
the domain theory fails to classify the second and third training examples as pos- 
itive examples. Nevertheless, the domain theory forms a useful approximation tothe target concept. KBANN uses the domain theory and training examples togetherto learn the target concept more accurately than it could from either alone. 
In the first stage of the KBANN algorithm (steps 1-3 in the algorithm), aninitial network is constructed that is consistent with the domain theory. For exam- 
ple, the network constructed from the Cup domain theory is shown in Figure 12.2. 
In general the network is constructed by creating a sigmoid threshold unit for eachHorn clause in the domain theory. KBANN follows the convention that a sigmoidoutput value greater than 0.5 is interpreted as True and a value below 0.5 as False. 
Each unit is therefore constructed so that its output will be greater than 0.5 justin those cases where the corresponding Horn clause applies. For each antecedentto the Horn clause, an input is created to the corresponding sigmoid unit. Theweights of the sigmoid unit are then set so that it computes the logical AND ofits inputs. In particular, for each input corresponding toa non-negated antecedent, 
CHAPTER 12 COMBINING INDUCTIVE AND ANALYTICAL LEARNING 343ExpensiveRottomlsFlatMadeofceramicMadeofstyrofoamMadeOfPaperHasHandleHandleOnTopHandleonsideLightHasconcavityConcavityPointsUpFragileStableLifableFIGURE 12.2A neural network equivalent to the domain theory. This network, created in the first stage of theKBANN algorithm, produces output classifications identical to those of the given domain theoryclauses. Dark lines indicate connections with weight W and correspond to antecedents of clausesfrom the domain theory. Light lines indicate connections with weights of approximately zero. 
the weight is set to some positive constant W. For each input corresponding to anegated antecedent, the weight is set to - W. The threshold weight of the unit, wois then set to -(n- .5) W, where nis the number of non-negated antecedents. Whenunit input values are 1 or 0, this assures that their weighted sum plus wo will bepositive (and the sigmoid output will therefore be greater than 0.5) if and only ifall clause antecedents are satisfied. Note for sigmoid units at the second and sub- 
sequent layers, unit inputs will not necessarily be 1 and 0 and the above argumentmay not apply. However, ifa sufficiently large value is chosen for W, this KBANNalgorithm can correctly encode the domain theory for arbitrarily deep networks. 
Towell and Shavlik (1994) report using W = 4.0 in many of their experiments. 
Each sigmoid unit input is connected to the appropriate network input or tothe output of another sigmoid unit, to mirror the graph of dependencies amongthe corresponding attributes in the domain theory. Asa final step many additionalinputs are added to each threshold unit, with their weights set approximately tozero. The role of these additional connections isto enable the network to induc- 
tively learn additional dependencies beyond those suggested by the given domaintheory. The solid lines in the network of Figure 12.2 indicate unit inputs withweights ofW, whereas the lightly shaded lines indicate connections with initialweights near zero. Itis easy to verify that for sufficiently large values ofW thisnetwork will output values identical to the predictions of the domain theory. 
The second stage of KBANN (step 4 in the algorithm of Table 12.2) usesthe training examples and the BACWROPAGATION algorithm to refine the initialnetwork weights. Of course if the domain theory and training examples containno errors, the initial network will already fit the training data. In the Cup ex- 
ample, however, the domain theory and training data are inconsistent, and thisstep therefore alters the initial network weights. The resulting trained networkis summarized in Figure 12.3, with dark solid lines indicating the largest posi- 
tive weights, dashed lines indicating the largest negative weights, and light lines344 MACHINE LEARNINGExpensive a;.- ,,*: ~ .- -...* ". , ,--" . 
BottodsFlat ' '*' StableMadeOfCeramicMadeofstyrofoamMadeOfPaperHasHandle LifrableHandleOnTopHandleOnSideLight 
\ cupOpen-VesselHasConcaviiyConcavityPointsUp ,i ,,.,,... *. 
Fragile ". ""- 
Large negative weight 
"- " Negligible weightFIGURE 12.3Result of inductively refining the initial network. KBANN uses the training examples to modifythe network weights derived from the domain theory. Notice the new dependency of Lifable onMadeOfStyrofoam and HandleOnTop. 
indicating negligible weights. Although the initial network rnisclassifies severaltraining examples from Table 12.3, the refined network of Figure 12.3 perfectlyclassifies all of these training examples. 
Itis interesting to compare the final, inductively refined network weights tothe initial weights derived from the domain theory. As can be seen in Figure 12.3, 
significant new dependencies were discovered during the inductive step, includingthe dependency of the Liftable unit on the feature MadeOfStyrofoam. Itis impor- 
tant to keep in mind that while the unit labeled Liftable was initially definedby the given Horn clause for Liftable, the subsequent weight changes performedby BACKPROPAGATION may have dramatically changed,the meaning of this hiddenunit. After training of the network, this unit may take ona very different meaningunrelated to the initial notion of Liftable. 
12.3.3 RemarksTo summarize, KBANN analytically creates a network equivalent to the givendomain theory, then inductively refines this initial hypothesis to better fit thetraining data. In doing so, it modifies the network weights as needed to overcomeinconsistencies between the domain theory and observed data. 
The chief benefit of KBANN over purely inductive BACKPROPAGATION (be- 
ginning with random initial weights) is that it typically generalizes more accuratelythan BACKPROPAGATION when given an approximately correct domain theory, es- 
pecially when training data is scarce. KBANN and other initialize-the-hypothesisapproaches have been demonstrated to outperform purely inductive systems inseveral practical problems. For example, Towel1 etal. (1990) describe the appli- 
cation of KBANN toa molecular genetics problem. Here the task was to learn toCHAPTER 12 COMBINING INDUCTIVE AND ANALYTICAL LEARNING 345recognize DNA segments called promoter regions, which influence gene activity. 
In this experiment KBANN was given an initial domain theory obtained from amolecular geneticist, and a set of 53 positive and 53 negative training examplesof promoter regions. Performance was evaluated using a leave-one-out strategyin which the system was run 106 different times. On each iteration KBANN wastrained using 105 of the 106 examples and tested on the remaining example. Theresults of these 106 experiments were accumulated to provide an estimate of thetrue error rate. KBANN obtained an error rate of 41106, compared toan error rateof 81106 using standard BACKPROPAGATION. A variant of the KBANN approach wasapplied byFu (1993), who reports an error rate of 21106 on the same data. Thus, 
the impact of prior knowledge in these experiments was to reduce significantlythe error rate. The training data for this experiment is available at World WideWeb site http:llwww.ics.uci.edu/~mlearn/MLRepository.html. 
Both Fu (1993) and Towel1 etal. (1990) report that Horn clauses extractedfrom the final trained network provided a refined domain theory that better fitthe observed data. Although itis sometimes possible to map from the learnednetwork weights back toa refined set of Horn clauses, in the general case thisis problematic because some weight settings have no direct Horn clause analog. 
Craven and Shavlik (1994) and Craven (1996) describe alternative methods forextracting symbolic rules from learned networks. 
To understand the significance of KBANN itis useful to consider how itshypothesis search differs from that of the purely inductive BACKPROPAGATION al- 
gorithm. The hypothesis space search conducted by both algorithms is depictedschematically in Figure 12.4. As shown there, the key difference is the initialhypothesis from which weight tuning is performed. In the case that multiple hy- 
potheses (weight vectors) can be found that fit the data-a condition that will beespecially likely when training data is scarce-KBANN is likely to converge to ahypothesis that generalizes beyond the data ina way that is more similar to thedomain theory predictions. On the other hand, the particular hypothesis to whichBACKPROPAGATION converges will more likely bea hypothesis with small weights, 
corresponding roughly toa generalization bias of smoothly interpolating betweentraining examples. In brief, KBANN uses a domain-specific theory to bias gen- 
eralization, whereas BACKPROPAGATION uses a domain-independent syntactic biastoward small weight values. Note in this summary we have ignored the effect oflocal minima on the search. 
Limitations of KBANN include the fact that it can accommodate only propo- 
sitional domain theories; that is, collections of variable-free Horn clauses. Itis alsopossible for KBANN tobe misled when given highly inaccurate domain theories, 
so that its generalization accuracy can deteriorate below the level of BACKPROPA- 
GATION. Nevertheless, it and related algorithms have been shown tobe useful forseveral practical problems. 
KBANN illustrates the initialize-the-hypothesis approach to combining ana- 
lytical and inductive learning. Other examples of this approach include Fu (1993); 
Gallant (1988); Bradshaw etal. (1989); Yang and Bhargava (1990); Lacher etal. 
(1991). These approaches vary in the exact technique for constructing the initialHypothesis SpaceHypotheses thatfit training dataequally wellInitial hypothesisfor KBANN 
\ 
i -Initial hypothesisfor BACKPROPAGATIOI\~ 
FIGURE 12.4Hypothesis space search in KBANN. KBANN initializes the network to fit the domain theory, whereasBACKPROPAGATION initializes the network to small random weights. Both then refine the weightsiteratively using the same gradient descent rule. When multiple hypotheses can be found that fit thetraining data (shaded region), KBANN and BACKPROPAGATION are likely to find different hypothesesdue to their different starting points. 
network, the application of BACKPROPAGATION to weight tuning, and in methods forextracting symbolic descriptions from the refined network. Pratt (1993a, 1993b) 
describes an initialize-the-hypothesis approach in which the prior knowledge isprovided bya previously learned neural network for a related task, rather thana manually provided symbolic domain theory. Methods for training the valuesof Bayesian belief networks, as discussed in Section 6.11, can also be viewedas using prior knowledge to initialize the hypothesis.. Here the prior knowledgecorresponds toa set of conditional independence assumptions that determine thegraph structure of the Bayes net, whose conditional probability tables are theninduced from the training data. 
12.4 USING PRIOR KNOWLEDGE TO ALTER THE SEARCHOBJECTIVEThe above approach begins the gradient descent search with a hypothesis thatperfectly fits the domain theory, then perturbs this hypothesis as needed to maxi- 
mize the fit to the training data. An alternative way of using prior knowledge isto incorporate it into the error criterion minimized by gradient descent, so that thenetwork must fit a combined function of the training data and domain theory. Inthis section, we consider using prior knowledge in this fashion. In particular, weconsider prior knowledge in the form of known derivatives of the target function. 
Certain types of prior knowledge can be expressed quite naturally in this form. 
For example, in training a neural network to recognize handwritten characters weCHAF'TER 12 COMBINING INDUCTIVE AND ANALYTICAL LEARNiNG 347can specify certain derivatives of the target function in order to express our priorknowledge that "the identity of the character is independent of small translationsand rotations of the image." 
Below we describe the TANGENTPROP algorithm, which trains a neural net- 
work to fit both training values and training derivatives. Section 12.4.4 then de- 
scribes how these training derivatives can be obtained from a domain theorysimilar to the one used in the Cup example of Section 12.3. In particular, itdiscusses how the EBNN algorithm constructs explanations of individual train- 
ing examples in order to extract training derivatives for use by TANGENTPROP. 
TANGENTPROP and EBNN have been demonstrated to outperform purely inductivemethods ina variety of domains, including character and object recognition, androbot perception and control tasks. 
12.4.1 The TANGENTPROP AlgorithmTANGENTPROP (Simard etal. 1992) accommodates domain knowledge expressedas derivatives of the target function with respect to transformations of its inputs. 
Consider a learning task involving an instance space X and target function f. Upto now we have assumed that each training example consists ofa pair (xi, f (xi)) 
that describes some instance xi and its training value f (xi). The TANGENTPROPalgorithm assumes various training derivatives of the target function are alsoprovided. For example, if each instance xiis described bya single real value, 
then each training example may beof the form (xi, f (xi), qlx, ). Here lx, 
denotes the derivative of the target function f with respect tox, evaluated at thepoint x = xi. 
To develop an intuition for the benefits of providing training derivatives aswell as training values during learning, consider the simple learning task depictedin Figure 12.5. The task isto learn the target function f shown in the leftmost plotof the figure, based on the three training examples shown: (xl, f (xl)), (x2, f (x2)), 
and (xg, f (xg)). Given these three training examples, the BACKPROPAGATION algo- 
rithm can be expected to hypothesize a smooth function, such as the function gdepicted in the middle plot of the figure. The rightmost plot shows the effect ofFIGURE 12.5Fitting values and derivatives with TANGENTPROP. Let fbe the target function for which three ex- 
amples (XI, f (xi)), (x2, f (x2)), and (x3, f (x3)) are known. Based on these points the learner mightgenerate the hypothesis g. If the derivatives are also known, the learner can generalize more accu- 
rately h. 
providing training derivatives, or slopes, as additional information for each train- 
ing example (e.g., (XI, f (XI), I,, )). By fitting both the training values f (xi) 
and these training derivatives PI,, the learner has a better chance to correctlygeneralize from the sparse training data. To summarize, the impact of includingthe training derivatives isto override the usual syntactic inductive bias of BACK- 
PROPAGATION that favors a smooth interpolation between points, replacing it byexplicit input information about required derivatives. The resulting hypothesis hshown in the rightmost plot of the figure provides a much more accurate estimateof the true target function f. 
In the above example, we considered only simple kinds of derivatives ofthe target function. In fact, TANGENTPROP can accept training derivatives withrespect to various transformations of the input x. Consider, for example, the taskof learning to recognize handwritten characters. In particular, assume the inputx corresponds toan image containing a single handwritten character, and thetask isto correctly classify the character. In this task, we might be interested ininforming the learner that "the target function is invariant to small rotations ofthe character within the image." In order to express this prior knowledge to thelearner, we first define a transformation s(a, x), which rotates the image xby a! 
degrees. Now we can express our assertion about rotational invariance by statingthat for each training instance xi, the derivative of the target function with respectto this transformation is zero (i.e., that rotating the input image does not alter thevalue of the target function). In other words, we can assert the following trainingderivative for every training instance xiaf ($(a, xi)) =o aawhere fis the target function and s(a, xi) is the image resulting from applyingthe transformation sto the image xi. 
How are such training derivatives used by TANGENTPROP to constrain theweights of the neural network? In TANGENTPROP these training derivatives areincorporated into the error function that is minimized by gradient descent. Recallfrom Chapter 4 that the BACKPROPAGATION algorithm performs gradient descent toattempt to minimize the sum of squared errorswhere xi denotes the ith training instance, f denotes the true target function, andf denotes the function represented by the learned neural network. 
In TANGENTPROP an additional term is added to the error function to penal- 
ize discrepancies between the trainin4 derivatives and the actual derivatives ofthe learned neural network function f. In general, TANGENTPROP accepts multi- 
ple transformations (e.g., we might wish to assert both rotational invariance andtranslational invariance of the character identity). Each transformation must beof the form sj(a, x) where a! isa continuous parameter, where sjis differen- 
tiable, and where sj(O, x) = x (e.g., for rotation of zero degrees the transforma- 
tion is the identity function). For each such transformation, sj(a!, x), TANGENT- 
CHAPTER 12 COMBINING INDUCTIVE AND ANALYTICAL LEARNING 349PROP considers the squared error between the specified training derivative andthe actual derivative of the learned neural network. The modified error func- 
tion iswhere pis a constant provided by the user to determine the relative importanceof fitting training values versus fitting training derivatives. Notice the first termin this definition ofE is the original squared error of the network versus trainingvalues, and the second term is the squared error in the network versus trainingderivatives. 
Simard etal. (1992) give the gradient descent rule for minimizing this ex- 
tended error function E. It can be derived ina fashion analogous to the derivationgiven in Chapter 4 for the simpler BACKPROPAGATION rule. 
12.4.2 An Illustrative ExampleSimard etal. (1992) present results comparing the generalization accuracy of TAN- 
GENTPROP and purely inductive BACKPROPAGATION for the problem of recognizinghandwritten characters. More specifically, the task in this case isto label imagescontaining a single digit between 0 and 9. In one experiment, both TANGENT- 
PROP and BACKPROPAGATION were trained using training sets of varying size, thenevaluated based on their performance over a separate test set of 160 examples. 
The prior knowledge given to TANGENTPROP was the fact that the classificationof the digit is invariant of vertical and horizontal translation of the image (i.e., 
that the derivative of the target function was 0 with respect to these transforma- 
tions). The results, shown in Table 12.4, demonstrate the ability of TANGENTPROPusing this prior knowledge to generalize more accurately than purely inductiveBACKPROPAGATION. 
Trainingset size10204080160320Percent error on test setTANGENTPROP I BACKPROPAGATIONTABLE 12.4Generalization accuracy for TANGENTPROP and BACKPROPAGATION, for handwritten digit recognition. 
TANGENTPROP generalizes more accurately due to its prior knowledge that the identity of the digitis invariant of translation. These results are from Sirnard etal. (1992). 
12.4.3 RemarksTo summarize, TANGENTPROP uses prior knowledge in the form of desired deriva- 
tives of the target function with respect to transformations of its inputs. It combinesthis prior knowledge with observed training data, by minimizing an objective func- 
tion that measures both the network's error with respect to the training examplevalues (fitting the data) and its error with respect to the desired derivatives (fittingthe prior knowledge). The value ofp determines the degree to which the networkwill fit one or the other of these two components in the total error. The behaviorof the algorithm is sensitive top, which must be chosen by the designer. 
Although TANGENTPROP succeeds in combining prior knowledge with train- 
ing data to guide learning of neural networks, itis not robust to errors in the priorknowledge. Consider what will happen when prior knowledge is incorrect, thatis, when the training derivatives input to the learner do not correctly reflect thederivatives of the true target function. In this case the algorithm will attempt to fitincorrect derivatives. It may therefore generalize less accurately than ifit ignoredthis prior knowledge altogether and used the purely inductive BACKPROPAGATIONalgorithm. Ifwe knew in advance the degree of error in the training derivatives, 
we might use this information to select the constant p that determines the relativeimportance of fitting training values and fitting training derivatives. However, thisinformation is unlikely tobe known in advance. In the next section we discussthe EBNN algorithm, which automatically selects values for pon an example-by- 
example basis in order to address the possibility of incorrect prior knowledge. 
Itis interesting to compare the search through hypothesis space (weightspace) performed by TANGENTPROP, KBANN, and BACKPROPAGATION. TANGENT- 
PROP incorporates prior knowledge to influence the hypothesis search by alteringthe objective function tobe minimized by gradient descent. This corresponds toaltering the goal of the hypothesis space search, as illustrated in Figure 12.6. LikeBACKPROPAGATION (but unlike KBANN), TANGENTPROP begins the search with aninitial network of small random weights. However, the gradient descent trainingrule produces different weight updates than BACKPROPAGATION, resulting ina dif- 
ferent final hypothesis. As shown in the figure, the set of hypotheses that minimizesthe TANGENTPROP objective may differ from the set that minimizes the BACKPROP- 
AGATION objective. Importantly, if the training examples and prior knowledge areboth correct, and the target function can be accurately represented by the ANN, 
then the set of weight vectors that satisfy the TANGENTPROP objective will be asubset of those satisfying the weaker BACKPROPAGATION objective. The differencebetween these two sets of final hypotheses is the set of incorrect hypotheses thatwill be considered by BACKPROPAGATION, but ruled out by TANGENTPROP due toits prior knowledge. 
Note one alternative to fitting the training derivatives of the target functionis to simply synthesize additional training examples near the observed trainingexamples, using the known training derivatives to estimate training values forthese nearby instances. For example, one could take a training image in the abovecharacter recognition task, translate ita small amount, and assert that the trans- 
Hypothesis SpaceHypotheses that Hypotheses thatmaximize fit to maximizefit to datadata and priorknowledgeTANGENTPROPSearch BACKPROPAGATIONSearchFIGURE 12.6Hypothesis space search in TANGENTPROP. TANGENTPROP initializes the network to small randomweights, just asin BACKPROPAGATION. However, it uses a different error function to drive the gradientdescent search. The error used by TANGENTPROP includes both the error in predicting training valuesand in predicting the training derivatives provided as prior knowledge. 
lated image belonged to the same class as the original example. We might expectthat fitting these synthesized examples using BACKPROPAGATION would produceresults similar to fitting the original training examples and derivatives using TAN- 
GENTPROP. Simard etal. (1992) report experiments showing similar generalizationerror in the two cases, but report that TANGENTPROP converges considerably moreefficiently. Itis interesting to note that the ALVINN system, which learns to steeran autonomous vehicle (see Chapter 4), uses a very similar approach to synthesizeadditional training examples. It uses prior knowledge of how the desired steer- 
ing direction changes with horizontal translation of the camera image to createmultiple synthetic training examples to augment each observed training example. 
12.4.4 The EBNN AlgorithmThe EBNN (Explanation-Based Neural Network learning) algorithm (Mitchell andThrun 1993a; Thrun 1996) builds on the TANGENTPROP algorithm in two significantways. First, instead of relying on the user to provide training derivatives, EBNNcomputes training derivatives itself for each observed training example. Thesetraining derivatives are calculated by explaining each training example in termsof a given domain theory, then extracting training derivatives from this explana- 
tion. Second, EBNN addresses the issue of how to weight the relative importanceof the inductive and analytical components of learning (i.e., how to select theparameter pin Equation [12.1]). The value ofp is chosen independently for eachtraining example, based ona heuristic that considers how accurately the domaintheory predicts the training value for this particular example. Thus, the analyticalcomponent of learning is emphasized for those training examples that are correctlyexplained by the domain theory and de-emphasized for training examples that arepoorly explained. 
The inputs to EBNN include (1) a set of training examples of the form 
(xi, f (xi)) with no training derivatives provided, and (2) a domain theory analo- 
gous to that used in explanation-based learning (Chapter 11) and in KBANN, butrepresented bya set of previously trained neural networks rather than a set ofHorn clauses. The output of EBNN isa new neural network that approximates thetarget function f. This learned network is trained to fit both the training examples 
(xi, f (xi)) and training derivatives off extracted from the domain theory. Fittingthe training examples (xi, f (xi)) constitutes the inductive component of learning, 
whereas fitting the training derivatives extracted from the domain theory providesthe analytical component. 
To illustrate the type of domain theory used by EBNN, consider Figure 12.7. 
The top portion of this figure depicts an EBNN domain theory for the target func- 
tion Cup, with each rectangular block representing a distinct neural network in thedomain theory. Notice in this example there is one network for each of the Hornclauses in the symbolic domain theory of Table 12.3. For example, the networklabeled Graspable takes as input the description ofan instance and produces asoutput a value indicating whether the object is graspable (EBNN typically repre- 
sents true propositions by the value 0.8 and false propositions by the value 0.2). 
This network is analogous to the Horn clause for Graspable given in Table 12.3. 
Some networks take the outputs of other networks as their inputs (e.g., the right- 
most network labeled Cup takes its inputs from the outputs of the Stable, Lifable, 
and OpenVessel networks). Thus, the networks that make up the domain theorycan be chained together to infer the target function value for the input instance, 
just as Horn clauses might be chained together for this purpose. In general, thesedomain theory networks may be provided to the learner by some external source, 
or they may be the result of previous learning by the same system. EBNN makesuse of these domain theory networks to learn the new,target function. It does notalter the domain theory networks during this process. 
The goal of EBNN isto learn a new neural network to describe the targetfunction. We will refer to this new network as the target network. In the example ofFigure 12.7, the target network Cup,,,,,, shown at the bottom of the figure takesas input the description ofan arbitrary instance and outputs a value indicatingwhether the object isa Cup. 
EBNN learns the target network by invoking the TANGENTPROP algorithmdescribed in the previous section. Recall that TANGENTPROP trains a network to fitboth training values and training derivatives. EBNN passes along to TANGENTPROPthe training values (xi, f (xi)) that it receives as input. In addition, EBNN providesTANGENTPROP with derivatives that it calculates from the domain theory. To seehow EBNN calculates these training derivatives, consider again Figure 12.7. Thetop portion of this figure shows the domain theory prediction of the target functionvalue for a particular training instance, xi. EBNN calculates the derivative of thisprediction with respect to each feature of the input instance. For the example in thefigure, the instance xiis described by features such as MadeOf Styrof oam = 0.2CHAFER 12 COMBINING INDUCTIVE AND ANALYTICAL LEARNING 353Explanation oftraining examplein terms ofdomain theory: 
BonomlsFku = T - 
ConcavilyPoinrsUp = T- 
Expensive = T- 
Fragile = T - 
HandIeOnTop = F - 
HandleOdide = T - 
HasConcovity =T - 
HosHandle = T - 
Light =T- 
M&ofcemic = T- 
MadeOfPoper = F - 
Modeofstyrofoam =F7- 
Target network: BO~O~ISFIU~ 
ConcaviryPointsUpExpensiveFmgileHandIeOnTopHandleOnSideHosConcavity CUPHasHandleLightMadeofceramicMadeofpaperMadeofstyrofoarnTrainingderivativesFIGURE 12.7Explanation ofa training example in EBNN. The explanation consists ofa prediction of the targetfunction value by the domain theory networks (top). Training derivatives are extracted from thisexplanation in order to train the separate target network (bottom). Each rectangular block representsa distinct multilayer neural network. 
(i.e., False), and the domain theory prediction is that Cup = 0.8 (i.e., True). 
EBNN calculates the partial derivative of this prediction with respect to eachinstance feature, yielding the set of derivativesacup acupaBottomIsFlat ' aConcavityPointsUp " " aMadeOf acup Styrof oam 1 This set of derivatives is the gradient of the domain theory prediction function withrespect to the input instance. The subscript refers to the fact that these derivatives354 MACHINE LEARNINGhold when x = xi. In the more general case where the target function has multipleoutput units, the gradient is computed for each of these outputs. This matrix ofgradients is called the Jacobian of the target function. 
To see the importance of these training derivatives in helping to learn thetarget network, consider the derivative ,E~~~i,,e. If the domain theory encodesthe knowledge that the feature Expensive is irrelevant to the target function Cup, 
then the derivative ,E~~e~i,,e extracted from the explanation will have the valuezero. A derivative of zero corresponds to the assertion that a change in the fea- 
ture Expensive will have no impact on the predicted value of Cup. On the otherhand, a large positive or negative derivative corresponds to the assertion that thefeature is highly relevant to determining the target value. Thus, the derivativesextracted from the domain theory explanation provide important information fordistinguishing relevant from irrelevant features. When these extracted derivativesare provided as training derivatives to TANGENTPROP for learning the target net- 
work Cup,,,,,,, they provide a useful bias for guiding generalization. The usualsyntactic inductive bias of neural network learning is replaced in this case by thebias exerted by the derivatives obtained from the domain theory. 
Above we described how the domain theory prediction can be used to gen- 
erate a set of training derivatives. Tobe more precise, the full EBNN algorithmis as follows. Given the training examples and domain theory, EBNN first cre- 
ates a new, fully connected feedforward network to represent the target function. 
This target network is initialized with small random weights, just asin BACK- 
PROPAGATION. Next, for each training example (xi, f (xi)) EBNN determines thecorresponding training derivatives ina two-step process. First, it uses the domaintheory to predict the value of the target function for instance xi. Let A(xi) de- 
note this domain theory prediction for instance xi. In other words, A(xi) is thefunction defined by the composition of the domain theory networks forming theexplanation for xi. Second, the weights and activations of the domain theory net- 
works are analyzed to extract the derivatives ofA(xi) 'with respect to each of thecomponents ofxi (i.e., the Jacobian ofA(x) evaluated atx = xi). Extracting thesederivatives follows a process very similar to calculating the 6 terms in the BACK- 
PROPAGATION algorithm (see Exercise 12.5). Finally, EBNN uses a minor variantof the TANGENTPROP algorithm to train the target network to fit the following errorfunctionwhereHere xi denotes the ith training instance and A(x) denotes the domain theoryprediction for input x. The superscript notation xj denotes the jth component ofthe vector x (i.e., the jth input node of the neural network). The coefficient c isa normalizing constant whose value is chosen to assure that for all i, 0 5 pi 5 1. 
CHAPTER 12 COMBmG INDUCTIVE AND ANALYTICAL LEARNING 355Although the notation here appears a bit tedious, the idea is simple. Theerror given by Equation (12.2) has the same general form as the error functionin Equation (12.1) minimized by TANGENTPROP. The leftmost term measures theusual sum of squared errors between the training value f (xi) and the value pre- 
dicted by the target network f"(xi). The rightmost term measures the squared errorbetween the training derivatives extracted from the domain theory and theactual derivatives of the target network e. Thus, the leftmost term contributesthe inductive constraint that the hypothesis must fit the observed training data, 
whereas the rightmost term contributes the analytical constraint that it must fitthe training derivatives extracted from the domain theory. Notice the derivativein Equation (12.2) is just a special case of the expression af(sfz") of Equa- 
tion (12.1), for which sj(a, xi) is the transformation that replaces x! byx/ + a. 
The precise weight-training rule used by EBNN is described by Thrun (1996). 
The relative importance of the inductive and analytical learning componentsis determined in EBNN by the constant pi, defined in Equation (12.3). The valueof piis determined by the discrepancy between the domain theory predictionA(xi) and the training value f (xi). The analytical component of learning is thusweighted more heavily for training examples that are correctly predicted by thedomain theory and is suppressed for examples that are not correctly predicted. 
This weighting heuristic assumes that the training derivatives extracted from thedomain theory are more likely tobe correct in cases where the training value iscorrectly predicted by the domain theory. Although one can construct situationsin which this heuristic fails, in practice it has been found effective in severaldomains (e.g., see Mitchell and Thrun [1993a]; Thrun [1996]). 
12.4.5 RemarksTo summarize, the EBNN algorithm uses a domain theory expressed asa set ofpreviously learned neural networks, together with a set of training examples, totrain its output hypothesis (the target network). For each training example EBNNuses its domain theory to explain the example, then extracts training derivativesfrom this explanation. For each attribute of the instance, a training derivative iscomputed that describes how the target function value is influenced bya smallchange to this attribute value, according to the domain theory. These trainingderivatives are provided toa variant of TANGENTPROP, which fits the target networkto these derivatives and to the training example values. Fitting the derivativesconstrains the learned network to fit dependencies given by the domain theory, 
while fitting the training values constrains itto fit the observed data itself. Theweight pi placed on fitting the derivatives is determined independently for eachtraining example, based on how accurately the domain theory predicts the trainingvalue for this example. 
EBNN has been shown tobe an effective method for learning from ap- 
proximate domain theories in several domains. Thrun (1996) describes its ap- 
plication toa variant of the Cup learning task discussed above and reports thatEBNN generalizes more accurately than standard BACKPROPAGATION, especiallywhen training data is scarce. For example, after 30 training examples, EBNNachieved a root-mean-squared error of 5.5 ona separate set of test data, comparedto an error of 12.0 for BACKPROPAGATION. Mitchell and Thrun (1993a) describeapplying EBNN to learning to control a simulated mobile robot, in which the do- 
main theory consists of neural networks that predict the effects of various robotactions on the world state. Again, EBNN using an approximate, previously learneddomain theory, outperformed BACKPROPAGATION. Here BACKPROPAGATION requiredapproximately 90 training episodes to reach the level of performance achievedby EBNN after 25 training episodes. O'Sullivan etal. (1997) and Thrun (1996) 
describe several other applications of EBNN to real-world robot perception andcontrol tasks, in which the domain theory consists of networks that predict theeffect of actions for an indoor mobile robot using sonar, vision, and laser rangesensors. 
EBNN bears an interesting relation to other explanation-based learning meth- 
ods, such as PROLOG-EBG described in Chapter 11. Recall from that chapter thatPROLOG-EBG also constructs explanations (predictions of example target values) 
based ona domain theory. In PROLOG-EBG the explanation is constructed from adomain theory consisting of Horn clauses, and the target hypothesis is refined bycalculating the weakest conditions under which this explanation holds. Relevantdependencies in the explanation are thus captured in the learned Horn clause hy- 
pothesis. EBNN constructs an analogous explanation, but itis based ona domaintheory consisting of neural networks rather than Horn clauses. Asin PROLOG-EBG, 
relevant dependencies are then extracted from the explanation and used to refinethe target hypothesis. In the case of EBNN, these dependencies take the formof derivatives because derivatives are the natural way to represent dependenciesin continuous functions such as neural networks. In contrast, the natural way torepresent dependencies in symbolic explanations or logical proofs isto describethe set of examples to which the proof applies. 
There are several differences in capabilities between EBNN and the sym- 
bolic explanation-based methods of Chapter 11. The main difference is that EBNNaccommodates imperfect domain theories, whereas PROLOG-EBG does not. Thisdifference follows from the fact that EBNN is built on the inductive mechanismof fitting the observed training values and uses the domain theory only asan addi- 
tional constraint on the learned hypothesis. A second important difference followsfrom the fact that PROLOG-EBG learns a growing set of Horn clauses, whereasEBNN learns a fixed-size neural network. As discussed in Chapter 11, one diffi- 
culty in learning sets of Horn clauses is that the cost of classifying a new instancegrows as learning proceeds and new Horn clauses are added. This problem isavoided in EBNN because the fixed-size target network requires constant time toclassify new instances. However, the fixed-size neural network suffers the cor- 
responding disadvantage that it may be unable to represent sufficiently complexfunctions, whereas a growing set of Horn clauses can represent increasingly com- 
plex functions. Mitchell and Thrun (1993b) provide a more detailed discussion ofthe relationship between EBNN and symbolic explanation-based learning methods. 
CHAPTER 12 COMBINING INDUCTIVE AND ANALYTICAL LEARNING 35712.5 USING PRIOR KNOWLEDGE TO AUGMENT SEARCHOPERATORSThe two previous sections examined two different roles for prior knowledge inlearning: initializing the learner's hypothesis and altering the objective functionthat guides search through the hypothesis space. In this section we consider athird way of using prior knowledge to alter the hypothesis space search: usingit to alter the set of operators that define legal steps in the search through thehypothesis space. This approach is followed by systems such as FOCL (Pazzaniet al. 1991; Pazzani and Kibler 1992) and ML-SMART (Bergadano and Giordana1990). Here we use FOCL to illustrate the approach. 
12.5.1 The FOCL AlgorithmFOCL isan extension of the purely inductive FOIL system described in Chap- 
ter 10. Both FOIL and FOCL learn a set of first-order Horn clauses to cover theobserved training examples. Both systems employ a sequential covering algorithmthat learns a single Horn clause, removes the positive examples covered by thisnew Horn clause, and then iterates this procedure over the remaining trainingexamples. In both systems, each new Horn clause is created by performing ageneral-to-specific search, beginning with the most general possible Horn clause 
(i.e., a clause containing no preconditions). Several candidate specializations ofthe current clause are then generated, and the specialization with greatest infor- 
mation gain relative to the training examples is chosen. This process is iterated, 
generating further candidate specializations and selecting the best, until a Hornclause with satisfactory performance is obtained. 
The difference between FOIL and FOCL lies in the way in which candidatespecializations are generated during the general-to-specific search for a single Hornclause. As described in Chapter 10, FOIL generates each candidate specializationby adding a single new literal to the clause preconditions. FOCL uses this samemethod for producing candidate specializations, but also generates additional spe- 
cializations based on the domain theory. The solid edges in the search tree of Fig- 
ure 12.8 show the general-to-specific search steps considered ina typical search byFOIL. The dashed edge in the search tree of Figure 12.8 denotes an additional can- 
didate specialization that is considered by FOCL and based on the domain theory. 
Although FOCL and FOIL both learn first-order Horn clauses, we illustratetheir operation here using the simpler domain of propositional (variable-free) Hornclauses. In particular, consider again the Cup target concept, training examples, 
and domain theory from Figure 12.3. To describe the operation of FOCL, we mustfirst draw a distinction between two kinds of literals that appear in the domaintheory and hypothesis representation. We will say a literal is operational ifit isallowed tobe used in describing an output hypothesis. For example, in the Cupexample of Figure 12.3 we allow output hypotheses to refer only to the 12 at- 
tributes that describe the training examples (e.g., HasHandle, HandleOnTop). 
Literals based on these 12 attributes are thus considered operational. In contrast, 
literals that occur only as intermediate features in the domain theory, but not asCup C 
[2+,3-II( i \ 
Cup C Fragile ... Cup C BottamlsFlal, 
I2++l Light, 
HmConcnvity, 
ConcavifyPointsUp 
[4+.2-ICup CHasConcavity, 
ConcavityPointsUpHandleOnTop 
[0+,2-ICup C BonomlsFlat, 
Light, 
HasConcaviry, 
ConcavifyPoinfsUp, 
1 ~andleon~o~ 
W+&ICup C BottomlsFlof, 
Light, 
HasConcavity, 
FIGURE 12.8Hypothesis space search in FOCL. To learn a single rule, FOCL searches from general to increasinglyspecific hypotheses. Two kinds of operators generate specializations of the current hypothesis. Onekind adds a single new literal (solid lines.in the figure). A second kind of operator specializes therule by adding a set of literals that constitute logically sufficient conditions for the target concept, 
according to the domain theory (dashed lines in the figure). FOCL selects among all these candidatespecializations, based on their performance over the data. Therefore, imperfect domain theories willimpact the hypothesis only if the evidence supports the theory. This example is based on the sametraining data and domain theory as the earlier KBANN example. 
primitive attributes of the instances, are considered nonoperational. An exampleof a nonoperational attribute in this case is the attribute Stable. 
At each point in its general-to-specific search, FOCL expands its currenthypothesis h using the following two operators: , 
1. For each operational literal that is not part ofh, create a specialization of hby adding this single literal to the preconditio s. This is also the method usedby FOIL to generate candidate successors. he solid arrows in Figure 12.8denote this type of specialization. 
PCHAPTER 12 COMBINING INDUCTIVE AND ANALYTICAL LEARNING 3592. Create an operational, logically sufficient condition for the target conceptaccording to the domain theory. Add this set of literals to the current precon- 
ditions ofh. Finally, prune the preconditions ofh by removing any literalsthat are unnecessary according to the training data. The dashed arrow inFigure 12.8 denotes this type of specialization. 
The detailed procedure for the second operator above isas follows. FOCLfirst selects one of the domain theory clauses whose head (postcondition) matchesthe target concept. If there are several such clauses, it selects the clause whosebody (preconditions) have the highest information gain relative to the trainingexamples of the target concept. For example, in the domain theory and trainingdata of Figure 12.3, there is only one such clause: 
Cup t Stable, Lifable, OpenvesselThe preconditions of the selected clause form a logically sufficient condition forthe target concept. Each nonoperational literal in these sufficient conditions isnow replaced, again using the domain theory and substituting clause precondi- 
tions for clause postconditions. For example, the domain theory clause Stable tBottomIsFlat is used to substitute the operational BottomIsFlat for the unopera- 
tional Stable. This process of "unfolding" the domain theory continues until thesufficient conditions have been restated in terms of operational literals. If thereare several alternative domain theory clauses that produce different results, thenthe one with the greatest information gain is greedily selected at each step ofthe unfolding process. The reader can verify that the final operational sufficientcondition given the data and domain theory in the current example isBottomIsFlat , HasHandle, Light, HasConcavity , ConcavityPointsUpAs a final step in generating the candidate specialization, this sufficient condition ispruned. For each literal in the expression, the literal is removed unless its removalreduces classification accuracy over the training examples. This step is includedto recover from overspecialization in case the imperfect domain theory includesirrelevant literals. In our current example, the above set of literals matches twopositive and two negative examples. Pruning (removing) the literal HasHandle re- 
sults in improved performance. The final pruned, operational, sufficient conditionsare, therefore, 
BottomZsFlat , Light, HasConcavity , ConcavityPointsUpThis set of literals is now added to the preconditions of the current hypothesis. 
Note this hypothesis is the result of the search step shown by the dashed arrowin Figure 12.8. 
Once candidate specializations of the current hypothesis have been gener- 
ated, using both of the two operations above, the candidate with highest informa- 
tion gain is selected. In the example shown in Figure 12.8 the candidate chosenat the first level in the search tree is the one generated by the domain theory. Thesearch then proceeds by considering further specializations of the theory-suggestedpreconditions, thereby allowing the inductive component of learning to refine thepreconditions derived from the domain theory. In this example, the domain theoryaffects the search only at the first search level. However, this will not always bethe case. Should the empirical support be stronger for some other candidate at thefirst level, theory-suggested literals may still be added at subsequent steps in thesearch. To summarize, FOCL learns Horn clauses of the formwhere cis the target concept, oiis an initial conjunction of operational literalsadded one ata time by the first syntactic operator, obis a conjunction of oper- 
ational literals added ina single step based on the domain theory, and ofis afinal conjunction of operational literals added one ata time by the first syntacticoperator. Any of these three sets of literals may be empty. 
The above discussion illustrates the use ofa propositional domain theoryto create candidate specializations of the hypothesis during the general-to-specificsearch for a single Horn clause. The algorithm is easily extended to first-orderrepresentations (i.e., representations including variables). Chapter 10 discusses indetail the algorithm used by FOIL to generate first-order Horn clauses, includingthe extension of the first of the two search operators described above to first-orderrepresentations. To extend the second operator to accommodate first-order domaintheories, variable substitutions must be considered when unfolding the domaintheory. This can be accomplished using a procedure related to the regressionprocedure described in Table 1 1.3. 
12.5.2 RemarksFOCL uses the domain theory to increase the number of candidate specializationsconsidered at each step of the search for a single Horn clause. Figure 12.9 com- 
pares the hypothesis space search performed by FOCL to that performed by thepurely inductive FOIL algorithm on which itis based. FOCL's theory-suggestedspecializations correspond to "macro" steps in FOIL'S search, in which severalliterals are added ina single step. This process can be viewed as promoting ahypothesis that might be considered later in the search to one that will be con- 
sidered immediately. If the domain theory is correct, the training data will bearout the superiority of this candidate over the others and it will be selected. If thedomain theory is incorrect, the empirical evaluation of all the candidates shoulddirect the search down an alternative path. 
To summarize, FOCL uses both a syntactic generation of candidate special- 
izations and a domain theory driven generation of candidate specializations at eachstep in the search. The algorithm chooses among these candidates based solely ontheir empirical support over the training data. Thus, the domain theory is used ina fashion that biases the learner, but leaves final search choices tobe made basedon performance over the training data. The bias introduced by the domain theoryis a preference in favor of Horn clauses most similar to operational, logicallysufficient conditions entailed by the domain theory. This bias is combined withHypothesis Space1 
Hypotheses thatfittraining dataequally wellFIGURE 12.9Hypothesis space search in FOCL. FOCL augments the set of search operators used by FOIL. WhereasFOIL considers adding a single new literal at each step, FOIL also considers adding multiple literalsderived from the domain theory. 
the bias of the purely inductive FOIL program, which isa preference for shorterhypotheses. 
FOCL has been shown to generalize more accurately than the purely induc- 
tive FOIL algorithm ina number of application domains in which an imperfect do- 
main theory is available. For example, Pazzani and Kibler (1992) explore learningthe concept "legal chessboard positions." Given 60 training examples describing30 legal and 30 illegal endgame board positions, FOIL achieved an accuracy of86% over an independent set of test examples. FOCL was given the same 60 train- 
ing examples, along with an approximate domain theory with an accuracy of 76%. 
FOCL produced a hypothesis with generalization accuracy of 94%-less than halfthe error rate of FOIL. Similar results have been obtained in other domains. Forexample, given 500 training examples of telephone network problems and theirdiagnoses from the telephone company NYNEX, FOIL achieved an accuracy of90%, whereas FOCL reached an accuracy of 98% when given the same trainingdata along with a 95% accurate domain theory. 
12.6 STATE OF THE ARTThe methods presented in this chapter are only a sample of the possible approachesto combining analytical and inductive learning. While each of these methods hasbeen demonstrated to outperform purely inductive learning methods in selecteddomains, none of these has been thoroughly tested or proven across a large varietyof problem domains. The topic of combining inductive and analytical learningremains a very active research area. 
12.7 SUMMARY AND FURTHER READINGThe main points of this chapter include: 
0 Approximate prior knowledge, or domain theories, are available in manypractical learning problems. Purely inductive methods such as decision treeinduction and neural network BACKPROPAGATION fail to utilize such domaintheories, and therefore perform poorly when data is scarce. Purely analyti- 
cal learning methods such as PROLOG-EBG utilize such domain theories, butproduce incorrect hypotheses when given imperfect prior knowledge. Meth- 
ods that blend inductive and analytical learning can gain the benefits of bothapproaches: reduced sample complexity and the ability to overrule incorrectprior knowledge. 
One way to view algorithms for combining inductive and analytical learningis to consider how the domain theory affects the hypothesis space search. 
In this chapter we examined methods that use imperfect domain theories to 
(1) create the initial hypothesis in the search, (2) expand the set of searchoperators that generate revisions to the current hypothesis, and (3) alter theobjective of the search. 
A system that uses the domain theory to initialize the hypothesis is KBANN. 
This algorithm uses a domain theory encoded as propositional rules to ana- 
lytically construct an artificial neural network that is equivalent to the domaintheory. This network is then inductively refined using the BACKPROPAGATIONalgorithm, to improve its performance over the training data. The result isa network biased by the original domain theory, whose weights are refinedinductively based on the training data. 
TANGENTPROP uses prior knowledge represented by desired derivatives ofthe target function. In some domains, such as image processing, this isa natural way to express prior knowledge. TANGENTPROP incorporates thisknowledge by altering the objective function minimized by gradient descentsearch through the space of possible hypotheses. 
EBNN uses the domain theory to alter the objective in searching the hy- 
pothesis space of possible weights for an artificial neural network. It usesa domain theory consisting of previously learned neural networks to per- 
form a neural network analog to symbolic explanation-based learning. As insymbolic explanation-based learning, the domain theory is used to explainindividual examples, yielding information about the relevance of differentexample features. With this neural network representation, however, infor- 
mation about relevance is expressed in the form of derivatives of the targetfunction value with respect to instance features. The network hypothesis istrained using a variant of the TANGENTPROP algorithm, in which the error tobe minimized includes both the error in network output values and the errorin network derivatives obtained from explanations. 
FOCL uses the domain theory to expand the set of candidates considered ateach step in the search. It uses an approximate domain theory representedby first order Horn clauses to learn a set of Horn clauses that approximatethe target function. FOCL employs a sequential covering algorithm, learningeach Horn clause bya general-to-specific search. The domain theory is usedto augment the set of next more specific candidate hypotheses consideredat each step of this search. Candidate hypotheses are then evaluated basedon their performance over the training data. In this way, FOCL combinesthe greedy, general-to-specific inductive search strategy of FOIL with therule-chaining, analytical reasoning of analytical methods. 
0 The question of how to best blend prior knowledge with new observationsremains one of the key open questions in machine learning. 
There are many more examples of algorithms that attempt to combine induc- 
tive and analytical learning. For example, methods for learning Bayesian beliefnetworks discussed in Chapter 6 provide one alternative to the approaches dis- 
cussed here. The references at the end of this chapter provide additional examplesand sources for further reading. 
EXERCISES12.1. Consider learning the target concept GoodCreditRisk defined over instances de- 
scribed by the four attributes HasStudentLoan, HasSavingsAccount, Isstudent, 
OwnsCar. Give the initial network created by KBANN for the following domaintheory, including all network connections and weights. 
GoodCreditRisk t Employed, LowDebtEmployed t -1sStudentLowDebt t -HasStudentLoan, HasSavingsAccount12.2. KBANN converts a set of propositional Horn clauses into an initial neural network. 
Consider the class ofn-of-m clauses, which are Horn clauses containing m literalsin the preconditions (antecedents), and an associated parameter n where nm. 
The preconditions ofan n-of-m Horn clause are considered tobe satisfied ifat leastn of its m preconditions are satisfied. For example, the clauseStudent t LiveslnDorm, Young, Studies; n = 2asserts that one isa Student ifat least two of these three preconditions are satisfied. 
Give an algorithm similar to that used by KBANN, that accepts a set ofpropositional n-of-m clauses and constructs a neural network consistent with thedomain theory. 
12.3. Consider extending KBANN to accept a domain theory consisting of first-orderrather than propositional Horn clauses (i.e., Horn clauses containing variables, as inChapter 10). Either give an algorithm for constructing a neural network equivalentto a set of Horn clauses, or discuss the difficulties that prevent this. 
12.4. This exercise asks you to derive a gradient descent rule analogous to that used byTANGENTPROP. Consider the instance space X consisting of the real numbers, andconsider the hypothesis space H consisting of quadratic functions ofx. That is, 
each hypothesis h(x) isof the form 
(a) Derive a gradient descent rule that minimizes the same criterion as BACKPROP- 
AGATION; that is, the sum of squared errors between the hypothesis and targetvalues of the training data. 
(b) Derive a second gradient descent rule that minimizes the same criterion asTANGENTPROP. Consider only the single transformation s(a, x) = x + a. 
12.5. EBNN extracts training derivatives from explanations by examining the weightsand activations of the neural networks that make up the explanation. Consider thesimple example in which the explanation is formed bya single sigmoid unit withn inputs. Derive a procedure for extracting the derivative 91,=,~ where xiis aparticular training instance input to the unit, f(x) is the sigmoid unit output, andxi denotes the jth input to the sigmoid unit. You may wish to use the notation x! 
to refer to the jth component ofxi. Hint: The derivation is similar to the derivationof the BACKPROPAGATION training rule. 
12.6. Consider again the search trace of FOCL shown in Figure 12.8. Suppose that thehypothesis selected at the first level in the search is changed toCup t -.HasHandleDescribe the second-level candidate hypotheses that will be generated by FOCL assuccessors to this hypothesis. You need only include those hypotheses generatedby FOCL's second search operator, which uses its domain theory. Don't forget topost-prune the sufficient conditions. Use the training data from Table 12.3. 
12.7. This chapter discussed three approaches to using prior knowledge to impact thesearch through the space of possible hypotheses. Discuss your ideas for how thesethree approaches could be integrated. Can you propose a specific algorithm thatintegrates at least two of these three for some specific hypothesis representation? 
What advantages and disadvantages would you anticipate from this integration? 
12.8. Consider again the question from Section 12.2.1, regarding what criterion to usefor choosing among hypotheses when both data and prior knowledge are available. 
Give your own viewpoint on this issue. 
REFERENCESAbu-Mostafa, Y. S. (1989). Learning from hints in neural networks. Journal of Complexity, 6(2), 
192-198. 
Bergadano, F., & Giordana, A. (1990). Guiding induction with domain theories. InR. Michalski etal. (Eds.), Machine learning: An art$cial intelligence approach 3 (pp. 474-492). San Mateo, 
CA: Morgan Kaufmann. 
Bradshaw, G., Fozzard, R., & Cice, L. (1989). A connectionist expert system that really works. InAdvances in neural information processing. San Mateo, CA: Morgan Kaufmam. 
Caruana, R. (1996). Algorithms and applications for multitask learning. Proceedings of the 13thInternational Conference on Machine Learning. San Francisco: Morgan Kaufmann. 
Cooper, G. C., & Herskovits, E. (1992). A Bayesian method for the induction of probabilistic networksfrom data. Machine Learning, 9, 309-347. 
Craven, M. W. (1996). Extracting comprehensible modelsfrom trained neural networks (PhD thesis) 
(UW Technical Report CS-TR-96-1326). Department of Computer Sciences, University ofWisconsin-Madison. 
Craven, M. W., & Shavlik, J. W. (1994). Using sampling and queries to extract rules from trainedneural networks. Proceedings of the 11th International Conference on Machine Learning (pp. 
3745). San Mateo, CA: Morgan Kaufmann. 
Fu, L. M. (1989). Integration of neural heuristics into knowledge-based inference. Connection Science, 
1(3), 325-339. 
Fu, L. M. (1993). Knowledge-based connectionism for revising domain theories. IEEE Transactionson Systems, Man, and Cybernetics, 23(1), 173-182. 
Gallant, S. I. (1988). Connectionist expert systems. CACM, 31(2), 152-169. 
Koppel, M., Feldman, R., & Segre, A. (1994). Bias-driven revision of logical domain theories. Journalof Artificial Intelligence, 1, 159-208. http:llwww.cs.washington.edulresearch/jairhome.html. 
Lacher, R., Hmska, S., & Kuncicky, D. (1991). Backpropagation learning in expert networks (Dept. 
of Computer Science Technical Report TR91-015). Florida State University, Tallahassee. 
Mach, R., & Shavlik, J. (1993). Using knowledge-based neural networks to improve algorithms: 
Refining the Chou-Fasman algorithm for protein folding. Machine Learning, 11(3), 195-215. 
Mitchell, T. M., & Thrun, S. B. (1993a). Explanation-based neural network learning for robot control. 
InS. Hanson, J. Cowan, & C. Giles (Eds.), Advances in neural infomtionprocessing systems5 (pp. 287-294). San Mateo, CA: Morgan-Kaufmann Press. 
Mitchell, T. M., & Thrun, S. B. (1993b). Explanation-based learning: A comparison of symbolic andneural network approaches. Tenth International Conference on Machine Learning, Amherst, 
MA. 
Mooney, R. (1993). Induction over the unexplained: Using overly-general domain theories to aidconcept learning. Machine Learning, lO(1). 
O'Sullivan, J., Mitchell, T., & Thrun, S. (1997). Explanation-based learning for mobile robot per- 
ception. InK. Ikeuchi & M. Veloso (Eds.), Symbolic Visual Learning (pp. 295-324). 
Ourston, D., & Mooney, R. J. (1994). Theory refinement combining analytical and empirical methods. 
Arti2cial Intelligence, 66(2). 
Pazzani, M. J., & Brunk, C. (1993). Finding accurate frontiers: A knowledge-intensive approach torelational learning. Proceedings of the I993 National Conference on Artificial Intelligence (pp. 
328-334). AAAI Press. 
Pazzani, M. J., Brunk, C. A., & Silverstein, G. (1991). A knowledge-intensive approach to learningrelational concepts. Proceedings of the Eighth International Workshop on Machine Learning 
(pp. 432436). San Mateo, CA: Morgan Kaufmann. 
Pazzani, M. J., & Kibler, D. (1992). The utility of knowledge in inductive learning. MachineLearning, 
9(1), 57-94. 
Pratt, L. Y. (1993a). Transferring previously learned BACKPROPAGATION neural networks to newlearning tasks (Ph.D. thesis). Department of Computer Science, Rutgers University, NewJersey. (Also Rutgers Computer Science Technical Report ML-TR-37.) 
Pratt, L. Y. (1993b). Discriminability-based transfer among neural networks. InJ. E. Moody etal. (Eds.), Advances in Nerual Infomtion Processing Systems 5. San Mateo, CA: MorganKaufmann. 
Rosenbloom, P. S., & Aasman, J. (1990). Knowledge level and inductive uses of chunking (ebl). 
Proceedings of the Eighth National Conference on Artificial Intelligence (pp. 821-827). AAAIPress. 
Russell, S., Binder, J., Koller, D., & Kanazawa, K. (1995). Local learning in probabilistic networkswith hidden variables. Proceedings of the 14th International Joint Conference on ArtificialIntelligence, Montreal. Morgan Kaufmann. 
Shavlik, J., & Towell, G. (1989). An approach to combining explanation-based and neural learningalgorithms. Connection Science, 1(3), 233-255. 
Simard, P. S., Victoni, B., LeCun, Y., & Denker, J. (1992). Tangent prop-A formalism for specifyingselected invariances inan adaptive network. InJ. Moody etal. (Eds.), Advances in NeuralInforination Processing System 4. San Mateo, CA: Morgan Kaufmann. 
Sudharth, S. C., & Holden, A. D. C. (1991). Symbolic-neural systems and the use of hints fordeveloping complex systems. International Journal of Man-Machine Studies, 35(3), 291-3 11. 
Thrun, S. (1996). Explanation based neural network learning: A lifelong learning approach. Boston: 
Kluwer Academic Publishers. 
Thrun, S., & Mitchell, T. M. (1993). Integrating inductive neural network learning and explanation- 
based learning. Proceedings of the 1993 International Joint Conference on Artificial Intelli- 
gence. 
Thrun, S., & Mitchell, T. M. (1995). Learning one more thing. Proceedings of the 1995 InternationalJoint Conference on Artificial Intelligence, Montreal. 
Towell, G., & Shavlik, J. (1989). An approach to combining explanation-based and neural learningalgorithms. Connection Science, (I), 233-255. 
Towell, G., & Shavlik, J. (1994). Knowledge-based artificial neural networks. Artificial Intelligence, 
70(1-2), 119-165. 
Towell, G., Shavlik, J., & Noordewier, M. (1990). Refinement of approximate domain theories byknowledge-based neural networks. Proceedings of the Eighth National Conference on ArtijcialIntelligence (pp. 861-866). Cambridge, MA: AAAI, MIT Press. 
Yang, Q., & Bhargava, V. (1990). Building expert systems bya modified perceptron network withrule-transfer algorithms (pp. 77-82). International Joint Conference on Neural Networks, IEEE. 
CHAPTERREINFORCEMENTLEARNINGReinforcement learning addresses the question of how an autonomous agent thatsenses and acts in its environment can learn to choose optimal actions to achieve itsgoals. This very generic problem covers tasks such as learning to control a mobilerobot, learning to optimize operations in factories, and learning to play board games. 
Each time the agent performs an action in its environment, a trainer may provide areward or penalty to indicate the desirability of the resulting state. For example, whentraining an agent to play a game the trainer might provide a positive reward when thegame is won, negative reward when itis lost, and zero reward in all other states. Thetask of the agent isto learn from this indirect, delayed reward, to choose sequencesof actions that produce the greatest cumulative reward. This chapter focuses onan algorithm called Q learning that can acquire optimal control strategies fromdelayed rewards, even when the agent has no prior knowledge of the effects ofits actions on the environment. Reinforcement learning algorithms are related todynamic programming algorithms frequently used to solve optimization problems. 
13.1 INTRODUCTIONConsider building a learning robot. The robot, or agent, has a set of sensors toobserve the state of its environment, and a set of actions it can perform to alterthis state. For example, a mobile robot may have sensors such asa camera andsonars, and actions such as "move forward" and "turn." Its task isto learn a controlstrategy, or policy, for choosing actions that achieve its goals. For example, therobot may have a goal of docking onto its battery charger whenever its batterylevel is low. 
This chapter is concerned with how such agents can learn successful controlpolicies by experimenting in their environment. We assume that the goals of theagent can be defined bya reward function that assigns a numerical value-animmediate payoff-to each distinct action the agent may take from each distinctstate. For example, the goal of docking to the battery charger can be captured byassigning a positive reward (e.g., +loo) to state-action transitions that immediatelyresult ina connection to the charger and a reward of zero to every other state-actiontransition. This reward function may be built into the robot, or known only to anexternal teacher who provides the reward value for each action performed by therobot. The task of the robot isto perform sequences of actions, observe their conse- 
quences, and learn a control policy. The control policy we desire is one that, fromany initial state, chooses actions that maximize the reward accumulated over timeby the agent. This general setting for robot learning is summarized in Figure 13.1. 
Asis apparent from Figure 13.1, the problem of learning a control policy tomaximize cumulative reward is very general and covers many problems beyondrobot learning tasks. In general the problem is one of learning to control sequentialprocesses. This includes, for example, manufacturing optimization problems inwhich a sequence of manufacturing actions must be chosen, and the reward tobe maximized is the value of the goods produced minus the costs involved. Itincludes sequential scheduling problems such as choosing which taxis to sendfor passengers ina large city, where the reward tobe maximized isa functionof the wait time of the passengers and the total fuel costs of the taxi fleet. Ingeneral, we are interested in any type of agent that must learn to choose actionsthat alter the state of its environment and where a cumulative reward functionis used to define the quality of any given action sequence. Within this class ofproblems we will consider specific settings, including settings in which the actionshave deterministic or nondeterministic outcomes, and settings in which the agentAgentI Environment IGoal: Learn to choose actions that maximizer +yr +y2r + ... , where 0 gy<l01 2FIGURE 13.1An agent interacting with its environment. 
The agent exists inan environment describedby some set of possible states S. It canperform any ofa set of possible actionsA. Each time it performs an action a, insome state st the agent receives a real-valuedreward r, that indicates the immediate valueof this state-action transition. This producesa sequence of states si, actions ai, andimmediate rewards rias shown in the figure. 
The agent's task isto learn a control policy, 
n : S + A, that maximizes the expectedsum of these rewards, with future rewardsdiscounted exponentially by their delay. 
has or does not have prior knowledge about the effects of its actions on theenvironment. 
Note we have touched on the problem of learning to control sequentialprocesses earlier in this book. In Section 11.4 we discussed explanation-basedlearning of rules to control search during problem solving. There the problem isfor the agent to choose among alternative actions at each step in its search for somegoal state. The techniques discussed here differ from those of Section 11.4, in thathere we consider problems where the actions may have nondeterministic outcomesand where the learner lacks a domain theory that describes the outcomes of itsactions. In Chapter 1 we discussed the problem of learning to choose actions whileplaying the game of checkers. There we sketched the design ofa learning methodvery similar to those discussed in this chapter. In fact, one highly successfulapplication of the reinforcement learning algorithms of this chapter isto a similargame-playing problem. Tesauro (1995) describes the TD-GAMMON program, whichhas used reinforcement learning to become a world-class backgammon player. Thisprogram, after training on 1.5 million self-generated games, is now considerednearly equal to the best human players in the world and has played competitivelyagainst top-ranked players in international backgammon tournaments. 
The problem of learning a control policy to choose actions is similar in somerespects to the function approximation problems discussed in other chapters. Thetarget function tobe learned in this case isa control policy, n : S + A, thatoutputs an appropriate action a from the set A, given the current state s from theset S. However, this reinforcement learning problem differs from other functionapproximation tasks in several important respects. 
0 Delayed reward. The task of the agent isto learn a target function n thatmaps from the current state sto the optimal action a = n(s). In earlierchapters we have always assumed that when learning some target functionsuch asn, each training example would bea pair of the form (s, n(s)). Inreinforcement learning, however, training information is not available in thisform. Instead, the trainer provides only a sequence of immediate reward val- 
ues as the agent executes its sequence of actions. The agent, therefore, facesthe problem of temporal credit assignment: determining which of the actionsin its sequence are tobe credited with producing the eventual rewards. 
0 Exploration. In reinforcement learning, the agent influences the distributionof training examples by the action sequence it chooses. This raises the ques- 
tion of which experimentation strategy produces most effective learning. Thelearner faces a tradeoff in choosing whether to favor exploration of unknownstates and actions (to gather new information), or exploitation of states andactions that it has already learned will yield high reward (to maximize itscumulative reward). 
0 Partially observable states. Although itis convenient to assume that theagent's sensors can perceive the entire state of the environment at each timestep, in many practical situations sensors provide only partial information. 
For example, a robot with a forward-pointing camera cannot see what isbehind it. In such cases, it may be necessary for the agent to consider itsprevious observations together with its current sensor data when choosingactions, and the best policy may be one that chooses actions specifically toimprove the observability of the environment. 
Life-long learning. Unlike isolated function approximation tasks, robot learn- 
ing often requires that the robot learn several related tasks within the sameenvironment, using the same sensors. For example, a mobile robot may needto learn how to dock on its battery charger, how to navigate through nar- 
row corridors, and how to pick up output from laser printers. This settingraises the possibility of using previously obtained experience or knowledgeto reduce sample complexity when learning new tasks. 
13.2 THE LEARNING TASKIn this section we formulate the problem of learning sequential control strategiesmore precisely. Note there are many ways todo so. For example, we might assumethe agent's actions are deterministic or that they are nondeterministic. We mightassume that the agent can predict the next state that will result from each action, orthat it cannot. We might assume that the agent is trained byan expert who showsit examples of optimal action sequences, or that it must train itself by performingactions of its own choice. Here we define one quite general formulation of theproblem, based on Markov decision processes. This formulation of the problemfollows the problem illustrated in Figure 13.1. 
Ina Markov decision process (MDP) the agent can perceive a set Sof distinctstates of its environment and has a set Aof actions that it can perform. At eachdiscrete time step t, the agent senses the current state st, chooses a current actiona,, and performs it. The environment responds by giving the agent a reward r, = 
r (st, a,) and by producing the succeeding state s,+l = 6(s,, a,). Here the functions6 and r are part of the environment and are not necessarily known to the agent. 
Inan MDP, the functions 6(st, a,) and r(s,, a,) depend only on the current stateand action, and not on earlier states or actions. In this chapter we consider onlythe case in which S and A are finite. In general, 6 and r may be nondeterministicfunctions, but we begin by considering only the deterministic case. 
The task of the agent isto learn a policy, n : S + A, for selecting its nextaction a, based on the current observed state st; that is, n(s,) = a,. How shall wespecify precisely which policy nwe would like the agent to learn? One obviousapproach isto require the policy that produces the greatest possible cumulativereward for the robot over time. To state this requirement more precisely, we definethe cumulative value Vn(s,) achieved by following an arbitrary policy n from anarbitrary initial state stas follows: 
CHAPTER 13 REINFORCEMENT LEARNING 371where the sequence of rewards rt+iis generated by beginning at state s, and byrepeatedly using the policy nto select actions as described above (i.e., a, = n(st), 
a,+l = n(~,+~), etc.). Here 0 5 y < 1 isa constant that determines the relativevalue of delayed versus immediate rewards. In particular, rewards received i timesteps into the future are discounted exponentially bya factor ofy '. Note ifwe sety = 0, only the immediate reward is considered. Aswe set y closer to 1, futurerewards are given greater emphasis relative to the immediate reward. 
The quantity VX(s) defined by Equation (13.1) is often called the discountedcumulative reward achieved by policy n from initial state s. Itis reasonable todiscount future rewards relative to immediate rewards because, in many cases, 
we prefer to obtain the reward sooner rather than later. However, other defini- 
tions of total reward have also been explored. For example, jinite horizon reward, 
c:=, rt+i, considers the undiscounted sum of rewards over a finite number hof . - 
steps. Another possibility is average reward, limb,, cF=~ rt+i, which consid- 
ers the average reward per time step over the entire lifetime of the agent. Inthis chapter we restrict ourselves to considering discounted reward as definedby Equation (13.1). Mahadevan (1996) provides a discussion of reinforcementlearning when the criterion tobe optimized is average reward. 
We are now ina position to state precisely the agent's learning task. Werequire that the agent learn a policy n that maximizes V"(s) for all states s. 
We will call such a policy an optimal policy and denote itby T*. 
n* r argmax V" (s), (Vs) 
XTo simplify notation, we will refer to the value function v"*(s) of such an optimalpolicy asV*(s). V*(s) gives the maximum discounted cumulative reward that theagent can obtain starting from state s; that is, the discounted cumulative rewardobtained by following the optimal policy beginning at state s. 
To illustrate these concepts, a simple grid-world environment is depictedin the topmost diagram of Figure 13.2. The six grid squares in this diagramrepresent six possible states, or locations, for the agent. Each arrow in the diagramrepresents a possible action the agent can take to move from one state to another. 
The number associated with each arrow represents the immediate reward r(s, a) 
the agent receives ifit executes the corresponding state-action transition. Notethe immediate reward in this particular environment is defined tobe zero forall state-action transitions except for those leading into the state labeled G. It isconvenient to think of the state Gas the goal state, because the only way the agentcan receive reward, in this case, isby entering this state. Note in this particularenvironment, the only action available to the agent once it enters the state G isto remain in this state. For this reason, we call Gan absorbing state. 
Once the states, actions, and immediate rewards are defined, and once wechoose a value for the discount factor y, we can determine the optimal policy n* 
and its value function V*(s). In this case, let us choose y = 0.9. The diagramat the bottom of the figure shows one optimal policy for this setting (there areothers as well). Like any policy, this policy specifies exactly one action that ther (s, a) (immediate reward) valuesQ(s, a) values V*(s) valuesOne optimal policyFIGURE 13.2A simple deterministic world to illustrate the basic concepts ofQ-learning. Each grid square representsa distinct state, each arrow a distinct action. The immediate reward function, r(s, a) gives reward 100for actions entering the goal state G, and zero otherwise. Values ofV*(s) and Q(s, a) follow fromr(s, a), and the discount factor y = 0.9. An optimal policy, corresponding to actions with maximalQ values, is also shown. 
agent will select in any given state. Not surprisingly, the optimal policy directsthe agent along the shortest path toward the state G. 
The diagram at the right of Figure 13.2 shows the values ofV* for eachstate. For example, consider the bottom right state in this diagram. The value ofV* for this state is 100 because the optimal policy in this state selects the "moveup" action that receives immediate reward 100. Thereafter, the agent will remainin the absorbing state and receive no further rewards. Similarly, the value ofV* 
for the bottom center state is 90. This is because the optimal policy will movethe agent from this state to the right (generating an immediate reward of zero), 
then upward (generating an immediate reward of 100). Thus, the discounted futurereward from the bottom center state iso+y100+y20+Y30+...=90CHAPTER 13 REINFORCEMENT LEARNING 373Recall that V* is defined tobe the sum of discounted future rewards over theinfinite future. In this particular environment, once the agent reaches the absorbingstate G its infinite future will consist of remaining in this state and receivingrewards of zero. 
13.3 Q LEARNINGHow can an agent learn an optimal policy n* for an arbitrary environment? It isdifficult to learn the function rt* : S + A directly, because the available trainingdata does not provide training examples of the form (s, a). Instead, the onlytraining information available to the learner is the sequence of immediate rewardsr(si, ai) for i = 0, 1,2, . . . . Aswe shall see, given this kind of training informationit is easier to learn a numerical evaluation function defined over states and actions, 
then implement the optimal policy in terms of this evaluation function. 
What evaluation function should the agent attempt to learn? One obviouschoice isV*. The agent should prefer state sl over state s2 whenever V*(sl) > 
V*(s2), because the cumulative future reward will be greater from sl. Of coursethe agent's policy must choose among actions, not among states. However, it canuse V* in certain settings to choose among actions as well. The optimal actionin state sis the action a that maximizes the sum of the immediate reward r(s, a) 
plus the value V* of the immediate successor state, discounted byy. 
n*(s) = argmax[r(s, a) fy V*(G(s, a))] 
a 
(recall that 6(s, a) denotes the state resulting from applying action ato state s.) 
Thus, the agent can acquire the optimal policy by learning V*, provided it hasperfect knowledge of the immediate reward function r and the state transitionfunction 6. When the agent knows the functions r and 6 used by the environmentto respond to its actions, it can then use Equation (13.3) to calculate the optimalaction for any state s. 
Unfortunately, learning V* isa useful way to learn the optimal policy onlywhen the agent has perfect knowledge of 6 and r. This requires that itbe able toperfectly predict the immediate result (i.e., the immediate reward and immediatesuccessor) for every possible state-action transition. This assumption is compara- 
ble to the assumption ofa perfect domain theory in explanation-based learning, 
discussed in Chapter 11. In many practical problems, such as robot control, itis impossible for the agent or its human programmer to predict in advance theexact outcome of applying an arbitrary action toan arbitrary state. Imagine, forexample, the difficulty in describing 6 for a robot arm shoveling dirt when theresulting state includes the positions of the dirt particles. In cases where either6 orr is unknown, learning V* is unfortunately ofno use for selecting optimalactions because the agent cannot evaluate Equation (13.3). What evaluation func- 
tion should the agent use in this more general setting? The evaluation function Q, 
defined in the following section, provides one answer. 
374 MACHINE LEARNING13.3.1 The Q FunctionLet us define the evaluation function Q(s, a) so that its value is the maximum dis- 
counted cumulative reward that can be achieved starting from state s and applyingaction aas the first action. In other words, the value ofQ is the reward receivedimmediately upon executing action a from state s, plus the value (discounted byy) of following the optimal policy thereafter. 
Q(s, a) - r(s, a) + YV*(6(s, a)) (1 3.4) 
Note that Q(s, a) is exactly the quantity that is maximized in Equation (13.3) 
in order to choose the optimal action ain state s. Therefore, we can rewriteEquation (13.3) in terms ofQ(s, a) asn * (s) = argmax Q (s , a) (13.5) 
aWhy is this rewrite important? Because it shows that if the agent learns the Qfunction instead of the V* function, it will be able to select optimal actions evenwhen it has no knowledge of thefunctions r and 6. As Equation (13.5) makes clear, 
it need only consider each available action ain its current state s and choose theaction that maximizes Q(s, a). 
It may at first seem surprising that one can choose globally optimal actionsequences by reacting repeatedly to the local values ofQ for the current state. 
This means the agent can choose the optimal action without ever conducting alookahead search to explicitly consider what state results from the action. Part ofthe beauty ofQ learning is that the evaluation function is defined to have preciselythis property-the value ofQ for the current state and action summarizes in asingle number all the information needed to determine the discounted cumulativereward that will be gained in the future if action ais selected in state s. 
To illustrate, Figure 13.2 shows the Q values for every state and action in thesimple grid world. Notice that the Q value for each state-action transition equalsthe r value for this transition plus the V* value for the resulting state discounted byy. Note also that the optimal policy shown in the figure corresponds to selectingactions with maximal Q values. 
13.3.2 An Algorithm for Learning QLearning the Q function corresponds to learning the optimal policy. How can Qbe learned? 
The key problem is finding a reliable way to estimate training values forQ, given only a sequence of immediate rewards r spread out over time. This canbe accomplished through iterative approximation. To see how, notice the closerelationship between Q and V*, 
V*(S) = max Q(s, a') 
a' 
which allows rewriting Equation (13.4) asQ(s, a) = r(s, a) + y max Q(W, a), a') a' 
CHAPTER 13 REINFORCEMENT LEARNJNG 375This recursive definition ofQ provides the basis for algorithms that iter- 
atively approximate Q (Watkins 1989). To describe the algorithm, we will usethe symbol Qto refer to the learner's estimate, or hypothesis, of the actual Qfunction. In this algorithm the learner represents its hypothesis Qby a large tablewith a separate entry for each state-action pair. The table entry for the pair (s, a) 
stores the value for ~(s, a)-the learner's current hypothesis about the actualbut unknown value Q(s, a). The table can be initially filled with random values 
(though itis easier to understand the algorithm if one assumes initial values ofzero). The agent repeatedly observes its current state s, chooses some action a, 
executes this action, then observes the resulting reward r = r(s, a) and the newstate s' = 6(s, a). It then updates the table entry for ~(s, a) following each suchtransition, according to the rule: 
Q(S, a) tr + y max &(st, a') a' 
(13.7) 
Note this training rule uses the agent's current Q values for the new states' to refine its estimate of ~(s, a) for the previous state s. This training ruleis motivated by Equation (13.6), although the training rule concerns the agent'sapproximation Q, whereas Equation (13.6) applies to the actual Q function. Notealthough Equation (13.6) describes Qin terms of the functions 6(s, a) and r(s, a), 
the agent does not need to know these general functions to apply the trainingrule of Equation (13.7). Instead it executes the action in its environment andthen observes the resulting new state s' and reward r. Thus, it can be viewed assampling these functions at the current values ofs and a. 
The above Q learning algorithm for deterministic Markov decision processesis described more precisely in Table 13.1. Using this algorithm the agent's estimateQ converges in the limit to the actual Q function, provided the system can bemodeled asa deterministic Markov decision process, the reward function r isQ learning algorithmFor each s, a initialize the table entry ~(s, a) to zero. 
Observe the current state sDo forever: 
Select an action a and execute itReceive immediate reward rObserve the new state s' 
Update the table entry for ~(s, a) as follows: 
~(s,a) cr + ymax&(s',af) 
a' 
SCS' 
TABLE 13.1Q learning algorithm, assuming deterministic rewards and actions. The discount factor y may be anyconstant such that 0 5 y < 1. 
bounded, and actions are chosen so that every state-action pair is visited infinitelyoften. 
13.3.3 An Illustrative ExampleTo illustrate the operation of the Q learning algorithm, consider a single actiontaken byan agent, and the corresponding refinement toQ shown in Figure 13.3. 
In this example, the agent moves one cell to the right in its grid world and receivesan immediate reward of zero for this transition. It then applies the training ruleof Equation (13.7) to refine its estimate Q for the state-action transition it justexecuted. According to the training rule, the new Q estimate for this transitionis the sum of the received reward (zero) and the highest Q value associated withthe resulting state (loo), discounted byy (.9). 
Each time the agent moves forward from an old state toa new one, Qlearning propagates Q estimates backward from the new state to the old. At thesame time, the immediate reward received by the agent for the transition is usedto augment these propagated values ofQ. 
Consider applying this algorithm to the grid world and reward functionshown in Figure 13.2, for which the reward is zero everywhere, except whenentering the goal state. Since this world contains an absorbing goal state, we willassume that training consists ofa series of episodes. During each episode, theagent begins at some randomly chosen state and is allowed to execute actionsuntil it reaches the absorbing goal state. When it does, the episode ends andInitial state: S] Next state: S2FIGURE 13.3The update toQ after executing a single^ action. The diagram on the left shows the initial states! of the robot (R) and several relevant Q values in its initial hypothesis. For example, the valueQ(s1, aright) = 72.9, where aright refers to the action that moves Rto its right. When the robotexecutes the action aright, it receives immediate reward r = 0 and transitions to state s2. It thenupdates its estimate i)(sl, aright) based on its Q estimates for the new state s2. Here y = 0.9. 
CHAPTER 13 REINFORCEMENT LEARNING 377the agent is transported toa new, randomly chosen, initial state for the nextepisode. 
How will the values ofQ evolve as the Q learning algorithm is applied inthis case? With all the Q values initialized to zero, the agent will make no changesto any Q table entry until it happens to reach the goal state and receive a nonzeroreward. This will result in refining the Q value for the single transition leadinginto the goal state. On the next episode, if the agent passes through this stateadjacent to the goal state, its nonzero Q value will allow refining the value forsome transition two steps from the goal, and soon. Given a sufficient number oftraining episodes, the information will propagate from the transitions with nonzeroreward back through the entire state-action space available to the agent, resultingeventually ina Q table containing the Q values shown in Figure 13.2. 
In the next section we prove that under certain assumptions the Q learningalgorithm of Table 13.1 will converge to the correct Q function. First considertwo general properties of this Q learning algorithm that hold for any deterministicMDP in which the rewards are non-negative, assuming we initialize all Q values tozero. The first property is that under these conditions the Q values never decreaseduring training. More formally, let Q,(s, a) denote the learned ~(s, a) value afterthe nth iteration of the training procedure (i.e., after the nth state-action transitiontaken by the agent). ThenA second general property that holds under these same conditions is that through- 
out the training process every Q value wi:l remain in the interval between zeroand its true Q value. 
13.3.4 ConvergenceWill the algorithm of Table 13.1 converge toward aQ equal to the true Q function? 
The answer is yes, under certain conditions. First, we must assume the system isa deterministic MDP. Second, we must assume the immediate reward values arebounded; that is, there exists some positive constant c such that for all states sand actions a, Ir(s, a)l < c. Third, we assume the agent selects actions in sucha fashion that it visits every possible state-action pair infinitely often. By thisthird condition we mean that if action ais a legal action from state s, then overtime the agent must execute action a from state s repeatedly and with nonzerofrequency as the length of its action sequence approaches infinity. Note theseconditions are in some ways quite general and in others fairly restrictive. Theydescribe a more general setting than illustrated by the example in the previoussection, because they allow for environments with arbitrary positive or negativerewards, and for environments where any number of state-action transitions mayproduce nonzero rewards. The conditions are also restrictive in that they requirethe agent to visit every distinct state-action transition infinitely often. This is avery strong assumption in large (or continuous!) domains. We will discuss strongerconvergence results later. However, the result described in this section providesthe basic intuition for understanding why Q learning works. 
The key idea underlying the proof of convergence is that the table entry 
~(s, a) with the largest error must have its error reduced bya factor ofy wheneverit is updated. The reason is that its new value depends only in part on error-proneQ estimates, with the remainder depending on the error-free observed immediatereward r. 
Theorem 13.1. Convergence ofQ learning for deterministic Markov decisionprocesses. Consider aQ learning agent ina deterministic MDP with bounded re- 
wards (Vs, a) lr(s, a)[ 5 c. The* Q learning agent uses the training rule of Equa- 
tion (13.7), initializes its table Q(s, a) to arbitrary finite values, and uses a discountfactor y such that 0 y < 1. Let Q,(s, a) denote the agent's hypothesis ~(s, a) 
following the nth update. If each state-action pair is visited infinitely often, thenQ,(s, a) converges toQ(s, a) asn + oo, for all s, a. 
Proof. Since each state-action transition occurs infinitely often, consider consecutiveintervals during which each state-action transition occurs at least once. The proofconsists of showing that the maximum error over all entries in the Q table is reducedby at least a factor ofy during each such interval. Q, is the agent's table of estimatedQ values after n updates. Let Anbe the maximum error inQ,; that isBelow we use s' to denote S(s, a). Now for any table entry (in@, a) that is updatedon iteration n + 1, the magnitude of the error in the revised estimate Q,+~(S, a) isIQ,+I(S, a) - Q(s, all = I(r + y max Qn(s', a')) - (r + ym?xQ(d, a'))] 
a' a 
= yI myQn(st, a') - myQ(s1, a') Ia a5 y max IQn(s1, a') - ~(s', a') Ia' 
5 Ymy IQ, (s", a') - QW, a') Is ,aIQn+i (s, a) - Q(s, all 5 Y AnThe third line above follows from the second line because for any two functions fiand f2 the following inequality holdsIn going from the third line to the fourth line above, note we introduce a newvariable s" over which the maximization is performed. This is legitimate becausethe maximum value will beat least as great when we allow this additional variableto vary. Note that by introducing this variable we obtain an expression that matchesthe definition ofA,. 
Thus, the updated Q,+~(S, a) for any s, ais at most y times the maximumerror in the Q,, table, A,. The largest error in the initial table, Ao, is bounded becausevalues of ~~(s, a) and Q(s, a) are bounded for all s, a. Now after the first intervalCHAPTER 13 REINFORCEMENT LEARNING 379during which each s, ais visited, the largest error in the table will beat most yAo. 
After k such intervals, the error will beat most ykAo. Since each state is visitedinfinitely often, the number of such intervals is infinite, and A, -+ 0 asn + oo. 
This proves the theorem. 013.3.5 Experimentation StrategiesNotice the algorithm of Table 13.1 does not specify how actions are chosen by theagent. One obvious strategy would be for the agent in state sto select the action athat maximizes ~(s, a), thereby exploiting its current approximation Q. However, 
with this strategy the agent runs the risk that it will overcommit to actions thatare found during early training to have high Q values, while failing to exploreother actions that have even higher values. In fact, the convergence theorem aboverequires that each state-action transition occur infinitely often. This will clearlynot occur if the agent always selects actions that maximize its current &(s, a). Forthis reason, itis common inQ learning to use a probabilistic approach to selectingactions. Actions with higher Q values are assigned higher probabilities, but everyaction is assigned a nonzero probability. One way to assign such probabilities iswhere P(ai 1s) is the probability of selecting action ai, given that the agent is instate s, and where k > 0 isa constant that determines how strongly the selectionfavors actions with high Q values. Larger values ofk will assign higher proba- 
bilities to actions with above average Q, causing the agent to exploit what it haslearned and seek actions it believes will maximize its reward. In contrast, smallvalues ofk will allow higher probabilities for other actions, leading the agentto explore actions that do not currently have high Q values. In some cases, k isvaried with the number of iterations so that the agent favors exploration duringearly stages of learning, then gradually shifts toward a strategy of exploitation. 
13.3.6 Updating SequenceOne important implication of the above convergence theorem is that Q learningneed not train on optimal action sequences in order to converge to the optimalpolicy. In fact, it can learn the Q function (and hence the optimal policy) whiletraining from actions chosen completely at random at each step, as long as theresulting training sequence visits every state-action transition infinitely often. Thisfact suggests changing the sequence of training example transitions in order toimprove training efficiency without endangering final convergence. To illustrate, 
consider again learning inan MDP with a single absorbing goal state, such as theone in Figure 13.1. Assume as before that we train the agent with a sequence ofepisodes. For each episode, the agent is placed ina random initial state and isallowed to perform actions and to update its Q table until it reaches the absorbinggoal state. A new training episode is then begun by removing the agent from thegoal state and placing itat a new random initial state. As noted earlier, if webegin with all Q values initialized to zero, then after the first full episode onlyone entry in the agent'sQ table will have been changed: the entry correspondingto the final transition into the goal state. Note that if the agent happens to followthe same sequence of actions from the same random initial state in its second fullepisode, then a second table entry would be made nonzero, and soon. Ifwe runrepeated identical episodes in this fashion, the frontier of nonzero Q values willcreep backward from the goal state at the rate of one new state-action transitionper episode. Now consider training on these same state-action transitions, but inreverse chronological order for each episode. That is, we apply the same updaterule from Equation (13.7) for each transition considered, but perform these updatesin reverse order. In this case, after the first full episode the agent will have updatedits Q estimate for every transition along the path it took to the goal. This trainingprocess will clearly converge in fewer iterations, although it requires that the agentuse more memory to store the entire episode before beginning the training for thatepisode. 
A second strategy for improving the rate of convergence isto store paststate-action transitions, along with the immediate reward that was received, andretrain on them periodically. Although at first it might seem a waste of effort toretrain on the same transition, recall that the updated ~(s, a) value is determinedby the values ~(s', a) of the successor state s' = 6(s, a). Therefore, if subsequenttraining changes one of the ~(s', a) values, then retraining on the transition (s, a) 
may result inan altered value for ~(s, a). In general, the degree to which we wishto replay old transitions versus obtain new ones from the environment dependson the relative costs of these two operations in the specific problem domain. Forexample, ina robot domain with navigation actions that might take several secondsto perform, the delay in collecting a new state-action transition from the externalworld might be several orders of magnitude more costly than internally replayinga previously observed transition. This difference can be very significant given thatQ learning can often require thousands of training iterations to converge. 
Note throughout the above discussion we have kept our assumption that theagent does not know the state-transition function 6(s, a) used by the environmentto create the successor state s' = S(s, a), or the function r(s, a) used to generaterewards. Ifit does know these two functions, then many more efficient methodsare possible. For example, if performing external actions is expensive the agentmay simply ignore the environment and instead simulate it internally, efficientlygenerating simulated actions and assigning the appropriate simulated rewards. 
Sutton (1991) describes the DYNA architecture that performs a number of simulatedactions after each step executed in the external world. Moore and Atkeson (1993) 
describe an approach called prioritized sweeping that selects promising states toupdate next, focusing on predecessor states when the current state is found tohave a large update. Peng and Williams (1994) describe a similar approach. Alarge number of efficient algorithms from the field of dynamic programming canbe applied when the functions 6 and r are known. Kaelbling etal. (1996) surveya number of these. 
CHAPTER 13 REINFORCEMENT LEARNING 38113.4 NONDETERMINISTIC REWARDS AND ACTIONSAbove we considered Q learning in deterministic environments. Here we considerthe nondeterministic case, in which the reward function r(s, a) and action transi- 
tion function 6(s, a) may have probabilistic outcomes. For example, in Tesaur~'~ 
(1995) backgammon playing program, action outcomes are inherently probabilis- 
tic because each move involves a roll of the dice. Similarly, in robot problemswith noisy sensors and effectors itis often appropriate to model actions and re- 
wards as nondeterministic. In such cases, the functions 6(s, a) and r(s, a) can beviewed as first producing a probability distribution over outcomes based ons anda, and then drawing an outcome at random according to this distribution. Whenthese probability distributions depend solely ons and a (e.g., they do not dependon previous states or actions), then we call the system a nondeterministic Markovdecision process. 
In this section we extend the Q learning algorithm for the deterministiccase to handle nondeterministic MDPs. To accomplish this, we retrace the lineof argument that led to the algorithm for the deterministic case, revising it whereneeded. 
In the nondeterministic case we must first restate the objective of the learnerto take into account the fact that outcomes of actions are no longer deterministic. 
The obvious generalization isto redefine the value V" ofa policy nto be the ex- 
pected value (over these nondeterministic outcomes) of the discounted cumulativereward received by applying this policywhere, as before, the sequence of rewards r,+iis generated by following policyn beginning at state s. Note this isa generalization of Equation (13.1), whichcovered the deterministic case. 
As before, we define the optimal policy n* tobe the policy n that maxi- 
mizes V"(s) for all states s. Next we generalize our earlier definition ofQ fromEquation (13.4), again by taking its expected value. 
where P(slls, a) is the probability that taking action ain state s will produce thenext state s'. Note we have used P(slls, a) here to rewrite the expected value ofV*(6(s, a)) in terms of the probabilities associated with the possible outcomes ofthe probabilistic 6. 
As before we can re-express Q recursivelyQ(s, a) = E[r(s, a)] + yP(sfls, a) myQ(sl, a') (13.9) 
S' 
awhich is the generalization of the earlier Equation (13.6). To summarize, we havesimply redefined Q(s, a) in the nondeterministic case tobe the expected value ofits previously defined quantity for the deterministic case. 
Now that we have generalized the definition ofQ to accommodate the non- 
deterministic environment functions r and 6, a new training rule is needed. Ourearlier training rule derived for the deterministic case (Equation 13.7) fails to con- 
verge in this nondeterministic setting. Consider, for example, a nondeterministicreward function r(s, a) that produces different rewards each time the transition 
(s, a} is repeated. In this case, the training rule will repeatedly alter the values ofQ(S, a), even ifwe initialize the Q table values to the correct Q function. In brief, 
this training rule does not converge. This difficulty can be overcome by modifyingthe training rule so that it takes a decaying weighted average of the current Qvalue and the revised estimate. Writing Q, to denote the agent's estimate on thenth iteration of the algorithm, the following revised training rule is sufficient toassure convergence ofQ toQ: 
Q~(s, a) -+ (1 - un)Qn-l(s, a) + a,[r + y max Q,-~(S', a')] at (13.10) 
wherea, = 
11 + visits, (s, a) 
where s and a here are the state and action updated during the nth iteration, andwhere visits,(s, a) is the total number of times this state-action pair has beenvisited upto and including the nth iteration. 
The key idea in this revised rule is that revisions toQ are made moregradually than in the deterministic case. Notice ifwe were to set a, to 1 inEquation (13.10) we would have exactly the training rule for the deterministic case. 
With smaller values ofa, this term is now averaged in with the current ~(s, a) toproduce the new updated value. Notice that the value ofa, in Equation (13.11) 
decreases asn increases, so that updates become smaller as training progresses. 
By reducing aat an appropriate rate during training, we can achieve convergenceto the correct Q function. The choice ofa, given above is one of many thatsatisfy the conditions for convergence, according to the following theorem due toWatkins and Dayan (1992). 
Theorem 13.2. Convergence ofQ learning for nondeterministic Markov de- 
cision processes. Consider aQ learning agent ina nondeterministic MDP withbounded rewards (Vs, a)lr(s, a)l 5 c. The Q learning agent uses the training rule ofEquation (13.10), initializes its table ~(s, a) to arbitrary finite values, and uses adiscount factor y such that 0 5 y < 1. Let n(i, s, a) be the iteration correspondingto the ith time that action ais applied to state s. If each state-action pair is visitedinfinitely often, 0 5 a,, < 1, andthen for all s and a, &,(s, a) + Q(s, a) asn + 00, with probability 1. 
While Q learning and related reinforcement learning algorithms can beproven to converge under certain conditions, in practice systems that use Q learn- 
ing often require many thousands of training iterations to converge. For exam- 
ple, Tesauro'sTD-GAMMON discussed earlier trained for 1.5 million backgammongames, each of which contained tens of state-action transitions. 
13.5 TEMPORAL DIFFERENCE LEARNINGThe Q learning algorithm learns by iteratively reducing the discrepancy betweenQ value estimates for adjacent state:,. In this sense, Q learning isa special caseof a general class of temporal diflerence algorithms that learn by reducing dis- 
crepancies between estimates made by the agent at different times. Whereas thetraining rule of Equation (13.10) reduces the difference between the estimated Qvalues ofa state and its immediate successor, we could just as well design an algo- 
rithm that reduces discrepancies between this state and more distant descendantsor ancestors. 
To explore this issue further, recall that our Q learning training rule calcu- 
lates a training value for &(st, a,) in terms of the values for &(s,+l, at+l) wheres,+lis the result of applying action a, to the state st. Let Q(')(s,, a,) denote thetraining value calculated by this one-step lookaheadOne alternative way to compute a training value for Q(s,, a,) isto base iton theobserved rewards for two steps2 
 st, a,) = rt + yr,+l + y max Q(s~+~, a) 
or, in general, for n stepsQ(~)(s,,~,) = rt + yr,+l + ,-. + y(n-l)rt+n-l + ynmax&(s,+,,a) 
Sutton (1988) introduces a general method for blending these alternativetraining estimates, called TD(h). The idea isto use a constant 0 5 h 5 1 tocombine the estimates obtained from various lookahead distances in the followingfashionAn equivalent recursive definition for Qh isNote ifwe choose h = 0 we have our original training estimate Q('), whichconsiders only one-step discrepancies in the Q estimates. Ash is increased, the al- 
gorithm places increasing emphasis on discrepancies based on more distant looka- 
heads. At the extreme value A. = 1, only the observed r,+i values are considered, 
with no contribution from the current Q estimate. Note when Q = Q, the trainingvalues given byQh will be identical for all values ofh such that 0 5 h 5 I. 
The motivation for the TD(h) method is that in some settings training willbe more efficient if more distant lookaheads are considered. For example, whenthe agent follows an optimal policy for choosing actions, then eh with h = 1 willprovide a perfect estimate for the true Q value, regardless of any inaccuracies inQ. On the other hand, if action sequences are chosen suboptimally, then the r,+iobserved far into the future can be misleading. 
Peng and Williams (1994) provide a further discussion and experimentalresults showing the superior performance of Qqn one problem domain. Dayan 
(1992) shows that under certain assumptions a similar TD(h) approach appliedto learning the V* function converges correctly for any h such that 0 5 A 5 1. 
Tesauro (1995) uses aTD(h) approach in his TD-GAMMON program for playingbackgammon. 
13.6 GENERALIZING FROM EXAMPLESPerhaps the most constraining assumption in our treatment ofQ learning up tothis point is that the target function is represented asan explicit lookup table, 
with a distinct table entry for every distinct input value (i.e., state-action pair). 
Thus, the algorithms we discussed perform a kind of rote learning and makeno attempt to estimate the Q value for unseen state-action pairs by generalizingfrom those that have been seen. This rote learning assumption is reflected in theconvergence proof, which proves convergence only if every possible state-actionpair is visited (infinitely often!). This is clearly an unrealistic assumption in largeor infinite spaces, or when the cost of executing actions is high. Asa result, 
more practical systems often combine function approximation methods discussedin other chapters with the Q learning training rules described here. 
Itis easy to incorporate function approximation algorithms such as BACK- 
PROPAGATION into the Q learning algorithm, by substituting a neural network forthe lookup table and using each ~(s, a) update asa training example. For example, 
we could encode the state s and action aas network inputs and train the networkto output the target values ofQ given by the training rules of Equations (13.7) 
and (13.10). An alternative that has sometimes been found tobe more successfulin practice isto train a separate network for each action, using the state as inputand Qas output. Another common alternative -isto train one network with thestate as input, but with one Q output for each action. Recall that in Chapter 1, wediscussed approximating an evaluation function over checkerboard states using alinear function and the LMS algorithm. 
In practice, a number of successful reinforcement learning systems have beendeveloped by incorporating such function approximation algorithms in place of thelookup table. Tesauro's successful TD-GAMMON program for playing backgammonused a neural network and the BACKPROPAGATION algorithm together with aTD(A) 
training rule. Zhang and Dietterich (1996) use a similar combination of BACKPROP- 
AGATION and TD(h) for job-shop scheduling tasks. Crites and Barto (1996) describea neural network reinforcement learning approach for an elevator scheduling task. 
Thrun (1996) reports a neural network based approach toQ learning to learn basiccontrol procedures for a mobile robot with sonar and camera sensors. Mahadevanand Connell (1991) describe aQ learning approach based on clustering states, 
applied toa simple mobile robot control problem. 
Despite the success of these systems, for other tasks reinforcement learningfails to converge once a generalizing function approximator is introduced. Ex- 
amples of such problematic tasks are given by Boyan and Moore (1995), Baird 
(1995), and Gordon (1995). Note the convergence theorems discussed earlier inthis chapter apply only when Qis represented byan explicit table. To see thedifficulty, consider using a neural network rather than an explicit table to repre- 
sent Q. Note if the learner updates the network to better fit the training Q valuefor a particular transition (si, ai), the altered network weights may also changethe Q estimates for arbitrary other transitions. Because these weight changes mayincrease the error inQ estimates for these other transitions, the argument prov- 
ing the original theorem no longer holds. Theoretical analyses of reinforcementlearning with generalizing function approximators are given by Gordon (1995) 
and Tsitsiklis (1994). Baird (1995) proposes gradient-based methods that circum- 
vent this difficulty by directly minimizing the sum of squared discrepancies inestimates between adjacent states (also called Bellman residual errors). 
13.7 RELATIONSHIP TO DYNAMIC PROGRAMMINGReinforcement learning methods such asQ learning are closely related toa longline of research on dynamic programming approaches to solving Markov decisionprocesses. This earlier work has typically assumed that the agent possesses perfectknowledge of the functions S(s, a) and r(s, a) that define the agent's environment. 
Therefore, it has primarily addressed the question of how to compute the optimalpolicy using the least computational effort, assuming the environment could beperfectly simulated and no direct interaction was required. The novel aspect ofQ learning is that it assumes the agent does not have knowledge ofS(s, a) andr(s, a), and that instead of moving about inan internal mental model of the statespace, it must move about the real world and observe the consequences. In thislatter case our primary concern is usually the number of real-world actions that theagent must perform to converge toan acceptable policy, rather than the number ofcomputational cycles it must expend. The reason is that in many practical domainssuch as manufacturing problems, the costs in time and in dollars of performingactions in the external world dominate the computational costs. Systems that learnby moving about the real environment and observing the results are typically calledonline systems, whereas those that learn solely by simulating actions within aninternal model are called ofline systems. 
The close correspondence between these earlier approaches and the rein- 
forcement learning problems discussed here is apparent by considering Bellman'sequation, which forms the foundation for many dynamic programming approachesto solving MDPs. Bellman's equation isNote the very close relationship between Bellman's equation and our earlier def- 
inition ofan optimal policy in Equation (13.2). Bellman (1957) showed that theoptimal policy n* satisfies the above equation and that any policy n satisfyingthis equation isan optimal policy. Early work on dynamic programming includesthe Bellman-Ford shortest path algorithm (Bellman 1958; Ford and Fulkerson1962), which learns paths through a graph by repeatedly updating the estimateddistance to the goal for each graph node, based on the distances for its neigh- 
bors. In this algorithm the assumption that graph edges and the goal node areknown is equivalent to our assumption that 6(s, a) and r(s, a) are known. Bartoet al. (1995) discuss the close relationship between reinforcement learning anddynamic programming. 
13.8 SUMMARY AND FURTHER READINGThe key points discussed in this chapter include: 
0 Reinforcement learning addresses the problem of learning control strategiesfor autonomous agents. It assumes that training information is available inthe form ofa real-valued reward signal given for each state-action transition. 
The goal of the agent isto learn an action policy that maximizes the totalreward it will receive from any starting state. 
0 The reinforcement learning algorithms addressed in this chapter fit a problemsetting known asa Markov decision process. In Markov decision processes, 
the outcome of applying any action to any state depends only on this ac- 
tion and state (and not on preceding actions:or states). Markov decisionprocesses cover a wide range of problems including many robot control, 
factory automation, and scheduling problems. 
0 Q learning is one form of reinforcement learning in which the agent learnsan evaluation function over states and actions. In particular, the evaluationfunction Q(s, a) is defined as the maximum expected, discounted, cumulativereward the agent can achieve by applying action ato state s. The Q learningalgorithm has the advantage that it can-be employed even when the learnerhas no prior knowledge of how its actions affect its environment. 
0 Q learning can be proven to converge to the correct Q function under cer- 
tain assumptions, when the learner's hypothesis ~(s, a) is represented by alookup table with a distinct entry for each (s, a) pair. It can be shown toconverge in both deterministic and nondeterministic MDPs. In practice, Qlearning can require many thousands of training iterations to converge ineven modest-sized problems. 
0 Q learning isa member ofa more general class of algorithms, called tem- 
poral difference algorithms. In general, temporal difference algorithms learnCHAFER 13 REINFORCEMENT LEARNING 387by iteratively reducing the discrepancies between the estimates produced bythe agent at different times. 
Reinforcement learning is closely related to dynamic programming ap- 
proaches to Markov decision processes. The key difference is that histori- 
cally these dynamic programming approaches have assumed that the agentpossesses knowledge of the state transition function 6(s, a) and reward func- 
tion r (s , a). In contrast, reinforcement learning algorithms such asQ learningtypically assume the learner lacks such knowledge. 
The common theme that underlies much of the work on reinforcement learn- 
ing isto iteratively reduce the discrepancy between evaluations of successivestates. Some of the earliest work on such methods is due to Samuel (1959). Hischeckers learning program attempted to learn an evaluation function for checkersby using evaluations of later states to generate training values for earlier states. 
Around the same time, the Bellman-Ford, single-destination, shortest-path algo- 
rithm was developed (Bellman 1958; Ford and Fulkerson 1962), which propagateddistance-to-goal values from nodes to their neighbors. Research on optimal controlled to the solution of Markov decision processes using similar methods (Bellman1961; Blackwell 1965). Holland's (1986) bucket brigade method for learning clas- 
sifier systems used a similar method for propagating credit in the face of delayedrewards. Barto etal. (1983) discussed an approach to temporal credit assignmentthat led to Sutton's paper (1988) defining the TD(k) method and proving its con- 
vergence for k = 0. Dayan (1992) extended this result to arbitrary values ofk. 
Watkins (1989) introduced Q learning to acquire optimal policies when the re- 
ward and action transition functions are unknown. Convergence proofs are knownfor several variations on these methods. In addition to the convergence proofspresented in this chapter see, for example, (Baird 1995; Bertsekas 1987; Tsitsiklis1994, Singh and Sutton 1996). 
Reinforcement learning remains an active research area. McCallum (1995) 
and Littman (1996), for example, discuss the extension of reinforcement learningto settings with hidden state variables that violate the Markov assumption. Muchcurrent research seeks to scale up these methods to larger, more practical prob- 
lems. For example, Maclin and Shavlik (1996) describe an approach in which areinforcement learning agent can accept imperfect advice from a trainer, based onan extension to the KBANN algorithm (Chapter 12). Lin (1992) examines the roleof teaching by providing suggested action sequences. Methods for scaling Up byemploying a hierarchy of actions are suggested by Singh (1993) and Lin (1993). 
Dietterich and Flann (1995) explore the integration of explanation-based methodswith reinforcement learning, and Mitchell and Thrun (1993) describe the appli- 
cation of the EBNN algorithm (Chapter 12) toQ learning. Ring (1994) explorescontinual learning by the agent over multiple tasks. 
Recent surveys of reinforcement learning are given by Kaelbling etal. 
(1996); Barto (1992); Barto etal. (1995); Dean etal. (1993). 
EXERCISES13.1. Give a second optimal policy for the problem illustrated in Figure 13.2. 
13.2. Consider the deterministic grid world shown below with the absorbing goal-stateG. Here the immediate rewards are 10 for the labeled transitions and 0 for allunlabeled transitions. 
(a) Give the V* value for every state in this grid world. Give the Q(s, a) value forevery transition. Finally, show an optimal policy. Use y = 0.8. 
(b) Suggest a change to the reward function r(s, a) that alters the Q(s, a) values, 
but does not alter the optimal policy. Suggest a change tor(s, a) that altersQ(s, a) but does not alter V*(s, a). 
(c) Now consider applying the Q learning algorithm to this grid world, assumingthe table ofQ values is initialized to zero. Assume the agent begins in thebottom left grid square and then travels clockwise around the perimeter ofthe grid until it reaches the absorbing goal state, completing the first trainingepisode. Describe which Q values are modified asa result of this episode, andgive their revised values. Answer the question again assuming the agent nowperforms a second identical episode. Answer it again for a third episode. 
13.3. Consider playing Tic-Tac-Toe against an opponent who plays randomly. In partic- 
ular, assume the opponent chooses with uniform probability any open space, unlessthere isa forced move (in which case it makes the obvious correct move). 
(a) Formulate the problem of learning an optimal Tic-Tac-Toe strategy in this caseas aQ-learning task. What are the states, transitions, and rewards in this non- 
deterministic Markov decision process? 
(b) Will your program succeed if the opponent plays optimally rather than ran- 
domly? 
13.4. Note in many MDPs itis possible to find two policies nl and n2 such that nloutperforms 172 if the agent begins in'some state sl, but n2 outperforms nlif itbegins in some other state s2. Put another way, Vnl (sl) > VR2(s1), but Vn2(s2) > 
VRl (s2) Explain why there will always exist a single policy that maximizes Vn(s) 
for every initial state s (i.e., an optimal policy n*). In other words, explain why anMDP always allows a policy n* such that (Vn, s) vn*(s) 2 Vn(s). 
REFERENCESBaird, L. (1995). Residual algorithms: Reinforcement learning with function approximation. Proceed- 
ings of the Twelfrh International Conference on Machine Learning @p. 30-37). San Francisco: 
Morgan Kaufmann. 
CHaPrER 13 REINFORCEMENT LEARNING 389Barto, A. (1992). Reinforcement learning and adaptive critic methods. InD. White & S. Sofge (Eds.), 
Handbook of intelligent control: Neural, fuzzy, and adaptive approaches (pp. 469-491). NewYork: Van Nostrand Reinhold. 
Barto, A., Bradtke, S., & Singh, S. (1995). Learning to act using real-time dynamic programming. 
ArtiJicial Intelligence, Special volume: Computational research on interaction and agency, 
72(1), 81-138. 
Barto, A., Sutton, R., & Anderson, C. (1983). Neuronlike adaptive elements that can solve difficultlearning control problems. IEEE Transactions on Systems, Man, and Cybernetics, 13(5), 834- 
846. 
Bellman, R. E. (1957). Dynamic Programming. Princeton, NJ: Princeton University Press. 
Bellrnan, R. (1958). Ona routing problem. Quarterly of Applied Mathematics, 16(1), 87-90. 
Bellman, R. (1961). Adaptive control processes. Princeton, NJ: Princeton University Press. 
Berenji, R. (1992). Learning and tuning fuzzy controllers through reinforcements. IEEE Transactionson Neural Networks, 3(5), 724-740. 
Bertsekas, D. (1987). Dynamicprogramming: Deterministic and stochastic models. Englewood Cliffs, 
NJ: Prentice Hall. 
Blackwell, D. (1965). Discounted dynamic programming. Annals of Mathematical Statistics, 36,226- 
235. 
Boyan, J., & Moore, A. (1995). Generalization in reinforcement learning: Safely approximating thevalue function. InG. Tesauro, D. Touretzky, & T. Leen (Eds.), Advances in Neural InformationProcessing Systems 7. Cambridge, MA: MIT Press. 
Crites, R., & Barto, A. (1996). Improving elevator performance using reinforcement learning. InD. S. Touretzky, M. C. Mozer, & M. C. Hasselmo (Eds.), Advances in Neural InformationProcessing Systems, 8. 
Dayan, P. (1992). The convergence ofTD(A) for general A. Machine Learning, 8, 341-362. 
Dean, T., Basye, K., & Shewchuk, J. (1993). Reinforcement learning for planning and control. InS. 
Minton (Ed.), Machine Learning Methods for Planning @p. 67-92). San Francisco: MorganKaufmann. 
Dietterich, T. G., & Flann, N. S. (1995). Explanation-based learning and reinforcement learning: 
A unified view. Proceedings of the 12th International Conference on Machine Learning @p. 
176-184). San Francisco: Morgan Kaufmann. 
Ford, L., & Fulkerson, D. (1962). Flows in networks. Princeton, NJ: Princeton University Press. 
Gordon, G. (1995). Stable function approximation in dynamic programming. Proceedings of theTwelfth International Conference on Machine Learning (pp. 261-268). San Francisco: MorganKaufmann. 
Kaelbling, L. P., Littman, M. L., & Moore, A. W. (1996). Reinforcement learning: A survey. Journalof AI Research, 4, 237-285. Online journal at http://www.cs.washington.edu/research/jair/- 
home.htm1. 
Holland, J. H. (1986). Escaping brittleness: The possibilities of general-purpose learning algorithmsapplied to parallel rule-based systems. In Michalski, Carbonell, & Mitchell (Eds.), Machinelearning: An artijicial intelligence approach (Vol. 2, pp. 593423). San Francisco: MorganKaufmann. 
Laird, J. E., & Rosenbloom, P. S. (1990). Integrating execution, planning, and learning in SOAR forexternal environments. Proceedings of the Eighth National Conference on Artificial Intelligence 
(pp. 1022-1029). Menlo Park, CA: AAAI Press. 
Lin, L. J. (1992). Self-improving reactive agents based on reinforcement learning, planning, andteaching. Machine Learning, 8, 293-321. 
Lin, L. J. (1993). Hierarchical learning of robot skills by reinforcement. Proceedings of the Interna- 
tional Conference on Neural Networks. 
Littman, M. (1996). Algorithms for sequential decision making (Ph.D. dissertation and TechnicalReport CS-96-09). Brown University, Department of Computer Science, Providence, RI. 
Maclin, R., & Shavlik, J. W. (1996). Creating advice-taking reinforcement learners. Machine Learn- 
ing, 22, 251-281. 
Mahadevan, S. (1996). Average reward reinforcement learning: Foundations, algorithms, and empir- 
ical results. Machine Learning, 22(1), 159-195. 
Mahadevan, S., & Connell, J. (1991). Automatic programming of behavior-based robots using rein- 
forcement learning. In Proceedings of the Ninth National Conference on ArtGcial Intelligence. 
San Francisco: Morgan Kaufmann. 
McCallum, A. (1995). Reinforcement learning with selective perception and hidden state (Ph.D. dis- 
sertation). Department of Computer Science, University of Rochester, Rochester, NY. 
Mitchell, T. M., & Thrun, S. B. (1993). Explanation-based neural network learning for robot control. 
InC. Giles, S. Hanson, & J. Cowan (Eds.), Advances in Neural Information Processing System5 (pp. 287-294). San Francisco: Morgan-Kaufmann. 
Moore, A., & Atkeson C. (1993). Prioritized sweeping: Reinforcement learning with less data andless real time. Machine Learning, 13, 103. 
Peng, J., & Williams, R. (1994). Incremental multi-step Q-learning. Proceedings of the Eleventhinternational Conference on Machine Learning (pp. 226-232). San Francisco: Morgan Kauf- 
mann. 
Ring, M. (1994). Continual learning in reinforcement environments (Ph.D. dissertation). ComputerScience Department, University of Texas at Austin, Austin, TX. 
Samuel, A. L. (1959). Some studies in machine learning using the game of checkers. IBM Journalof Research and Development, 3, 21 1-229. 
Singh, S. (1992). Reinforcement learning with a hierarchy of abstract models. Proceedings of theTenth National Conference on Art@cial Intelligence (pp. 202-207). San Jose, CA: AAAIPress. 
Singh, S. (1993). Learning to solve markovian decision processes (Ph.D. dissertation). Also CMPSCITechnical Report 93-77, Department of Computer Science, University of Massachusetts atAmherst. 
Singh, S., & Sutton, R. (1996). Reinforcement learning with replacing eligibility traces. MachineLearning, 22, 123. 
Sutton, R. (1988). Learning to predict by the methods of temporal differences. Machine learning, 3, 
9-44Sutton R. (1991). Planning by incremental dynamic programming. Proceedings of the Eighth Znter- 
national Conference on Machine Learning (pp. 353-357). San Francisco: Morgan Kaufmann. 
Tesauro, G. (1995). Temporal difference learning and TD-GAMMON. Communications of the ACM, 
38(3), 58-68. 
Thrun, S. (1992). The role of exploration in learning control. InD. White & D. Sofge (Eds.), 
Handbook of intelligent control: Neural, fizzy, and adaptive approaches (pp. 527-559). NewYork: Van Nostrand Reinhold. 
Thrun, S. (1996). Explanation-based neural network learning: A lifelong learning approach. Boston: 
Kluwer Academic Publishers. 
Tsitsiklis, J. (1994). Asynchronous stochastic approximation and Q-learning. Machine Learning, 
16(3), 185-202. 
Watkins, C. (1989). Learning from delayed rewards (Ph.D. dissertation). King's College, Cambridge, 
England. 
Watkins, C., & Dayan, P. (1992). Q-learning. Machine Learning, 8, 279-292. 
Zhang, W., & Dietterich, T. G. (1996). High-performance job-shop scheduling with a time-delayTD(A) network. InD. S. Touretzky, M. C. Mozer, & M. E. Hasselmo (Eds.), Advances inneural information processing systems, 8, 1024-1030. 
APPENDIXNOTATIONBelow isa summary of notation used in this book. 
(a, b]: Brackets of the form [, 1, (, and ) are used to represent intervals, 
where square brackets represent intervals including the boundaryand round parentheses represent intervals excluding the boundary. 
For example, (1, 31 represents the interval 1 < x 5 3. 
Cxi: The sumx~ +x2+...+xn. 
i=ln 
Hxi: The product xl .x2..-xn. 
i=lF: The symbol for logical entailment. For example, AF B denotesthat B follows deductively from A. 
>,: The symbol for the more general than relation. For example, hi >, 
hj denotes that hypothesis hiis more general than hi. 
argmax f (x): The value ofx that maximizes f (x). For example, 
xexargmax x2 = -3x~{1,2,-3) 
f(x): A function that approximates the function f (x). 
6: In PAC-learning, a bound on the probability of failure. In artificialneural network learning, the error term associated with a single unitoutput. 
E : 
r]: 
P: 
n: 
VE(G): 
C: 
D : 
D: 
E [x] : 
E(G): 
Error: 
H: 
h (x) : 
P(x): 
Pr(x) : 
p(x>: 
Q<s, a): 
3: 
VC(H): 
VSH,D: 
A bound on the error ofa hypothesis (in PAC-learning). 
The learning rate in neural network and related learning methods. 
The mean ofa probability distribution. 
The standard deviation ofa probability distribution. 
The gradient ofE with respect to the vector G. 
Class of possible target functions. 
The training data. 
A probability distribution over the instance space. 
The expected value ofx. 
The sum of squared errors ofan artifial neural network whoseweights are given by the vector G. 
The error ina discrete-valued hypothesis or prediction. 
Hypothesis space. 
The prediction produced by hypothesis h for instance x. 
The probability (mass) ofx. 
The probability (mass) of the event x. 
The probability density ofx. 
The Q function from reinforcement learning. 
The set of real numbers. 
The Vapnik-Chervonenkis dimension of the hypothesis space H. 
The Version Space; that is, the set of hypotheses from H that areconsistent with D. 
In artificial neural networks, the weight from node ito node j. 
Instance space. 
INDEXES400 SUBJECT INDEXSUBJECT INDEXPage numbers in italics refer to tables; numbers in bold to figures. An "n" fol- 
lowing a page number refers toa footnote on that page. 
Absorbing state, 371ABSTRIPS, 329Acyclic neural networks. See Multilayerfeedforward networksAdaline rule. See Delta ruleAdditive Chernoff bounds, 210-21 1Adelines, 123Agents, in reinforcement learning, 368Agnostic learning, 210-21 1,225ALVINN system, 82-83, 84Analytical-inductive learning. SeeInductive-analytical learningAnalytical learning, 307-330inductive learning, comparison with, 
310, 328-329, 334-336, 362ANN learning. See Neural networklearningANNs. See Neural networks, artificialAntecedents of Horn clause, 285AQ algorithm, 279-280AQ14 algorithm, comparison with GABIL, 
256,258Arbitrary functions, representation byfeedforward networks, 105-106Artificial intelligence, influence onmachine learning, 4Artificial neural networks. See Neuralnetworks, artificialASSISTANT, 77Astronomical structures, machine learningclassification of, 3Attributes: 
choice of, in sequential vs. simultaneouscovering algorithms, 280-281continuous-valued, 72-73cost-sensitive measures, 75-76discrete-valued, 72measures for selection of, 73-74, 77missing values, strategies for, 75Autonomous vehicles, 3, 4, 82-83, 84Average reward, 371Backgammon learning program. SeeTD-GAMMONBACKPROPAGATION algorithm, 83,97, 124applications of, 81, 84, 85, 96, 113convergence and local minima, 104-105definition of, 98discovery of hidden layer representations, 
106-109, 123feedforward networks as hypothesisspace, 105-106gradient descent search, 89, 115-1 16, 
123inductive bias of, 106KBANN algorithm: 
comparison with, 344-345use in, 339momentum, addition of, 100, 104overfitting in, 108, 110-1 11in Q learning, 384search of hypothesis space, 97, 106, 
122-123in decision tree learning, comparisonwith, 106by genetic algorithms, comparisonwith, 259by KBANN and TANGENTPROPalgorithms, comparison with, 
350-351stochastic gradient descent version, 
98-100, 104-105, 107-108TANGENTPROP algorithm, comparisonwith, 349weight update rule: 
alternative error functions, 117-1 18derivation of, 101-102for hidden unit weights, 103in KBANN algorithm, 343-344optimization methods, 119 , 
for output unit weights, 102-103, 171Backtracking, ID3 algorithm and, 62Backward chaining search for explanationgeneration, 3 14Baldwin effect, 250, 267computational models for, 267-268Bayes classifier, naive. See Naive BayesclassifierBayes optimal classifier, 174-176, 197, 
222learning Boolean concepts using versionspaces, 176Bayes optimal learner. See Bayes optimalclassifierBayes rule. See Bayes theoremBayes theorem, 4, 156-159in BRUTE-FORCE MAP LEARNINGalgorithm, 160-162concept learning and, 158-163in inductive-analytical learning, 338Bayesian belief networks, 184-191choice among alternative networks, 190conditional independence in, 185constraint-based approaches in, 191gradient ascent search in, 188-190inference methods, 187-188joint probability distributionrepresentation, 185-1 87learning from training data, 188-191naive Bayes classifier, comparison with, 
186representation of causal knowledge, 187Bayesian classifiers, 198. See also Bayesoptimal classifier; Naive BayesclassifierBayesian learning, 154-198decision tree learning, comparison with, 
198Bayesian methods, influence on machinelearning, 4Beam search: 
general-to-specific. See General-to- 
specific beam searchgenerate-and-test. See Generate-and-testbeam searchBellman-Ford shortest path algorithm, 386, 
1117Bellman residual errors, 385Bellman's equation, 385-386BFS-ID3 algorithm, 63Binomial distribution, 133-137, 143, 151Biological evolution, 249, 250, 266-267Biological neural networks, comparisonwith artificial neural networks, 82Bit strings, 252-253, 258-259, 269Blocks, stacking of. See Stacking problemsBody of Horn clause, 285Boolean conjunctions, PAC learning of, 
211-212Boolean functions: 
representation by feedforward networks, 
105-106representation by perceptrons, 87-88Boundary set representation for versionspaces, 3 1-36definition of, 3 1Bounds: 
one-sided, 141, 144two-sided, 141Brain, neural activity in, 82Breadth first search in ID3 algorithm, 63BRUTE-FORCE MAP LEARNING algorithm, 
159-162Bayes theorem in, 160-162C4.5 algorithm, 55, 77GABIL, comparison with, 256,258missing attribute values, method forhandling, 75rule post-pruning in, 71-72CADET system, 241-244CANDIDATE-ELIMINATION algorithm, 
29-37,4547applications of, 29, 302Bayesian interpretation of, 163computation of version spaces, 32-36definition of, 33ID3 algorithm, comparison with, 61-64inductive bias of, 43-46, 63-64limitations of, 29, 37, 41, 42, 46search of hypothesis space, 64Candidate specializations: 
generated by FOCL algorithm, 357-361generated by FOIL algorithm, 287-288, 
CART system, 77CASCADE-CORRELATION algorithm, 
121-123Case-based reasoning, 23 1, 240-244, 246, 
247advantages of, 243-244applications of, 240other instance-based learning methods, 
comparison with, 240Causal knowledge, representation byBayesian belief networks, 187Central Limit Theorem, 133, 142-143, 167Checkers learning program, 2-3,5-14, 387algorithms for, 14design, 13as sequential control process, 369Chemical mass spectroscopy, 
CANDIDATE-ELIMINATION algorithmin, 29Chess learning program, 308-310explanation-based learning in, 325Chunking, 327, 330CIGOL, 302Circuit design, genetic programming in, 
265-266 
' Circuit layout, genetic algorithms in, 
256Classification problems, 54CLA~~IFYJAIVEBAYES-TEXT, 182-183CLAUDIEN, 302Clauses, 284, 285CLS. See Concept Learning SystemClustering, 191CN2 algorithm, 278, 301choice of attribute-pairs in, 280-281Complexity, sample. See SamplecomplexityComputational complexity, 202Computational complexity theory, 
influence on machine learning, 4Computational learning theory, 
201-227Concept learning, 20-47algorithms for, 47Bayes theorem and, 158-163definition of, 21genetic algorithms in, 256ID3 algorithm specialized for, 56notation for, 22-23search of hypothesis space, 23-25, 
4-7task design in, 21-22Concept Learning System, 77Concepts, partially learned, 38-39Conditional independence, 185in Bayesian belief networks, 186-187Confidence intervals, 133, 138-141, 150, 
151for discrete-valued hypotheses, 13 1-132, 
140-141derivation of, 142-143one-sided, 144, 145Conjugate gradient method, 119Conjunction of boolean literals, PAClearning of, 21 1-212Consequent of Horn clause, 285Consistent learners, 162-163bound on sample complexity, 207-210, 
225equation for, 209Constants, in logic, 284, 285Constraint-based approaches in Bayesianbelief networks, 191Constructive induction, 292Continuous functions, representationby feedforward networks, 
105-106Continuous-valued hypotheses, trainingerror of, 89-90Continuous-valued target function, 197maximum likelihood (ML) hypothesisfor, 164-167Control theory, influence on machinelearning, 4Convergence ofQ learning algorithm: 
in deterministic environments, 377-380, 
386in nondeterministic environments, 
382-383, 386Credit assignment, 5Critic, 12, 13Cross entropy, 170minimization of, 1 18Cross-validation, 11 1-1 12for comparison of learning algorithms, 
145-151k-fold. See k-fold cross-validationin k-NEAREST NEIGHBOR algorithm, 235leave-one-out, 235in neural network learning, 11 1-1 12Crossover mask, 254Crossover operators, 252-254, 261, 
262single-point, 254, 261two-point, 254, 257-258uniform, 255Crowding, 259, 
Cumulative reward, 371Curse of dimensionality, 235Data mining, 17Decision tree learning, 52-77algorithms for, 55, 77. See also C4.5algorithm, ID3 algorithmapplications of, 54Bayesian learning, comparison with, 198impact of pruning on accuracy, 128-129inductive bias in, 63-66k-NEAREST NEIGHBOR algorithm, 
comparison with, 235Minimum Description Length principlein, 173-174neural network learning, comparisonwith, 85overfitting in, 6749, 76-77, 11 1post-pruning in, 68-69, 77reduced-error pruning in, 69-7 1rule post-pruning in, 71-72, 281search of hypothesis space, 60-62by BACKPROPAGATION algorithm, 
comparison with, 106Deductive learning, 321-322Degrees of freedom, 147Delayed learning methods, comparisonwith eager learning, 244-245Delayed reward, in reinforcement learning, 
369Delta rule, 11, 88-90, 94, 99, 123Demes, 268Determinations, 325Deterministic environments, Q learningalgorithm for, 375Directed acyclic neural networks. SeeMultilayer feedforward networksDiscounted cumulative reward. 371Discrete-valued hypotheses: 
confidence intervals for, 131-132, 
140-141derivation of, 142-143training error of, 205Discrete-valued target functions, 
approximation by decision treelearning, 52Disjunctive sets of rules, learning bysequential covering algorithms, 
275-276Distance-weighted k-NEAREST NEIGHBORalgorithm, 233-234Domain-independent learning algorithms, 
336Domain theory, 310, 329. See alsoimperfect domain theory; Perfectdomain theory; Prior knowledgein analytical learning, 31 1-312as KBANN neural network, 342-343in PROLOG-EBG, 322weighting of components in EBNN, 
35 1-352DYNA, 380Dynamic programming: 
applications to reinforcement learning, 
380reinforcement learning and, 385-387Eager learning methods, comparison withlazy learning, 244245EBG algorithm, 3 13EBNN algorithm, 351-356, 362, 387other explanation-based learningmethods, comparison with, 356prior knowledge and gradient descent in, 
339TANGENTPROP algorithm in, 353weighting of inductive-analyticalcomponents in, 355,362EGGS algorithm, 3 13EM algorithm, 190-196, 197applications of, 191, 194derivation of algorithm for k-means, 
195-196search for maximum likelihood (ML) 
hypothesis, 194-195Entailment, 321nrelationship with 8-subsumption andmore-general-than partial ordering, 
299-300Entropy, 55-57, 282of optimal code, 172nEnvironment, in reinforcement learning, 
368Equivalent sample size, 179-1 80Error bars for discrete-valued hypotheses. 
See Confidence intervals, fordiscrete-valued hypothesesError of hypotheses: 
sample. See Sample errortraining. See Training errortrue. See True errorEstimation bias, 133, 137-138, 151Estimator, 133, 137-138, 143, 150-151Evolution of populations: 
argument for Occam's razor, 66in genetic algorithms, 260-262Evolutionary computation, 250, 262applications of, 269Example-driven search, comparison withgenerate-and-test beam search, 281Expected value, 133, 136Experiment generator, 12-13Explanation-based learning, 3 12-330applications of, 325-328derivation of new features, 320-321inductive bias in, 322-323inductive learning and, 330lazy methods in, 328limitations of, 308, 329prior knowledge in, 308-309reinforcement learning and, 330utility analysis in, 327-328Explanations generated by backwardchaining search, 314Explicit prior knowledge, 329Exploration in reinforcement learning, 369Face recognition, 17BACKPROPAGATION algorithm in, 8 1, 
112-1 17Feedforward networks. See MultilayerFIND-S algorithm, 26-28, 46Bayesian interpretation of, 162-163definition of, 26inductive bias of, 45limitations of, 28-29mistake-bound learning in, 220-221PAC learning of boolean conjunctionswith, 212search of hypothesis space, 27-28Finite horizon reward, 37 1First-order Horn clauses, 283-284, 
3 18-3 19. See also First-order rulesin analytical learning, 3 1 1in PROLOG-EBG, 313, 314First-order logic, basic definitions, 285First-order representations, applications of, 
275First-order resolution rule, 296-297First-order rules, 274-275, 283, 301, 302. 
See also First-order Horn clausesin FOIL algorithm, 285-291propositional rules, comparison with, 
283Fitness function, 250-252, 255-256, 258Fitness proportionate selection, 255Fitness sharing, 259FOCL algorithm, 302extensions to FOIL, 357search step alteration with priorknowledge, 339-340FOIL algorithm, 286,290-291, 302extensions in FOCL, 357information gain measure in, 289LEARN-ONE-RULE and sequentialcovering algorithms, comparisonwith, 287learning first-order rules in, 285-291post-pruning in, 291recursive rule learning in, 290Function approximation, 8Function approximation algorithms: 
choice of, 9-1 1as lookup table substitute, 384Functions, in logic, 284, 285GABIL, 256-259, 269C4.5 and AQ14 algorithms, comm.risonfeedforward networks with, 25-6, 258extensions to, 258-259ID5R algorithm, comparison with, 258Gain ratio, 73-74GAS. See Genetic algorithmsGaussian distribution. See NormaldistributionGaussian kernel function, 238-240General-to-specific beam search, 277-279, 
302advantages of, 281in CN2 algorithm, 278in FOCL algorithm, 357-361in FOIL algorithm, 287,357-358General-to-specific ordering ofhypotheses, 24-25, 4546. See alsoMore-general-than partial orderingGeneralization accuracy in neuralnetworks, 1 10-1 1 1Generalizer, 12, 13Generate-and-test beam search, 250example-driven search, comparison with, 
28 1inverse entailment operators, comparisonwith, 299inverse resolution, comparison with, 
298-299Genetic algorithms, 249-270advantages of, 250applications of, 256, 269fitness function in, 255-256limitations of, 259parallelization of, 268representation of hypotheses, 252-253search of hypothesis space, 259, 
268-269Genetic operators, 252-255, 257, 261-262Genetic programming, 250, 262-266, 269applications of, 265, 269performance of, 266representation in, 262-263Gibbs algorithm, 176Global method, 234GOLEM, 28 1GP. See Genetic programmingGradient ascent search, 170-171in Bayesian belief networks, 188-190Gradient descent search, 89-91, 93, 97, 
115-116, 123in EBNN algorithm, 339least-squared error hypothesis in, 167limitations of, 92weight update rule, 91-92, 237stochastic approximation to, 92-94, 
98-100, 104-105, 107-108Gradient of error, 91Greedy search: 
in sequential covering algorithms, 
276-278in PROLOG-EBG, 323GRENDEL program, 303Ground literal, 285HALVING algorithm, 223mistake-bound learning in, 221-222Handwriting recognition, 34BACKPROPAGATION algorithm in, 8 1TANGENTPROP algorithm in, 348-349Head of Horn clause, 285Hidden layer representations, discoveryby BACKPROPAGATION algorithm, 
106-109, 123Hidden units: 
BACKPROPAGATION weight tuning rulefor, 103CASCADE-CORRELATION algorithm, 
addition by, 121-123choice in radial basis function networks, 
239-240in face recognition task, 1 15-1 17Hill-climbing search: 
in FOIL algorithm, 286,287in genetic algorithms, 268in ID3 algorithm, 60-61Hoeffding bounds, 210-21 1Horn clauses, 284, 285Horn clauses, first-order. See First-orderHorn clausesHuman learning: 
explanations in, 309prior knowledge in, 330Hypotheses. See also Discrete-valuedhypotheses; General-to-specificordering of hypotheses; Hypothesisspaceerror differences between two, 143-144estimation of accuracy, 129-130Hypotheses, estimation of accuracy 
(continued) 
bias and variance in estimate, 129, 
151, 152errors in, 129-131, 151evaluation of, 128-129justification of, in inductive vs. analyticallearning, 334-336representations of, 23testing of, 144-145Hypothesis space, 14-15bias in, 40-42, 46, 129finite, sample complexity for, 207-214, 
225infinite, sample complexity for, 214-220VC dimension of, 214-217Hypothesis space searchby BACKPROPAGATION algorithm, 97, 
106, 122-123comparison with decision treelearning, 106comparison with KBANN andTANGENTPROP algorithms, 350-35 1by CANDIDATE-ELIMINATION algorithm, 
64in concept learning, 23-25, 46-47constraints on, 302-303by FIND-S algorithm, 27-28by FOIL algorithm, 286-287, 357-361by genetic algorithms, 250, 259by gradient descent, 90-91by ID3 algorithm, 60-62,64, 76by KBANN algorithm, 346by learning algorithms, 24by LEARN-ONE-RULE, 277in machine learning, 14-15, 18use of prior knowledge, 339-340, 362ID3 algorithm, 55-64,77backtracking and, 62CANDIDATE-ELIMINATION algorithm, 
comparison with, 61-62choice of attributes in, 280-281choice of decision tree, 63cost-sensitive measures, 75-76extensions to, 77. See also C4.5algorithminductive bias of, 63-64, 76LEARN-ONE-RULE, search comparisonwith, 277limitations of, 61-62overfitting in, 67-68search of hypothesis space, 60-62, 64, 
76sequential covering algorithms, 
comparison with, 280-281specialized for concept learning, 56use of information gain in, 58-60ID5R algorithm, comparison with GABIL, 
258ILP. See Inductive logic programmingImage encoding in face recognition, 114Imperfect domain theory: 
in EBNN algorithm, 356in explanation-based learning, 330in FOCL algorithm, 360in KBANN algorithm, 344-345Incremental explanation methods, 328Incremental gradient descent. SeeStochastic gradient descentINCREMENTAL VERSION SPACE MERGINGalgorithm, 47Inductive-analytical learning, 334-363advantages of, 362explanation-based learning and, 330learning problem, 337-338prior knowledge methods to alter search, 
339-340,362properties of ideal systems, 337weighting of components in EBNNalgorithm, 351-352,355weighting prior knowledge in, 338Inductive bias, 39-45, 137-138. See alsoOccam's razor; Preference bias; 
Restriction biasof BACKPROPAGATION algorithm, 106bias-free learning, 40-42of CANDIDATE-ELIMINATION algorithm, 
43-46, 63-64in decision tree learning, 63-66definition of, 43in explanation-based learning, 322-323of FIND-S algorithm, 45of ID3 algorithm, 63-64,76of inductive learning algorithms, 42-46of k-NEAREST NEIGHBOR algorithm, 234of LMS algorithm, 64of ROTE-LEARNER algorithm, 44-45Inductive inference. See Inductive learningInductive learning, 42, 307-308. Seealso Decision tree learning; 
Genetic algorithms; Inductive logicprogramming; Neural networklearninganalytical learning, comparison with, 
310, 328-329, 334-336, 362inductive bias in, 4246Inductive learning hypothesis, 23Inductive logic programming, 275,29 1PROLOG-EBG, comparison with, 322Information gain, 73definition of, 57-58in FOIL algorithm, 289in ID3 algorithm, 55,5840Information theory: 
influence on machine learning, 4Minimum Description Length principleand, 172Initialize-thehypothesis approach, 
339-346Bayesian belief networks in, 346Instance-based learning, 230-247. See alsoCase-based reasoning; k-NEARESTNEIGHBOR algorithm; Locallyweighted regressionadvantages, 245-246case-based reasoning, comparison withother methods, 240limitations of, 23 1Inverse entailment, 292, 302first-order, 297generate-and-test beam search, 
comparison with, 299in PROGOL, 300-302Inverse resolution, 294-296, 302first-order, 297-298generate-and-test beam search, 
comparison with, 298-299limitations of, 300Inverted deduction, 291-293J 
Jacobian, 354Job-shop scheduling, genetic algorithms in, 
Joint probability distribution, in Bayesianbelief networks, 185-187k-fold cross-validation, 112, 147, 150k-means problem, 19 1-193derivation ofEM algorithm for, 195-196k-NEAREST NEIGHBOR algorithm, 23 1-233, 
246applications of, 234cross-validation in, 235decision tree and rule learning, 
comparison with, 235distance-weighted, 233-234inductive bias of, 234memory indexing in, 236k-term CNF expressions, 2 13-214k-term DNF expressions, 213-214K2 algorithm, 190-191KBANN algorithm, 340-347, 362, 387advantages of, 344BACKPROPAGATION algorithm, 
comparison with, 344-345BACKPROPAGATION weight update rulein, 343-344hypothesis space search byBACKPROPAGATION andTANGENTPROP, comparison with, 
350-35 1limitations of, 345prior knowledge in, 339kd-tree, 236Kernel function, 236, 238, 246Kernel function, Gaussian. See Gaussiankernel functionKnowledge-Based Artificial NeuralNetwork (KBANN) algorithm. SeeKBANN algorithmKnowledge compilation, 320Knowledge level learning, 323-325Knowledge reformulation, 320Lamarckian evolution, 266Language bias. See Restriction biasLazy explanation methods, 328Lazy learning methods, comparison witheager learning, 244-245LEARN-ONE-RULE algorithm: 
FOIL algorithm, comparison with, 287ID3 algorithm, search comparison with, 
277rule performance in, 282rule post-pruning in, 28 1variations of, 279-280,286Learning: 
human. See Human learningmachine. See Machine learningLearning algorithmsconsistent learners, 162-163design of, 9-11, 17domain-independent, 336error differences between two, 145-15 1search of hypothesis space, 24Learning problems, 2-5, 17computational theory of, 201-202in inductive-analytical learning, 337-338Learning rate, 88, 91Learning systems: 
design of, 5-14, 17program modules, 11-1' : 
Least mean squares algori ,m. See LMSalgorithmLeast-squared error hypothesis: 
classifiers for, 198gradient descent in, 167maximum likelihood (ML) hypothesisand, 164-167Leave-one-out cross-validation, 235Legal case reasoning, case-based reasoningin, 240LEMMA-ENUMERATOR algorithm, 324Lifelong learning, 370Line search, 119Linear programming, as weight updatealgorithm, 95Linearly separable sets, 86, 89, 95LIST-THEN-ELLMINATE algorithm, 30Literal, 284, 285LMS algorithm, 11, 15inductive bias of, 64LMS weight update rule. See Delta ruleLocal method, 234Locally weighted regression, 23 1, 
236-238, 246limitations of, 238Logical constants, 284, 285Logical terms, 284, 285Logistic function, 96, 104Lookup table: 
function approximation algorithms assubstitute, 384neural network as substitute, 384Lower bound on sample complexity, 
217-218m-estimate of probability, 179-180, 198, 
282Machine learning, 15. See also entriesbeginning with Learningapplications, 3, 17definition of, 2influence of other disciplines on, 4, 17search of hypothesis space, 14-15, 18Manufacturing process control, 17MAP hypothesis. See Maximuma posteriori hypothesisMAP LEARNING algorithm, BRUTE-FORCE. 
See BRUTE-FORCE MAP LEARNINGalgorithmMarkov decision processes (MDP), 370, 
387applications of, 386MARKUS, 302MARVIN, 302Maximally general hypotheses, 
computation by CANDIDATE- 
ELIMINATION algorithm, 3 1, 
46Maximally specific hypotheses: 
computation by CANDIDATE- 
ELIMINATION algorithm, 3 1, 
46computation by FIND-S algorithm, 
26-28, 62-63Maximum a posteriori (MAP) hypothesis, 
157, 197. See also BRUTE-FORCEMAP LEARNING algorithmnaive Bayes classifier and, 178output of consistent learners, 162-163Maximum likelihood (ML) hypothesis, 157EM algorithm search for, 194-195least-squared error hypothesis and, - - 
weight update rules in, 237-238 164-167prediction of probabilities with, 
167-170MDP. See Markov decision processesMean error, 143Mean value, 133, 136Means-ends planner, 326Mechanical design, case-based reasoningin, 240-244Medical diagnosis: 
attribute selection measure, 76Bayes theorem in, 157-158META-DENDRAL, 302MFOIL, 302Minimum Description Length principle, 
66,69, 171-173, 197, 198in decision tree learning, 173-174in inductive logic programming, 
292-293MIS, 302Mistake-bound learning, 202, 220, 226in CANDIDATE-ELIMINATION algorithm, 
221-222in FIND-S algorithm, 220-221in HALVING algorithm, 221-222in LIST-THEN-ELIMINATE algorithm, 
221-222in WEIGHTED-MAJORITY algorithm, 
224-225Mistake bounds, optimal. See Optimalmistake boundsML hypothesis. See Maximum likelihoodhypothesisMomentum, addition to BACKPROPAGATIONalgorithm, 100, 104More-general-than partial ordering, 24-28, 
46in CANDIDATE-ELIMINATION algorithm, 
29in FIND-S algorithm, 26-28O-subsumption, entailment, and, 299-300in version spaces, 31Multilayer feedforward networksBACKPROPAGATION algorithm in, 95-101function representation in, 105-106, 115representation of decision surfaces, 96training of multiple networks, 105VC dimension of, 218-220Naive Bayes classifier, 154-155, 177-179, 
197Bayesian belief network, comparisonwith, 186maximum a posteriori (MAP) hypothesisand, 178use in text classification, 180-184Naive Bayes learner. See Naive BayesclassifierNegation-as-failure strategy, 279, 319, 
321nNegative literal, 284, 285Neural network learning, 81-124. 
See also BACKPROPAGATIONalgorithm; CASCADE-CORRELATIONalgorithm, EBNN algorithm, 
KBANN algorithm, TANGENTPROPalgorithmapplications of, 83, 85in face recognition, 113cross-validation in, 11 1-1 12decision tree learning, comparison with, 
85discovery of hidden layer representationsin, 107overfitting in, 123in Q learning, 384representation in, 82-83, 84, 105-106Neural networks, artificial, 81-124. 
See also Multilayer feedforwardnetworks; Radial basis functionnetworks; Recurrent networksbiological neural networks, comparisonwith, 82creation by KBANN algorithm, 342-343VC dimension of, 218-220Neural networks, biological, 82Neurobiology, influence on machinelearning, 4, 82New features: 
derivation in BACKPROPAGATIONalgorithm, 106-109, 123derivation in explanation-based learning, 
320-321NEWSWEEDER system, 183-184Nondeterministic environments, Q learningMutation operator, 252, 253, 255, 257, 262 in, 381-383410 SUBJECT INDEXNormal distribution, 133, 139-140, 143, 
151, 165for noise, 167in paired tests, 149Occam's razor, 4, 65-66, 171Offline learning systems, 385One-sided bounds, 141, 144Online learning systems, 385Optimal brain damage approach, 122Optimal code, 172Optimal mistake bounds, 222-223Optimal policy for selecting actions, 
371-372Optimization problems: 
explanation-based learning in, 325genetic algorithms in, 256, 269reinforcement learning in, 256Output encoding in face recognition, 
114-1 15Output units, BACKPROPAGATION weightupdate rule for, 102-103Overfitting, 123in BACKPROPAGATION algorithm, 108, 
11&111in decision tree learning, 66-69, 76-77, 
111definition of, 67Minimum Description Length principleand, 174in neural network learning, 123PAC learning, 203-207, 225, 226of boolean conjunctions, 21 1-212definition of, 206-207training error in, 205true error in, 204-205Paired tests, 147-150, 152Parallelization in genetic algorithms, 268Partially learned concepts, 38-39Partially observable states in reinforcementlearning, 369-370Perceptron training rule, 88-89, 94,95Perceptrons, 86, 95, 96, 123representation of boolean functions, 
VC dimension of, 219weight update rule, 88-89, 94, 95Perfect domain theory, 3 12-3 13Performance measure, 6Performance system, 11-12, 13Philosophy, influence on machinelearning, 4Planning problems: 
PRODIGY in, 327case-based reasoning in, 240-241Policy for selecting actions, 370-372Population evolution, in genetic algorithms, 
260-262Positive literal, 284, 285Post-pruning: 
in decision tree learning, 68-69, 77, 
28 1in FOIL algorithm, 291in LEARN-ONE-RULE, 28 1Posterior probability, 155-156, 162Power law of practice, 4Power set, 40-42Predicates, 284, 285Preference bias, 64, 76, 77Prior knowledge, 155-156, 336. See alsoDomain theoryto augment search operators, 357-361in Bayesian learning, 155derivatives of target function, 346-356, 
362in explanation-based learning, 
308-309explicit, use in learning, 329in human learning, 330initialize-the-hypothesis approach, 
339-346, 362in PROLOG-EBG, 313search alteration in inductive-analyticallearning, 339-340, 362weighting in inductive-analyticallearning, 338, 362Prioritized sweeping, 380Probabilistic reasoning, 163Probabilities: 
estimation of, 179-1 80formulas, 159maximum likelihood (ML) hypothesisfor prediction of, 167-17087-88 probability density, 165Probability distribution, 133. See alsoBinomial distribution: Normaldistributionapproximately correct (PAC) 
learning. See PAC learninghxess control in manufacturing, 17PRODIGY, 326-327, 330Product rule, 159 
~WL, 300-302 
~oL@% 275,302, 330PROLOG-EBG, 313-321, 328-329applications of, 325deductive learning in, 321-322definition of, 314derivation of new features in, 320-321domain theory in, 322EBNN algorithm, comparison with, 356explanation of training examples, 
314-318weakest preimage in, 329inductive bias in, 322-323inductive logic programming, 
comp'arison with, 322limitations of, 329perfect domain theory in, 313prior knowledge in, 313properties of, 3 19regression process in, 3 16-3 18Propositional rules: 
learning by sequential coveringalgorithms, 275learning first-order rules, comparisonwith, 283psychology, influence on machinelearning, 4Q function: 
in deterministic environments, 374convergence ofQ learning towards, 
377-380in nondeterministic environments, 381convergence ofQ learning towards, 
382Q learning algorithm, 372-376. See alsoReinforcement learningadvantages of, 386in deterministic environments, 375convergence, 377-380training rule, 375-376strategies in, 379lookup table, neural network substitutionfor, 384in nondeterministic environments, 
381-383convergence, 382-383training rule, 382updating sequence, 379Query strategies, 37-38Radial basis function networks, 23 1, 
238-240, 245, 246, 247advantages of, 240Random variable, 133, 134, 137, 151Randomized method, 150Rank selection, 256RBF networks. See Radial basis functionnetworksRDT program, 303Real-valued target function. SeeContinuous-valued target functionRecurrent networks, 119-121. See alsoNeural networks, artificialRecursive rules, 284learning by FOIL algorithm, 290Reduced-error pruning, in decision treelearning, 69-71REGRESS algorithm, 3 17-3 18Regression, 236in PROLOG-EBG, 316-381Reinforcement learning, 367-387. See alsoQ learning algorithmapplications of, 387differences from other methods, 369-370dynamic programming and, 380, 
385-387explanation-based learning and, 330function approximation algorithms in, : 
384-3851 
Relational descriptions, learning of, 302Relative frequency, 282Relative mistake bound forWEIGHTED-MAJORITY algorithm, 
224-225Residual, 236Resolution rule, 293-294first-order, 296-297inverse entailment operator and, 
294-296propositional, 294Restriction bias, 64Reward function, in reinforcementlearning, 368Robot control: 
by BACKPROPAGATION and EBNNalgorithms, comparison of, 356genetic programming in, 269Robot driving. See Autonomous vehiclesRobot perception, attribute cost measuresin, 76Robot planning problems, explanation- 
based learning in, 327ROTE-LEARNER algorithm, inductive biasof, 44-45Roulette wheel selection, 255Rule for estimating training values, 10, 383Rule learning, 274-303in decision trees, 71-72in explanation-based learning, 3 1 1-3 19by FOCL algorithm, 357-360by genetic algorithms, 256-259, 
269-270, 274Rule post-pruning, in decision treelearning, 7 1-72Rules: 
disjunctive sets of, learning by sequentialcovering algorithms, 275-276first-order. See First-order rulespropositional. See Propositional rulesSafeToStack, 310-312Sample complexity, 202. See also Trainingexamplesbound for consistent learners, 207-210, 
225equation for, 209for finite hypothesis spaces, 207-214for infinite hypothesis spaces, 214-220of k-term CNF and DNF expressions, 
213-214of unbiased concepts, 212-213Sample error, 130-131, 133-134, 143training error and, 205Sampling theory, 132-141Scheduling problems: 
case-based reasoning in, 241explanation-based learning in, 325PRODIGY in, 327reinforcement learning in, 368Schema theorem, 260-262genetic operators in, 261-262Search bias. See Preference biasSearch control problems: 
explanation-based learning in, 325-328, 
329, 330limitations of, 327-328as sequential control processes, 369Search of hypothesis space. See Hypothesisspace searchSequential control processes, 368-369learning task in, 370-373search control problems in, 369Sequential covering algorithms, 274, 
275-279, 301, 313, 363choice of attribute-pairs in, 280-282definition of, 276FOIL algorithm, comparison with, 287, 
301-302ID3 algorithm, comparison with, 
280-28 1simultaneous covering algorithms, 
comparison with, 280-282variations of, 279-280, 286Shattering, 214-215Shepard's method, 234Sigmoid function, 97, 104Sigmoid units, 95-96, 115Simultaneous covering algorithms: 
choice of attributes in, 280-281sequential covering algorithms, 
comparison with, 280-282Single-point crossover operator, 254, 261SOAR, 327, 330Specific-to-general search, 281in FOIL algorithm, 287Speech recognition, 3BACKPROPAGATION algorithm in, 8 1representation by multilayer network, 
95, 96VC dimension bound, 2 17-2 18 weight sharing in, 1 18Split infomation, 73-74Squashing function, 96Stacking problems. See also SafeToStackanalytical learning in, 3 10explanation-based learning in, 3 10genetic programming in, 263-265PRODIGY in, 327Standard deviation, 133, I 36-1 37State-transition function, 380Statistics: 
basic definitions, 133influence on machine learning, 4Stochastic gradient descent, 93-94, 
98-100, 104-105Student t tests, 147-150, 152Substitution, 285, 296Sum rule, 159t tests, 147-150, 152TANGENTPROP algorithm, 347-350, 362BACKPROPAGATION algorithm, 
comparison with, 349in EBNN algorithm, 352search of hypothesis spaceby KBANN and BACKPROPAGATIONalgorithms, comparison with, 
350-35 1tanh function, 97Target concept, 22-23,4041PAC learning of, 21 1-213Target function, 7-8, 17continuous-valued. See Continuous- 
valued target functionrepresentation of, 8-9, 14, 17TD-GAMMON, 3, 14, 369, 383TD(Q and BACKPROPAGATION algorithmin, 384TD(h), 383-384, 387Temporal credit assignment, inreinforcement learning, 369Temporal difference learning, 383-384, 
386-387Terms, in logic, 284, 285Text classification, naive Bayes classifierin, 180-184Theorem of total probability, 1590-subsumption, 302with entailment andmore-general-than partial ordering, 
299-300Tournament selection, 256Training and validation set approach, 69. 
See also Validation setTraining derivatives, 117-1 18Training error: 
of continuous-valued hypotheses, 89-90of discrete-valued hypotheses, 205in multilayer networks, 98alternative error functions, 1 17-1 18Training examples, 5-6, 17, 23. See alsoSample complexityexplanation in PROLOG-EBG, 3 14-3 18in PAC learning, 205-207bounds on, 226Voronoi diagram of, 233Training experience, 5-6, 17Training values, rule for estimation of, 10True error, 130-131, 133, 137, 150, 
204-205of two hypotheses, differences in, 
143-144in version spaces, 208-209Two-point crossover operator, 255, 
257-258Two-sided bounds, 141Unbiased estimator, 133, 137Unbiased learners, 4042sample complexity of, 2 12-2 1 3Uniform crossover operator, 255Unifying substitution, 285, 296Unsupe~ised learning, 191Utility analysis, in explanation-basedlearning, 327-328Validation set. See also Training and Ivalidation set approach icross-validation and, 1 1 1-1 12error over, 1 10Vapnik-Chervonenkis (VC) dimension. SeeVC dimensionVariables, in logic, 284, 285Variance, 133, 136-137, 138, 143VC dimension, 214-217, 226bound on sample complexity, 217-218definition of, 215of neural networks, 218-220Version space representation theorem, 32Version spaces, 29-39, 46, 47, 207-208Bayes optimal classifier and, 176definition of, 30exhaustion of, 208-210, 226representations of, 30-32Voronoi diagram, 233Weakest preimage, 316, 329Weight decay, 11 1, 117Weight sharing, 1 18Weight update rules, 10-1 1BACKPROPAGATION weight update rule, 
101-103alternative error functions, 117-1 18in KBANN algorithm, 343-344optimization methods, 119output units, 171delta rule, 1 1, 88-90, 94gradient ascent, 170-17 1gradient descent, 91-92, 95linear programming, 95perceptron training rule, 88-89stochastic gradient descent, 93-94WEIGHTED-MAJORITY algorithm, 222-226mistake-bound learning in, 224-225Weighted voting, 222, 223, 226Widrow-Hoff rule. See Delta rule