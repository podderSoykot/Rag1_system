# RAG System - Retrieval-Augmented Generation Pipeline

A comprehensive Retrieval-Augmented Generation (RAG) system that processes PDF documents, creates searchable vector indexes, and generates contextual answers using local LLMs (Ollama) with Pinecone vector database and TF-IDF retrieval.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [How It Works](#how-it-works)
- [Features](#features)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Usage](#usage)
- [Configuration](#configuration)
- [Architecture](#architecture)
- [Performance](#performance)
- [Troubleshooting](#troubleshooting)

## ğŸ¯ Overview

This RAG system combines:
- **Pinecone** (cloud vector database) for fast semantic search
- **Ollama** (local LLM) for fast answer generation
- **TF-IDF** for keyword-based retrieval
- **Hybrid search** combining semantic and keyword matching

**Key Point**: Pinecone is used for **finding** relevant information, while Ollama is used for **generating** the answer.

## ğŸ”„ How It Works

The RAG system uses a two-stage process:

```
User Question
    â†“
1. RETRIEVAL (Pinecone/FAISS)
   - Query is embedded using sentence-transformers
   - Pinecone searches for similar document chunks
   - Returns top-k most relevant chunks
    â†“
2. PROMPT BUILDING
   - Relevant chunks + user query â†’ formatted prompt
    â†“
3. GENERATION (Ollama/TinyLlama)
   - Prompt sent to Ollama API
   - Ollama generates answer based on context
    â†“
4. POST-PROCESSING
   - Clean answer (remove repetition, format)
   - Return final answer
```

### Component Roles

**Pinecone (Vector Database)**
- **Purpose**: Store and search document embeddings
- **Used for**: Finding relevant document chunks
- **NOT used for**: Generating answers

**Ollama (LLM)**
- **Purpose**: Generate answers from context
- **Used for**: Answer generation
- **Input**: Prompt with retrieved chunks + query
- **Output**: Generated answer text

## âœ¨ Features

- **Document Ingestion**: PDF to text conversion and intelligent chunking with structured content preservation
- **Indexing**: Pinecone vector database (cloud-based, scalable) with FAISS fallback
- **Retrieval**: Hybrid search with parallelized semantic (Pinecone) and keyword (TF-IDF) retrieval, with result caching
- **Synthesis**: Generate answers using Ollama (default, fast) or local LLM (TinyLlama) with answer post-processing
- **Performance**: Optimized for speed with parallel searches, caching, and performance monitoring
- **Quality**: Enhanced prompts and answer post-processing for better, more direct responses
- **Monitoring**: Detailed timing information and answer quality metrics

## ğŸš€ Installation

### Prerequisites

- Python 3.8 or higher
- Ollama (for best performance) - [Download](https://ollama.ai)
- Pinecone account (optional, for cloud vector storage) - [Sign up](https://www.pinecone.io/)

### Step 1: Clone and Install Dependencies

```bash
# Clone the repository
git clone <your-repo-url>
cd Rag1_system

# Install Python dependencies
pip install -r rag_system/requirements.txt
```

### Step 2: Install Ollama (Recommended)

**Windows:**
1. Download from https://ollama.com/download
2. Run `OllamaSetup.exe`
3. Follow the installation wizard
4. Ollama will start automatically

**After installation, pull a model:**
```bash
ollama pull llama3.2:3b
```

**Verify installation:**
```bash
ollama list
```

### Step 3: Set Up Environment Variables

Create a `.env` file in the root directory (`Rag1_system/.env`):

```bash
# Ollama Configuration (default, recommended for speed)
USE_OLLAMA=true
OLLAMA_MODEL=llama3.2:3b
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_TIMEOUT=120

# Pinecone Configuration (optional, for cloud vector storage)
USE_PINECONE=true
PINECONE_API_KEY=your-pinecone-api-key-here
PINECONE_INDEX_NAME=rag-system
PINECONE_ENVIRONMENT=us-east-1

# Search Configuration
USE_HYBRID_SEARCH=true
SEMANTIC_WEIGHT=0.7
TFIDF_WEIGHT=0.3

# Chunking Configuration
CHUNK_SIZE=800
CHUNK_OVERLAP=100

# Performance Settings
CACHE_ENABLED=true
CACHE_SIZE=100
EMBEDDING_BATCH_SIZE=128
```

**Get your API keys:**
- Pinecone API key: https://www.pinecone.io/
- If you don't want to use Pinecone, set `USE_PINECONE=false` to use local FAISS storage
- If you don't want to use Ollama, set `USE_OLLAMA=false` to use local TinyLlama (slower)

## ğŸƒ Quick Start

1. **Place your PDF documents** in `rag_system/data/raw/`

2. **Run the system:**
   ```bash
   cd rag_system
   python main.py
   ```

3. **The system will:**
   - Automatically process PDFs (if not already processed)
   - Create embeddings and index them in Pinecone/FAISS
   - Enter interactive mode for Q&A

4. **Ask questions:**
   ```
   Enter your question (or 'quit' to exit): What are the chapters in this book?
   ```

## ğŸ“– Usage

### Command Line Interface

**Single Query (Non-Interactive):**
```bash
cd rag_system
python main.py --query "What are the chapters in this book?" --top_k 3
```

**Interactive Mode:**
```bash
cd rag_system
python main.py
```

**Command Line Options:**
- `--query`: Run a single question non-interactively
- `--top_k`: Number of chunks to retrieve (default: 3)
- `--force-ingestion`: Force re-processing of all documents
- `--no-timing`: Disable timing information

### Programmatic Usage

```python
from rag_system.main import rag_pipeline

# Run a query
answer = rag_pipeline("What is the main topic?", top_k=3)
print(answer)
```

### Re-indexing Documents

If you add new PDFs or want to re-process existing ones:

```bash
cd rag_system
python main.py --force-ingestion
```

## âš™ï¸ Configuration

All settings are managed in `rag_system/config/settings.py` and via environment variables (`.env` file).

### Key Configuration Options

**Retrieval (Pinecone/FAISS):**
- `USE_PINECONE=true` - Use Pinecone cloud vector database (recommended)
- `PINECONE_API_KEY` - Your Pinecone API key
- Falls back to FAISS if Pinecone is unavailable

**Generation (Ollama/TinyLlama):**
- `USE_OLLAMA=true` - Use Ollama for answer generation (default, recommended)
- `OLLAMA_MODEL=llama3.2:3b` - Model to use
- Falls back to TinyLlama if Ollama is unavailable

**Chunking:**
- `CHUNK_SIZE=800` - Size of text chunks (characters)
- `CHUNK_OVERLAP=100` - Overlap between chunks for better context

**Performance:**
- `CACHE_ENABLED=true` - Enable result caching
- `USE_HYBRID_SEARCH=true` - Combine semantic and keyword search

## ğŸ—ï¸ Architecture

### Directory Structure

```
Rag1_system/
â”œâ”€â”€ rag_system/
â”‚   â”œâ”€â”€ config/              # Configuration files
â”‚   â”‚   â””â”€â”€ settings.py
â”‚   â”œâ”€â”€ data/                # Data storage
â”‚   â”‚   â”œâ”€â”€ raw/             # Original PDF files
â”‚   â”‚   â”œâ”€â”€ processed/       # Extracted text files
â”‚   â”‚   â””â”€â”€ embeddings/      # Vector database files
â”‚   â”œâ”€â”€ ingestion/          # Document processing
â”‚   â”‚   â”œâ”€â”€ data_loader.py  # PDF to text extraction
â”‚   â”‚   â”œâ”€â”€ chunker.py      # Intelligent text chunking
â”‚   â”‚   â”œâ”€â”€ embedder.py     # Generate embeddings
â”‚   â”‚   â””â”€â”€ indexer.py      # Store in Pinecone/FAISS
â”‚   â”œâ”€â”€ retrieval/          # Document search
â”‚   â”‚   â”œâ”€â”€ retriever.py    # Hybrid search (Pinecone + TF-IDF)
â”‚   â”‚   â””â”€â”€ query_expansion.py
â”‚   â”œâ”€â”€ synthesis/          # Answer generation
â”‚   â”‚   â”œâ”€â”€ prompt_builder.py
â”‚   â”‚   â”œâ”€â”€ generator.py    # Main generator interface
â”‚   â”‚   â”œâ”€â”€ local_generator.py  # Ollama/TinyLlama integration
â”‚   â”‚   â””â”€â”€ postprocessor.py   # Answer cleaning
â”‚   â”œâ”€â”€ main.py             # CLI entry point
â”‚   â””â”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ RAG_ARCHITECTURE.md    # Detailed architecture docs
â””â”€â”€ INSTALL_OLLAMA.md      # Ollama installation guide
```

### Components

1. **Ingestion** (`rag_system/ingestion/`)
   - `data_loader.py`: PDF to text extraction using pypdf
   - `chunker.py`: Intelligent text chunking with sentence boundaries
   - `embedder.py`: Generate embeddings using sentence-transformers
   - `indexer.py`: Store embeddings in Pinecone/FAISS and create TF-IDF index

2. **Retrieval** (`rag_system/retrieval/`)
   - `retriever.py`: Hybrid search combining Pinecone (semantic) and TF-IDF (keyword)
   - `query_expansion.py`: Query enhancement for better recall

3. **Synthesis** (`rag_system/synthesis/`)
   - `prompt_builder.py`: Build context-aware prompts
   - `generator.py`: Main generator interface with post-processing
   - `local_generator.py`: Ollama API integration
   - `postprocessor.py`: Clean and format answers

### Data Flow

```
PDF Documents (data/raw/)
    â†“ [Ingestion]
Text Extraction â†’ Chunking â†’ Embeddings
    â†“
Pinecone Index / FAISS Index
    â†“ [Query Time]
User Question â†’ Embed Query â†’ Search Pinecone
    â†“
Retrieve Top-K Chunks â†’ Build Prompt
    â†“
Send to Ollama â†’ Generate Answer
    â†“
Post-process â†’ Return Answer
```

## âš¡ Performance

### Speed Optimizations

- **Use Ollama**: 5-10x faster than local TinyLlama (default enabled)
- **Use Pinecone**: Cloud-based vector search is faster than local FAISS for large datasets
- **Parallel Retrieval**: Semantic and TF-IDF searches run in parallel when both enabled
- **Result Caching**: Similar queries are cached for instant responses

### Quality Improvements

- **Answer Post-processing**: Removes repetition, formats lists, cleans meta-phrases
- **Enhanced Prompts**: Optimized for direct answers based on query type (list, complex, simple)
- **Structured Content**: Better chunking preserves chapters, sections, and structured content
- **Validation**: Answer completeness validation with helpful feedback

### Monitoring

The system provides detailed timing information:
- **Retrieval time**: Breakdown of semantic (Pinecone) vs TF-IDF search
- **Prompt building time**: Usually <1ms
- **Generation time**: Ollama generation time
- **Total time**: End-to-end query processing time
- **Answer quality metrics**: Length, word count, completeness warnings

**Example Output:**
```
[Timing]
  Retrieval:  2053ms
  Prompt:     1ms
  Generation: 34124ms
  Total:      42352ms (42.35s)

[Answer Quality]
  Length: 526 chars, 89 words
```

## ğŸ”§ Troubleshooting

### Ollama Issues

**Ollama not found:**
- Restart your terminal/PowerShell after installation
- Verify Ollama is running: `ollama list`
- Check if Ollama service is running (should start automatically)

**Model not found:**
- Pull the model: `ollama pull llama3.2:3b`
- Verify: `ollama list`
- Check model name in `.env` matches pulled model

### Pinecone Issues

**Connection failed:**
- Verify API key in `.env` file
- Check internet connection
- Verify index name exists in Pinecone dashboard
- System will fall back to FAISS if Pinecone fails

### Performance Issues

**Slow generation:**
- Ensure Ollama is enabled (`USE_OLLAMA=true`)
- Use a smaller model: `OLLAMA_MODEL=phi3:mini`
- Reduce `top_k` for faster retrieval

**Slow retrieval:**
- Use Pinecone for large datasets
- Reduce `CHUNK_SIZE` for faster processing
- Enable caching: `CACHE_ENABLED=true`

### Import Errors

**Module not found:**
- Ensure you're running from `rag_system/` directory
- Or install as package: `pip install -e .`
- Check Python path includes project root

## ğŸ“¦ Requirements

See `rag_system/requirements.txt` for all dependencies. Key packages:

- `transformers>=4.41.0` - Hugging Face transformers
- `torch` - PyTorch for model inference
- `sentence-transformers` - Embedding generation
- `pinecone-client` - Pinecone vector database client
- `faiss-cpu` - Local vector search (fallback)
- `pypdf` - PDF text extraction
- `nltk>=3.8` - Natural language processing
- `python-dotenv` - Environment variable management
- `requests` - HTTP requests for Ollama API

## ğŸ“ Notes

- **No OpenAI Required**: System uses only local LLMs (Ollama/TinyLlama)
- **Privacy**: All processing happens locally (except Pinecone storage)
- **Scalability**: Pinecone handles large document collections efficiently
- **Fallbacks**: System gracefully falls back to FAISS/TinyLlama if cloud services unavailable

## ğŸ“§ Contact

For questions or support:
- **Author**: Soykot Podder
- **Email**: diptopodder95@gmail.com

## ğŸ“„ License

[Add your license information here]

---

**Happy Querying!** ğŸš€
