# RAG System - Retrieval-Augmented Generation Pipeline

A comprehensive Retrieval-Augmented Generation (RAG) system that combines document ingestion, intelligent retrieval, and AI-powered answer generation. This system processes PDF documents, creates searchable indexes, and generates contextual answers using OpenAI's language models.

## ğŸš€ Features

- **Document Processing**: Extract text from PDF documents with automatic chunking
- **Intelligent Indexing**: TF-IDF based document indexing for efficient retrieval
- **Semantic Search**: Find relevant document chunks based on query relevance
- **AI Generation**: Generate contextual answers using OpenAI GPT models
- **Modular Architecture**: Clean separation of ingestion, retrieval, and synthesis components
- **Easy Configuration**: Simple settings management with environment variables

## ğŸ—ï¸ Architecture

```
RAG System/
â”œâ”€â”€ ingestion/          # Document processing and indexing
â”‚   â”œâ”€â”€ data_loader.py  # PDF to text conversion
â”‚   â”œâ”€â”€ chunker.py      # Text chunking and segmentation
â”‚   â”œâ”€â”€ embedder.py     # Text embedding generation
â”‚   â””â”€â”€ indexer.py      # Document indexing and storage
â”œâ”€â”€ retrieval/          # Document search and retrieval
â”‚   â”œâ”€â”€ retriever.py    # Main search functionality
â”‚   â””â”€â”€ query_expansion.py # Query enhancement (future)
â”œâ”€â”€ synthesis/          # Answer generation
â”‚   â”œâ”€â”€ prompt_builder.py # Context-aware prompt construction
â”‚   â”œâ”€â”€ generator.py    # OpenAI API integration
â”‚   â””â”€â”€ postprocessor.py # Answer refinement (future)
â”œâ”€â”€ config/             # Configuration management
â”‚   â””â”€â”€ settings.py     # System settings and paths
â””â”€â”€ main.py             # Main execution pipeline
```

## ğŸ“‹ Prerequisites

- Python 3.8 or higher
- OpenAI API key
- Sufficient disk space for document storage and indexes

## ğŸ› ï¸ Installation

1. **Clone the repository**
   ```bash
   git clone <your-repo-url>
   cd rag_system
   ```

2. **Create a virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Set up environment variables**
   Create a `.env` file in the root directory:
   ```env
   OPENAI_API_KEY=your_openai_api_key_here
   ```

## ğŸ“ Data Structure

Place your PDF documents in the `data/raw/` directory. The system will automatically:
- Convert PDFs to text in `data/processed/`
- Create chunks in `data/processed/chunks/`
- Store indexes in `data/embeddings/chroma_db/`

```
data/
â”œâ”€â”€ raw/                # Original PDF files
â”œâ”€â”€ processed/          # Extracted text files
â”‚   â””â”€â”€ chunks/        # Text chunks for indexing
â””â”€â”€ embeddings/         # Search indexes
    â””â”€â”€ chroma_db/     # Vector database and TF-IDF indexes
```

## ğŸš€ Usage

### Basic Usage

1. **Run the complete pipeline**
   ```bash
   python main.py
   ```

2. **Use the RAG system programmatically**
   ```python
   from main import rag_pipeline
   
   # Ask a question
   answer = rag_pipeline("What are the main concepts in the document?")
   print(answer)
   ```

### Advanced Usage

1. **Custom ingestion**
   ```python
   from ingestion.data_loader import process_pdfs
   from ingestion.chunker import process_files
   from ingestion.indexer import index_documents
   
   # Process specific directories
   process_pdfs("path/to/pdfs", "path/to/output")
   process_files("path/to/texts", "path/to/chunks")
   index_documents("path/to/chunks", "path/to/index", "model_name")
   ```

2. **Custom retrieval**
   ```python
   from retrieval.retriever import Retriever
   
   retriever = Retriever("path/to/index", "model_name")
   results = retriever.search("your query", top_k=5)
   ```

## âš™ï¸ Configuration

Key configuration options in `config/settings.py`:

- **Data Paths**: Configure input/output directories
- **Embedding Model**: Choose the sentence transformer model
- **OpenAI Settings**: Model selection and API configuration
- **Chunk Size**: Adjust text chunking parameters (default: 500 characters)

## ğŸ”§ Customization

### Adding New Document Types

Extend `data_loader.py` to support additional file formats:

```python
def process_documents(input_dir, output_dir, file_extensions):
    # Add support for .docx, .txt, etc.
    pass
```

### Implementing Vector Search

Replace TF-IDF with vector embeddings:

```python
from ingestion.embedder import get_embedding_model
import chromadb

# Use ChromaDB for vector storage
client = chromadb.Client()
collection = client.create_collection("documents")
```

### Custom Prompt Templates

Modify `prompt_builder.py` for different use cases:

```python
def build_prompt(query: str, docs: list, template: str = "default"):
    if template == "summarization":
        return f"Summarize the following context:\n{context}\n\nQuery: {query}"
    # Add more templates...
```

## ğŸ§ª Testing

Run the system with sample data:

1. **Add test PDFs** to `data/raw/`
2. **Execute the pipeline**:
   ```bash
   python main.py
   ```
3. **Verify outputs** in the processed directories

## ğŸ› Troubleshooting

### Common Issues

1. **Missing OpenAI API Key**
   - Ensure `.env` file exists with `OPENAI_API_KEY`
   - Check API key validity in OpenAI dashboard

2. **PDF Processing Errors**
   - Verify PDF files are not corrupted
   - Check file permissions in data directories

3. **Memory Issues**
   - Reduce chunk size in settings
   - Process documents in smaller batches

4. **Import Errors**
   - Ensure virtual environment is activated
   - Reinstall dependencies: `pip install -r requirements.txt`

### Debug Mode

Enable verbose logging by modifying the main execution:

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## ğŸ“ˆ Performance

- **Processing Speed**: ~100 pages/minute (varies by document complexity)
- **Memory Usage**: ~50MB base + document size
- **Index Size**: ~10% of original document size
- **Query Response**: <2 seconds for typical queries

## ğŸ”® Future Enhancements

- [ ] Vector embedding support with ChromaDB
- [ ] Query expansion and refinement
- [ ] Answer post-processing and validation
- [ ] Web interface for document uploads
- [ ] Multi-language support
- [ ] Advanced chunking strategies
- [ ] Document metadata extraction
- [ ] Performance monitoring and analytics

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- OpenAI for providing the language model APIs
- Sentence Transformers for embedding capabilities
- PyPDF for PDF processing functionality

## ğŸ“ Support

For questions, issues, or contributions:
- Create an issue on GitHub
- Contact the development team
- Check the troubleshooting section above

---

**Note**: This system is designed for research and development purposes. Ensure compliance with data privacy regulations when processing sensitive documents.
